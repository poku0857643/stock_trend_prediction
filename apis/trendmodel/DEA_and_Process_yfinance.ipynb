{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Collect and Process Data for Stock Trends from yFinance",
   "id": "c8cbe6a58b84b1b2"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-30T03:55:20.484718Z",
     "start_time": "2025-05-30T03:55:20.482430Z"
    }
   },
   "source": "import yfinance as yf",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T03:55:21.847171Z",
     "start_time": "2025-05-30T03:55:21.843616Z"
    }
   },
   "cell_type": "code",
   "source": "yf.__version__",
   "id": "7d6ba94775464727",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.2.61'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T03:56:24.798661Z",
     "start_time": "2025-05-30T03:56:24.550345Z"
    }
   },
   "cell_type": "code",
   "source": "df_ = yf.download(['AAPL','TSM'], start=\"2024-01-01\")",
   "id": "812a1d4ad0f33f0b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  2 of 2 completed\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T03:56:26.623982Z",
     "start_time": "2025-05-30T03:56:26.614747Z"
    }
   },
   "cell_type": "code",
   "source": "df_",
   "id": "96464208fb204cdc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Price            Close                    High                     Low  \\\n",
       "Ticker            AAPL         TSM        AAPL         TSM        AAPL   \n",
       "Date                                                                     \n",
       "2024-01-02  184.290421   99.724220  187.070068  100.794838  182.553143   \n",
       "2024-01-03  182.910522   98.388412  184.528677   99.449206  182.096477   \n",
       "2024-01-04  180.587555   97.366905  181.758969   98.653611  179.565044   \n",
       "2024-01-05  179.862839   97.838371  181.431354   98.850051  178.860187   \n",
       "2024-01-08  184.210999  100.421593  184.250716  100.961814  180.180517   \n",
       "...                ...         ...         ...         ...         ...   \n",
       "2025-05-22  201.360001  196.190002  202.750000  196.830002  199.699997   \n",
       "2025-05-23  195.270004  191.979996  197.699997  192.800003  193.460007   \n",
       "2025-05-27  200.210007  197.679993  200.740005  198.309998  197.429993   \n",
       "2025-05-28  200.419998  196.139999  202.729996  198.070007  199.899994   \n",
       "2025-05-29  199.949997  197.149994  203.809998  199.800003  198.509995   \n",
       "\n",
       "Price                         Open                Volume            \n",
       "Ticker             TSM        AAPL         TSM      AAPL       TSM  \n",
       "Date                                                                \n",
       "2024-01-02   98.800936  185.789438  100.431416  82488700   9020900  \n",
       "2024-01-03   97.887489  182.880742   98.555387  58414500   6650600  \n",
       "2024-01-04   97.357088  180.825800   97.759792  71983600   7996700  \n",
       "2024-01-05   97.042780  180.666963   97.239220  62303300   7344900  \n",
       "2024-01-08   98.712542  180.766224   98.840227  59144500  12455600  \n",
       "...                ...         ...         ...       ...       ...  \n",
       "2025-05-22  191.339996  200.710007  191.339996  46742400  11825000  \n",
       "2025-05-23  190.029999  193.669998  192.020004  78432900   9403100  \n",
       "2025-05-27  193.699997  198.300003  194.100006  56288500  12228200  \n",
       "2025-05-28  195.479996  200.589996  196.089996  45339700  17787600  \n",
       "2025-05-29  196.080002  203.580002  199.460007  51396800  14609900  \n",
       "\n",
       "[353 rows x 10 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Price</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Close</th>\n",
       "      <th colspan=\"2\" halign=\"left\">High</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Low</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Open</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticker</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>TSM</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>TSM</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>TSM</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>TSM</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>TSM</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-01-02</th>\n",
       "      <td>184.290421</td>\n",
       "      <td>99.724220</td>\n",
       "      <td>187.070068</td>\n",
       "      <td>100.794838</td>\n",
       "      <td>182.553143</td>\n",
       "      <td>98.800936</td>\n",
       "      <td>185.789438</td>\n",
       "      <td>100.431416</td>\n",
       "      <td>82488700</td>\n",
       "      <td>9020900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-03</th>\n",
       "      <td>182.910522</td>\n",
       "      <td>98.388412</td>\n",
       "      <td>184.528677</td>\n",
       "      <td>99.449206</td>\n",
       "      <td>182.096477</td>\n",
       "      <td>97.887489</td>\n",
       "      <td>182.880742</td>\n",
       "      <td>98.555387</td>\n",
       "      <td>58414500</td>\n",
       "      <td>6650600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-04</th>\n",
       "      <td>180.587555</td>\n",
       "      <td>97.366905</td>\n",
       "      <td>181.758969</td>\n",
       "      <td>98.653611</td>\n",
       "      <td>179.565044</td>\n",
       "      <td>97.357088</td>\n",
       "      <td>180.825800</td>\n",
       "      <td>97.759792</td>\n",
       "      <td>71983600</td>\n",
       "      <td>7996700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-05</th>\n",
       "      <td>179.862839</td>\n",
       "      <td>97.838371</td>\n",
       "      <td>181.431354</td>\n",
       "      <td>98.850051</td>\n",
       "      <td>178.860187</td>\n",
       "      <td>97.042780</td>\n",
       "      <td>180.666963</td>\n",
       "      <td>97.239220</td>\n",
       "      <td>62303300</td>\n",
       "      <td>7344900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-08</th>\n",
       "      <td>184.210999</td>\n",
       "      <td>100.421593</td>\n",
       "      <td>184.250716</td>\n",
       "      <td>100.961814</td>\n",
       "      <td>180.180517</td>\n",
       "      <td>98.712542</td>\n",
       "      <td>180.766224</td>\n",
       "      <td>98.840227</td>\n",
       "      <td>59144500</td>\n",
       "      <td>12455600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-05-22</th>\n",
       "      <td>201.360001</td>\n",
       "      <td>196.190002</td>\n",
       "      <td>202.750000</td>\n",
       "      <td>196.830002</td>\n",
       "      <td>199.699997</td>\n",
       "      <td>191.339996</td>\n",
       "      <td>200.710007</td>\n",
       "      <td>191.339996</td>\n",
       "      <td>46742400</td>\n",
       "      <td>11825000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-05-23</th>\n",
       "      <td>195.270004</td>\n",
       "      <td>191.979996</td>\n",
       "      <td>197.699997</td>\n",
       "      <td>192.800003</td>\n",
       "      <td>193.460007</td>\n",
       "      <td>190.029999</td>\n",
       "      <td>193.669998</td>\n",
       "      <td>192.020004</td>\n",
       "      <td>78432900</td>\n",
       "      <td>9403100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-05-27</th>\n",
       "      <td>200.210007</td>\n",
       "      <td>197.679993</td>\n",
       "      <td>200.740005</td>\n",
       "      <td>198.309998</td>\n",
       "      <td>197.429993</td>\n",
       "      <td>193.699997</td>\n",
       "      <td>198.300003</td>\n",
       "      <td>194.100006</td>\n",
       "      <td>56288500</td>\n",
       "      <td>12228200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-05-28</th>\n",
       "      <td>200.419998</td>\n",
       "      <td>196.139999</td>\n",
       "      <td>202.729996</td>\n",
       "      <td>198.070007</td>\n",
       "      <td>199.899994</td>\n",
       "      <td>195.479996</td>\n",
       "      <td>200.589996</td>\n",
       "      <td>196.089996</td>\n",
       "      <td>45339700</td>\n",
       "      <td>17787600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-05-29</th>\n",
       "      <td>199.949997</td>\n",
       "      <td>197.149994</td>\n",
       "      <td>203.809998</td>\n",
       "      <td>199.800003</td>\n",
       "      <td>198.509995</td>\n",
       "      <td>196.080002</td>\n",
       "      <td>203.580002</td>\n",
       "      <td>199.460007</td>\n",
       "      <td>51396800</td>\n",
       "      <td>14609900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>353 rows × 10 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T04:02:46.778165Z",
     "start_time": "2025-05-30T04:02:46.198697Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import get_all_tickers\n",
    "\n",
    "# A real list of 300 would require extensive data scraping and cleaning.\n",
    "\n",
    "top_global_tickers = [\n",
    "    # US Tech Giants\n",
    "    'AAPL', 'MSFT', 'GOOG', 'GOOGL', 'AMZN', 'NVDA', 'META', 'TSLA', 'ADBE', 'CRM', 'NFLX', 'INTC', 'CSCO',\n",
    "    # US Blue Chips/Diversified\n",
    "    'JPM', 'XOM', 'JNJ', 'PG', 'V', 'MA', 'UNH', 'HD', 'KO', 'PEP', 'DIS', 'BAC', 'WMT', 'BRK-B', 'LLY', 'AVGO', 'ORCL',\n",
    "    # European Giants (examples - remember exchange suffixes)\n",
    "    'ASML.AS', 'SAP.DE', 'RMS.PA', 'SIE.DE', 'HSBA.L', 'BP.L', 'SHEL.L', 'VOD.L', 'DAI.DE', 'BNP.PA', 'OR.PA', 'NOVN.SW', 'ROG.SW',\n",
    "    'LVMUY', # LVMH Moët Hennessy Louis Vuitton SE (ADR)\n",
    "    'NVO', # Novo Nordisk A/S (ADR)\n",
    "    'SMNEY', # Siemens AG (ADR)\n",
    "    # Asian Giants (examples - remember exchange suffixes)\n",
    "    '7203.T', '9984.T', '8058.T', '0005.HK', '0700.HK', '0001.HK', '2330.TW', '2454.TW', '005930.KS', '000660.KS',\n",
    "    'RELIANCE.NS', 'TCS.NS', 'BABA', 'TCEHY',\n",
    "    '2222.SR', # Saudi Aramco\n",
    "    'ICBC.HK', # Industrial and Commercial Bank of China\n",
    "    '601398.SS', # ICBC (Shanghai)\n",
    "    # Canadian (examples)\n",
    "    'RY', 'TD', 'ENB',\n",
    "    # Australian (examples)\n",
    "    'CBA.AX', 'BHP.AX', 'ANZ.AX',\n",
    "    # Add more from various regions and sectors to reach 300\n",
    "    # ... this list would be much longer ...\n",
    "]\n",
    "\n",
    "# You would extend top_global_tickers significantly to reach 300.\n",
    "# A programmatic way using 'get-all-tickers' would be more efficient:\n",
    "\n",
    "try:\n",
    "    from get_all_tickers import get_biggest_n_tickers, Region, SectorConstants\n",
    "    # This might give you a good starting point for the top N global companies\n",
    "    # Note: 'get_biggest_n_tickers' might prioritize US listings or those readily available.\n",
    "    # You might need to combine it with regional searches.\n",
    "    top_300_from_library = get_biggest_n_tickers(300)\n",
    "    print(f\"Fetched {len(top_300_from_library)} tickers using get_biggest_n_tickers.\")\n",
    "    print(top_300_from_library[:10]) # Print first 10 for review\n",
    "\n",
    "    # Example of getting tickers by region (you'd need to combine these)\n",
    "    # asia_tickers = get_tickers_by_region(Region.ASIA)\n",
    "    # print(f\"Fetched {len(asia_tickers)} tickers from Asia.\")\n",
    "\n",
    "    # Let's use the programmatically generated list for fetching data\n",
    "    final_tickers_to_fetch = top_300_from_library\n",
    "\n",
    "except ImportError:\n",
    "    print(\"The 'get-all-tickers' library is not installed. Please install it using 'pip install get-all-tickers'.\")\n",
    "    print(\"Proceeding with a very small, hardcoded example list for demonstration.\")\n",
    "    final_tickers_to_fetch = ['AAPL', 'MSFT', 'GOOG', '2330.TW', 'HSBA.L'] # Fallback for demonstration\n",
    "\n",
    "# Now, fetch data using yfinance\n",
    "print(f\"\\nAttempting to fetch data for {len(final_tickers_to_fetch)} tickers using yfinance...\")\n",
    "\n",
    "data = {}\n",
    "for ticker_symbol in final_tickers_to_fetch:\n",
    "    try:\n",
    "        ticker = yf.Ticker(ticker_symbol)\n",
    "        # Fetch current price or a small amount of history to confirm it works\n",
    "        hist = ticker.history(period=\"1d\") # Get last day's data\n",
    "        if not hist.empty:\n",
    "            data[ticker_symbol] = hist\n",
    "            print(f\"Successfully fetched data for {ticker_symbol}\")\n",
    "        else:\n",
    "            print(f\"No data returned for {ticker_symbol}. It might be an invalid ticker or no recent trading.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {ticker_symbol}: {e}\")\n",
    "\n",
    "print(\"\\n--- Summary of fetched data ---\")\n",
    "for ticker, df in data.items():\n",
    "    print(f\"Ticker: {ticker}, Data Points: {len(df)}\")"
   ],
   "id": "5f8ae7b5043f7b6c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 'get-all-tickers' library is not installed. Please install it using 'pip install get-all-tickers'.\n",
      "Proceeding with a very small, hardcoded example list for demonstration.\n",
      "\n",
      "Attempting to fetch data for 5 tickers using yfinance...\n",
      "Successfully fetched data for AAPL\n",
      "Successfully fetched data for MSFT\n",
      "Successfully fetched data for GOOG\n",
      "Successfully fetched data for 2330.TW\n",
      "Successfully fetched data for HSBA.L\n",
      "\n",
      "--- Summary of fetched data ---\n",
      "Ticker: AAPL, Data Points: 1\n",
      "Ticker: MSFT, Data Points: 1\n",
      "Ticker: GOOG, Data Points: 1\n",
      "Ticker: 2330.TW, Data Points: 1\n",
      "Ticker: HSBA.L, Data Points: 1\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T07:44:23.503103Z",
     "start_time": "2025-06-03T07:44:23.497479Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "5dd187ab5abb57cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Time-Series Model",
   "id": "5c789ac8a5e33097"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T04:06:47.192085Z",
     "start_time": "2025-06-01T04:06:47.131678Z"
    }
   },
   "cell_type": "code",
   "source": "\n",
   "id": "3227ef0d8fbb0135",
   "outputs": [],
   "execution_count": 124
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T04:22:30.827657Z",
     "start_time": "2025-06-01T04:22:30.814270Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "acaf47b20b3fd617",
   "outputs": [],
   "execution_count": 142
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T04:22:34.413915Z",
     "start_time": "2025-06-01T04:22:34.411028Z"
    }
   },
   "cell_type": "code",
   "source": "\n",
   "id": "ac81424da9c2d2a5",
   "outputs": [],
   "execution_count": 143
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T04:23:53.499161Z",
     "start_time": "2025-06-01T04:22:36.945427Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "b6d9f27c1a9a141b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((619025, 15, 4), (619025,))"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 144
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T04:24:00.051665Z",
     "start_time": "2025-06-01T04:24:00.047166Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "5d6abb68019ea737",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.00658545, 0.00649954, 0.0064565 , 0.01359914],\n",
       "        [0.00649732, 0.0064463 , 0.00627456, 0.01436664],\n",
       "        [0.00628189, 0.00620433, 0.00619588, 0.01314381],\n",
       "        [0.00620844, 0.00641243, 0.00626964, 0.01659475],\n",
       "        [0.0065218 , 0.00642211, 0.00573365, 0.05156577],\n",
       "        [0.00602728, 0.00625272, 0.00611228, 0.02527831],\n",
       "        [0.00622313, 0.00622852, 0.00618604, 0.01836575],\n",
       "        [0.00614479, 0.00608334, 0.00572873, 0.02381803],\n",
       "        [0.0058755 , 0.00593331, 0.00560579, 0.01928401],\n",
       "        [0.00585102, 0.00576393, 0.00575823, 0.0098205 ],\n",
       "        [0.00586571, 0.00584136, 0.00565497, 0.01162401],\n",
       "        [0.00564048, 0.00567681, 0.00550745, 0.01523524],\n",
       "        [0.00570903, 0.0057736 , 0.00574348, 0.01195414],\n",
       "        [0.00581185, 0.00577844, 0.00584675, 0.00993728],\n",
       "        [0.00575309, 0.00593331, 0.00581232, 0.01193198]]),\n",
       " np.float64(0.006012474296794487))"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 145
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T04:24:06.428563Z",
     "start_time": "2025-06-01T04:24:06.366920Z"
    }
   },
   "cell_type": "code",
   "source": "\n",
   "id": "433ae4a1a795c484",
   "outputs": [],
   "execution_count": 146
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T12:02:51.246898Z",
     "start_time": "2025-06-02T12:02:51.219856Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "d59a3eb762c529ff",
   "outputs": [],
   "execution_count": 162
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Add Early Stopping, Model Saving, and Model Checkpointing",
   "id": "46042b08b34f7381"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T12:03:09.571718Z",
     "start_time": "2025-06-02T12:03:09.566236Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Early Stopping Class\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0.01, restore_best_weights=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.best_weights = None\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(model)\n",
    "        elif val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            self.save_checkpoint(model)\n",
    "        else:\n",
    "            self.counter += 1\n",
    "\n",
    "        if self.counter >= self.patience:\n",
    "            if self.restore_best_weights:\n",
    "                model.load_state_dict(self.best_weights)\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def save_checkpoint(self, model):\n",
    "        self.best_weights = model.state_dict().copy()\n",
    "\n"
   ],
   "id": "1130db69052a114f",
   "outputs": [],
   "execution_count": 163
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T12:03:22.822689Z",
     "start_time": "2025-06-02T12:03:22.814994Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Model Saving Utilities\n",
    "def save_model(model, optimizer, epoch, loss, filepath):\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    torch.save(checkpoint, filepath)\n",
    "    print(f\"Model saved to {filepath}\")\n",
    "\n",
    "def load_model(model, optimizer, filepath):\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    print(f\"Model loaded from {filepath}\")\n",
    "    print(f\"Resumed from epoch {epoch}, loss: {loss:.6f}\")\n",
    "    return epoch, loss"
   ],
   "id": "36a9775e2832bcc4",
   "outputs": [],
   "execution_count": 164
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T12:03:26.737518Z",
     "start_time": "2025-06-02T12:03:26.725440Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def make_prediction(model, X_data, scaler, features, target, sequence_length=30):\n",
    "    \"\"\"\n",
    "    Make a prediction using the trained model.\n",
    "\n",
    "    Parameters:\n",
    "    - model: Trained LSTM model\n",
    "    - X_data: Input data for prediction\n",
    "    - scaler: Scaler used for normalization\n",
    "    - features: List of feature names\n",
    "    - target: Target variable name\n",
    "    - sequence_length: Length of the input sequence\n",
    "\n",
    "    Returns:\n",
    "    - Predicted value for the target variable\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        if len(X_data.shape) == 3:\n",
    "            # If X_data is already in the correct shape, no need to reshape\n",
    "            last_sequence = torch.tensor(X_data[-1], dtype=torch.float32).unsqueeze(0)\n",
    "        else:\n",
    "            # If X_data is 2D, reshape it to match the expected input shape\n",
    "            last_sequence = torch.tensor(X_data[-sequence_length:], dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        print(f\"Input shape for prediction: {last_sequence.shape}\")\n",
    "\n",
    "        # Maker prediction\n",
    "        # Prepare input data\n",
    "        predicted_norm = model(last_sequence).item()\n",
    "        print(f\"Predicted norm for prediction: {predicted_norm}\")\n",
    "\n",
    "\n",
    "        # Inverse normalize the predicted value\n",
    "        # Create a dummy array to hold the last sequence and the predicted value\n",
    "        dummy = np.zeros((1, len(features) + 1)) # +1 for the target column\n",
    "        dummy[0, :-1] = last_sequence.numpy().flatten() # fill with the last sequence values\n",
    "\n",
    "        # Inverse transform the predicted value\n",
    "        predicted_close = scaler.inverse_transform(dummy)[0, -1]\n",
    "\n",
    "        return predicted_close, predicted_norm"
   ],
   "id": "a5c7927b14dd7fce",
   "outputs": [],
   "execution_count": 165
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Enhannced training with early stopping and model savling\n",
    "def train_model_with_enhancements(model, train_loader, val_loader, X_data, scaler, features, num_epochs=100, patience=10, save_dir=\"models\"):\n",
    "    # create save directory if it doesn't exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    # Initialize training components\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    scheduler = torch.optim.lr_scheduler.RedduceLROnPlateau(\n",
    "        optimizer, mode=\"min\", factor=0.5, patience=3\n",
    "    )\n",
    "    # Early stopping\n",
    "    early_stopping = EarlyStopping(patience=patience, min_delta=1e-6)\n",
    "\n",
    "    # Training loop\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    print(\"Starting enhanced training with early stopping...\")\n",
    "    print(f\"Model will be saved to {save_dir}\")\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_epoch_losses = []\n",
    "        train_nan_batches = 0\n",
    "\n",
    "        for batch_idx, (xb, yb) in enumerate(train_loader):\n",
    "            # Use safe training step\n",
    "            loss_value = safe_training_step(model, criterion, optimizer, xb, yb)\n",
    "\n",
    "            if loss_value is None:\n",
    "                train_nan_batches += 1\n",
    "                continue\n",
    "            train_epoch_losses.append(loss_value)\n",
    "\n",
    "        # Validation phase\n",
    "        model.val()\n",
    "        val_epoch_losses = []\n",
    "        val_nan_batches = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                if torch.isnan(xb).any() or torch.isnan(yb).any():\n",
    "                    val_nan_batches += 1\n",
    "                    continue\n",
    "\n",
    "\n",
    "                pred = model(xb)\n",
    "                if torch.isnan(pred).any():\n",
    "                    val_nan_batches += 1\n",
    "                    continue\n",
    "\n",
    "                loss = criterion(pred, yb)\n",
    "                if not torch.isnan(loss):\n",
    "                    val_epoch_losses.append(loss.item())\n",
    "\n",
    "        # Calculate average losses\n",
    "        if train_epoch_losses and val_epoch_losses:\n",
    "            avg_train_loss = np.mean(train_epoch_losses)\n",
    "            avg_val_loss = np.mean(val_epoch_losses)\n",
    "\n",
    "            train_losses.append(avg_train_loss)\n",
    "            val_losses.append(avg_val_loss)\n",
    "\n",
    "            # Update learning rate\n",
    "            scheduler.step(avg_val_loss)\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "            # print progress\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "            print(f\" Train Loss: {avg_train_loss: .6f}\")\n",
    "            print(f\" Val Loss: {avg_val_loss: .6f}\")\n",
    "            print(f\" Learning Rate: {current_lr:.2e}\")\n",
    "\n",
    "            if train_nan_batches > 0:\n",
    "                print(f\" Skipped train batches: {train_nan_batches}\")\n",
    "            if val_nan_batches > 0:\n",
    "                print(f\" Skipped val batches: {val_nan_batches}\")\n",
    "\n",
    "            # Save the best model\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                best_model_path = os.path.join(save_dir, \"best_model.pth\")\n",
    "                save_model(model, optimizer, epoch, avg_val_loss, best_model_path)\n",
    "\n",
    "                # Make prediction with best model\n",
    "                try:\n",
    "                    predicted_price, norm_pred = make_prediction(model, X_data, scaler, features)\n",
    "                    print(f\" Prediction with best model: ${predicted_price:.2f}\")\n",
    "                except Exception as e:\n",
    "                    print(f\" Prediction failed: {e}\")\n",
    "\n",
    "            # Early stopping check\n",
    "            if early_stopping(avg_val_loss, model):\n",
    "                print(f\"\\nEarly stopping triggered after {epoch+1} epochs\")\n",
    "                print(f\"Best validation loss: {best_val_loss:.6f}\")\n",
    "                break\n",
    "        else:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}: All batches failed.\")\n",
    "            break\n",
    "\n",
    "        print(\"-\"*60)\n",
    "\n",
    "    # Save final model\n",
    "    final_model_path = os.path.join(save_dir, \"final_model.pth\")\n",
    "    if train_losses:\n",
    "        save_model(model, optimizer, epoch, train_losses[-1], final_model_path)\n",
    "\n",
    "    if os.path.exists(os.path.join(save_dir, \"best_model.pth\")):\n",
    "        load_model(model, optimizer, os.path.join(save_dir, \"best_model.pth\"))\n",
    "\n",
    "    return train_losses, val_losses, best_val_loss\n",
    "\n"
   ],
   "id": "2d0bd33f8d9faf90",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T14:58:45.116470Z",
     "start_time": "2025-06-02T14:45:33.808499Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Early Stopping Class\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, min_delta=0, restore_best_weights=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.best_weights = None\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(model)\n",
    "        elif val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            self.save_checkpoint(model)\n",
    "        else:\n",
    "            self.counter += 1\n",
    "\n",
    "        if self.counter >= self.patience:\n",
    "            if self.restore_best_weights:\n",
    "                model.load_state_dict(self.best_weights)\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def save_checkpoint(self, model):\n",
    "        \"\"\"Save model when validation loss decreases.\"\"\"\n",
    "        self.best_weights = model.state_dict().copy()\n",
    "\n",
    "# Model saving utilities\n",
    "def save_model(model, optimizer, epoch, loss, filepath, scheduler=None):\n",
    "    \"\"\"Save complete model checkpoint\"\"\"\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    if scheduler is not None:\n",
    "        checkpoint['scheduler_state_dict'] = scheduler.state_dict()\n",
    "\n",
    "    torch.save(checkpoint, filepath)\n",
    "    print(f\"✅ Model saved to {filepath}\")\n",
    "\n",
    "def load_model(model, optimizer, filepath, scheduler=None):\n",
    "    \"\"\"Load model checkpoint\"\"\"\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "    if scheduler is not None and 'scheduler_state_dict' in checkpoint:\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    print(f\"✅ Model loaded from {filepath}\")\n",
    "    print(f\"   Resumed from epoch {epoch}, loss: {loss:.6f}\")\n",
    "    return epoch, loss\n",
    "\n",
    "# Create model with proper parameter order\n",
    "model = LSTMModel(\n",
    "    input_size=len(features),\n",
    "    output_size=1,  # Specify output_size explicitly\n",
    "    hidden_size=64,\n",
    "    num_layers=2,\n",
    "    dropout=0.2  # Optional: add dropout for better stability\n",
    ")\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Add learning rate scheduler for better training stability\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=3\n",
    ")\n",
    "\n",
    "# Initialize early stopping\n",
    "early_stopping = EarlyStopping(patience=7, min_delta=1e-6)\n",
    "\n",
    "# Create directories for saving models\n",
    "save_dir = 'model_checkpoints'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Training history tracking\n",
    "train_losses = []\n",
    "best_loss = float('inf')\n",
    "start_epoch = 0\n",
    "\n",
    "# Check if there's a checkpoint to resume from\n",
    "checkpoint_path = os.path.join(save_dir, 'latest_checkpoint.pth')\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(f\"Found checkpoint at {checkpoint_path}\")\n",
    "    try:\n",
    "        start_epoch, _ = load_model(model, optimizer, checkpoint_path, scheduler)\n",
    "        start_epoch += 1  # Start from next epoch\n",
    "        print(f\"Resuming training from epoch {start_epoch}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load checkpoint: {e}\")\n",
    "        print(\"Starting training from scratch...\")\n",
    "        start_epoch = 0\n",
    "\n",
    "print(\"Starting enhanced training with NaN-safe LSTM model...\")\n",
    "print(f\"Models will be saved to: {save_dir}\")\n",
    "print(f\"Training from epoch {start_epoch + 1} to {start_epoch + 10}\")\n",
    "\n",
    "for epoch in range(start_epoch, start_epoch + 10):\n",
    "    model.train()  # Set model to training mode\n",
    "    epoch_losses = []\n",
    "    nan_batches = 0\n",
    "\n",
    "    print(f\"\\n📊 Epoch {epoch+1}/{start_epoch + 10}\")\n",
    "\n",
    "    for batch_idx, (xb, yb) in enumerate(dataloader):\n",
    "        # Input validation (the model will also check this)\n",
    "        if torch.isnan(xb).any() or torch.isnan(yb).any():\n",
    "            print(f\"⚠️  Warning: NaN in input data at batch {batch_idx}, skipping...\")\n",
    "            nan_batches += 1\n",
    "            continue\n",
    "\n",
    "        # Use the safe training step function\n",
    "        loss_value = safe_training_step(model, criterion, optimizer, xb, yb)\n",
    "\n",
    "        if loss_value is None:\n",
    "            print(f\"⚠️  Warning: Training step failed at batch {batch_idx}\")\n",
    "            nan_batches += 1\n",
    "            continue\n",
    "\n",
    "        epoch_losses.append(loss_value)\n",
    "\n",
    "        # Optional: Print progress every 100 batches\n",
    "        if batch_idx % 100 == 0 and batch_idx > 0:\n",
    "            current_avg = sum(epoch_losses[-100:]) / min(100, len(epoch_losses))\n",
    "            print(f\"   Batch {batch_idx}: Current Loss: {loss_value:.6f}, Avg(last 100): {current_avg:.6f}\")\n",
    "\n",
    "    # Calculate average loss for the epoch\n",
    "    if epoch_losses:\n",
    "        avg_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "        train_losses.append(avg_loss)\n",
    "\n",
    "        print(f\"✅ Epoch {epoch+1} completed:\")\n",
    "        print(f\"   Average Loss: {avg_loss:.6f}\")\n",
    "        print(f\"   Processed batches: {len(epoch_losses)}\")\n",
    "        if nan_batches > 0:\n",
    "            print(f\"   Skipped batches (NaN): {nan_batches}\")\n",
    "\n",
    "        # Update learning rate scheduler\n",
    "        scheduler.step(avg_loss)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"   Current Learning Rate: {current_lr:.2e}\")\n",
    "\n",
    "        # Save best model\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            best_model_path = os.path.join(save_dir, 'best_model.pth')\n",
    "            save_model(model, optimizer, epoch, avg_loss, best_model_path, scheduler)\n",
    "            print(f\"   🏆 New best model! Loss: {avg_loss:.6f}\")\n",
    "\n",
    "            # Try to make a prediction with the best model\n",
    "            try:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    # Get a sample from the dataloader for prediction demo\n",
    "                    sample_batch = next(iter(dataloader))\n",
    "                    sample_x, sample_y = sample_batch\n",
    "                    if not torch.isnan(sample_x).any():\n",
    "                        pred = model(sample_x[:1])  # Predict for first sample\n",
    "                        print(f\"   🎯 Sample prediction: {pred.item():.6f} (target: {sample_y[0].item():.6f})\")\n",
    "                model.train()\n",
    "            except Exception as e:\n",
    "                print(f\"   ⚠️  Prediction demo failed: {e}\")\n",
    "\n",
    "        # Save checkpoint every epoch\n",
    "        checkpoint_path = os.path.join(save_dir, 'latest_checkpoint.pth')\n",
    "        save_model(model, optimizer, epoch, avg_loss, checkpoint_path, scheduler)\n",
    "\n",
    "        # Check model health\n",
    "        if not check_model_weights(model):\n",
    "            print(\"❌ ERROR: Model weights corrupted with NaN/Inf. Stopping training.\")\n",
    "            break\n",
    "\n",
    "        # Early stopping check\n",
    "        if early_stopping(avg_loss, model):\n",
    "            print(f\"\\n🛑 Early stopping triggered after epoch {epoch+1}\")\n",
    "            print(f\"   Best loss achieved: {best_loss:.6f}\")\n",
    "            break\n",
    "\n",
    "    else:\n",
    "        print(f\"❌ Epoch {epoch+1}: All batches failed! Stopping training.\")\n",
    "        break\n",
    "\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "print(\"\\n🎉 Training completed!\")\n",
    "\n",
    "# Load best model for final evaluation\n",
    "best_model_path = os.path.join(save_dir, 'best_model.pth')\n",
    "if os.path.exists(best_model_path):\n",
    "    print(\"\\n🔄 Loading best model for final evaluation...\")\n",
    "    load_model(model, optimizer, best_model_path, scheduler)\n",
    "\n",
    "# Final model evaluation\n",
    "print(\"\\n📈 Final model evaluation...\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    predictions = []\n",
    "    targets = []\n",
    "\n",
    "    for xb, yb in dataloader:\n",
    "        if torch.isnan(xb).any() or torch.isnan(yb).any():\n",
    "            continue\n",
    "\n",
    "        pred = model(xb)\n",
    "        if not torch.isnan(pred).any():\n",
    "            loss = criterion(pred, yb)\n",
    "            if not torch.isnan(loss):\n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "\n",
    "                # Store some predictions for analysis\n",
    "                if len(predictions) < 10:\n",
    "                    predictions.extend(pred.cpu().numpy().flatten()[:5])\n",
    "                    targets.extend(yb.cpu().numpy().flatten()[:5])\n",
    "\n",
    "    if num_batches > 0:\n",
    "        final_avg_loss = total_loss / num_batches\n",
    "        print(f\"✅ Final average validation loss: {final_avg_loss:.6f}\")\n",
    "        print(f\"   Evaluated on {num_batches} batches\")\n",
    "\n",
    "        # Show some sample predictions vs targets\n",
    "        if predictions and targets:\n",
    "            print(\"\\n📊 Sample Predictions vs Targets:\")\n",
    "            for i, (pred, target) in enumerate(zip(predictions[:5], targets[:5])):\n",
    "                diff = abs(pred - target)\n",
    "                print(f\"   Sample {i+1}: Pred={pred:.6f}, Target={target:.6f}, Diff={diff:.6f}\")\n",
    "    else:\n",
    "        print(\"❌ No valid batches for final evaluation\")\n",
    "\n",
    "# Check final model state\n",
    "if check_model_weights(model):\n",
    "    print(\"✅ Final model weights are healthy\")\n",
    "else:\n",
    "    print(\"❌ Final model weights contain NaN/Inf\")\n",
    "\n",
    "# Training summary\n",
    "print(f\"\\n📋 Training Summary:\")\n",
    "print(f\"   Total epochs completed: {len(train_losses)}\")\n",
    "print(f\"   Best training loss: {min(train_losses) if train_losses else 'N/A':.6f}\")\n",
    "print(f\"   Final training loss: {train_losses[-1] if train_losses else 'N/A':.6f}\")\n",
    "print(f\"   Models saved in: {save_dir}\")\n",
    "\n",
    "# Save final model\n",
    "final_model_path = os.path.join(save_dir, 'final_model.pth')\n",
    "if train_losses:\n",
    "    save_model(model, optimizer, len(train_losses)-1, train_losses[-1], final_model_path, scheduler)\n",
    "\n",
    "print(f\"\\n💾 Saved models:\")\n",
    "print(f\"   • Best model: {os.path.join(save_dir, 'best_model.pth')}\")\n",
    "print(f\"   • Final model: {final_model_path}\")\n",
    "print(f\"   • Latest checkpoint: {os.path.join(save_dir, 'latest_checkpoint.pth')}\")\n",
    "\n",
    "# Prediction function for future use\n",
    "def make_prediction_with_saved_model(model_path, input_data, scaler=None, features=None):\n",
    "    \"\"\"\n",
    "    Make predictions using a saved model\n",
    "\n",
    "    Args:\n",
    "        model_path: Path to saved model\n",
    "        input_data: Input sequence for prediction\n",
    "        scaler: Optional scaler for inverse transform\n",
    "        features: Optional feature list for scaler\n",
    "    \"\"\"\n",
    "    # Load model\n",
    "    checkpoint = torch.load(model_path)\n",
    "\n",
    "    # Create new model instance (you'll need to adjust parameters as needed)\n",
    "    pred_model = LSTMModel(\n",
    "        input_size=input_data.shape[-1] if len(input_data.shape) > 1 else len(features),\n",
    "        output_size=1,\n",
    "        hidden_size=64,\n",
    "        num_layers=2,\n",
    "        dropout=0.2\n",
    "    )\n",
    "\n",
    "    pred_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    pred_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if not isinstance(input_data, torch.Tensor):\n",
    "            input_data = torch.tensor(input_data, dtype=torch.float32)\n",
    "\n",
    "        if len(input_data.shape) == 2:\n",
    "            input_data = input_data.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "        prediction = pred_model(input_data).item()\n",
    "\n",
    "        # Inverse transform if scaler provided\n",
    "        if scaler is not None and features is not None:\n",
    "            dummy = np.zeros((1, len(features) + 1))\n",
    "            dummy[0, -1] = prediction\n",
    "            actual_prediction = scaler.inverse_transform(dummy)[0, -1]\n",
    "            return actual_prediction, prediction\n",
    "\n",
    "        return prediction\n",
    "\n",
    "print(f\"\\n🎯 To make predictions later, use:\")\n",
    "print(f\"   prediction = make_prediction_with_saved_model('{best_model_path}', your_input_data)\")\n",
    "print(f\"   # Or with inverse scaling:\")\n",
    "print(f\"   actual_price, norm_pred = make_prediction_with_saved_model('{best_model_path}', your_input_data, scaler, features)\")"
   ],
   "id": "1dabd8d70664d3ab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting enhanced training with NaN-safe LSTM model...\n",
      "Models will be saved to: model_checkpoints\n",
      "Training from epoch 1 to 10\n",
      "\n",
      "📊 Epoch 1/10\n",
      "⚠️  Warning: NaN in input data at batch 52, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 63, skipping...\n",
      "   Batch 100: Current Loss: 0.601820, Avg(last 100): 0.589328\n",
      "⚠️  Warning: NaN in input data at batch 141, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 145, skipping...\n",
      "   Batch 200: Current Loss: 0.288965, Avg(last 100): 0.483820\n",
      "⚠️  Warning: NaN in input data at batch 206, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 263, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 280, skipping...\n",
      "   Batch 300: Current Loss: 0.455564, Avg(last 100): 0.467640\n",
      "⚠️  Warning: NaN in input data at batch 327, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 358, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 383, skipping...\n",
      "   Batch 400: Current Loss: 0.407235, Avg(last 100): 0.411500\n",
      "   Batch 500: Current Loss: 0.822177, Avg(last 100): 0.405119\n",
      "⚠️  Warning: NaN in input data at batch 520, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 570, skipping...\n",
      "   Batch 600: Current Loss: 0.402526, Avg(last 100): 0.410800\n",
      "⚠️  Warning: NaN in input data at batch 616, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 632, skipping...\n",
      "   Batch 700: Current Loss: 0.300243, Avg(last 100): 0.363766\n",
      "⚠️  Warning: NaN in input data at batch 746, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 770, skipping...\n",
      "   Batch 800: Current Loss: 0.172783, Avg(last 100): 0.323188\n",
      "   Batch 900: Current Loss: 0.192158, Avg(last 100): 0.314985\n",
      "⚠️  Warning: NaN in input data at batch 980, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 999, skipping...\n",
      "   Batch 1000: Current Loss: 0.160059, Avg(last 100): 0.247663\n",
      "⚠️  Warning: NaN in input data at batch 1019, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1020, skipping...\n",
      "   Batch 1100: Current Loss: 0.392816, Avg(last 100): 0.282599\n",
      "⚠️  Warning: NaN in input data at batch 1176, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1185, skipping...\n",
      "   Batch 1200: Current Loss: 0.276252, Avg(last 100): 0.225787\n",
      "⚠️  Warning: NaN in input data at batch 1236, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1241, skipping...\n",
      "   Batch 1300: Current Loss: 0.144767, Avg(last 100): 0.227757\n",
      "⚠️  Warning: NaN in input data at batch 1334, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1366, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1399, skipping...\n",
      "   Batch 1400: Current Loss: 0.057898, Avg(last 100): 0.171215\n",
      "   Batch 1500: Current Loss: 0.076460, Avg(last 100): 0.161221\n",
      "   Batch 1600: Current Loss: 0.050260, Avg(last 100): 0.105587\n",
      "⚠️  Warning: NaN in input data at batch 1660, skipping...\n",
      "   Batch 1700: Current Loss: 0.074260, Avg(last 100): 0.130335\n",
      "   Batch 1800: Current Loss: 0.054717, Avg(last 100): 0.104551\n",
      "⚠️  Warning: NaN in input data at batch 1849, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1857, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1873, skipping...\n",
      "   Batch 1900: Current Loss: 0.024231, Avg(last 100): 0.086573\n",
      "⚠️  Warning: NaN in input data at batch 1937, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1996, skipping...\n",
      "   Batch 2000: Current Loss: 0.082421, Avg(last 100): 0.067428\n",
      "⚠️  Warning: NaN in input data at batch 2014, skipping...\n",
      "   Batch 2100: Current Loss: 0.029318, Avg(last 100): 0.073525\n",
      "⚠️  Warning: NaN in input data at batch 2130, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2173, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2178, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2180, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2186, skipping...\n",
      "   Batch 2200: Current Loss: 0.098221, Avg(last 100): 0.066770\n",
      "   Batch 2300: Current Loss: 0.021419, Avg(last 100): 0.057608\n",
      "⚠️  Warning: NaN in input data at batch 2301, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2363, skipping...\n",
      "   Batch 2400: Current Loss: 0.016703, Avg(last 100): 0.045176\n",
      "⚠️  Warning: NaN in input data at batch 2410, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2485, skipping...\n",
      "   Batch 2500: Current Loss: 0.040685, Avg(last 100): 0.030460\n",
      "⚠️  Warning: NaN in input data at batch 2508, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2593, skipping...\n",
      "   Batch 2600: Current Loss: 0.118014, Avg(last 100): 0.030254\n",
      "⚠️  Warning: NaN in input data at batch 2688, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2690, skipping...\n",
      "   Batch 2700: Current Loss: 0.007448, Avg(last 100): 0.020071\n",
      "⚠️  Warning: NaN in input data at batch 2701, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2733, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2780, skipping...\n",
      "   Batch 2800: Current Loss: 0.015105, Avg(last 100): 0.018477\n",
      "⚠️  Warning: NaN in input data at batch 2821, skipping...\n",
      "   Batch 2900: Current Loss: 0.003981, Avg(last 100): 0.014328\n",
      "⚠️  Warning: NaN in input data at batch 2919, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2953, skipping...\n",
      "   Batch 3000: Current Loss: 0.008917, Avg(last 100): 0.014977\n",
      "⚠️  Warning: NaN in input data at batch 3027, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3077, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3081, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3088, skipping...\n",
      "   Batch 3100: Current Loss: 0.006644, Avg(last 100): 0.008422\n",
      "⚠️  Warning: NaN in input data at batch 3107, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3151, skipping...\n",
      "   Batch 3200: Current Loss: 0.038837, Avg(last 100): 0.011108\n",
      "⚠️  Warning: NaN in input data at batch 3254, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3260, skipping...\n",
      "   Batch 3300: Current Loss: 0.041426, Avg(last 100): 0.012105\n",
      "⚠️  Warning: NaN in input data at batch 3376, skipping...\n",
      "   Batch 3400: Current Loss: 0.004221, Avg(last 100): 0.007254\n",
      "⚠️  Warning: NaN in input data at batch 3478, skipping...\n",
      "   Batch 3500: Current Loss: 0.001961, Avg(last 100): 0.009493\n",
      "   Batch 3600: Current Loss: 0.008038, Avg(last 100): 0.006345\n",
      "⚠️  Warning: NaN in input data at batch 3611, skipping...\n",
      "   Batch 3700: Current Loss: 0.006635, Avg(last 100): 0.005781\n",
      "⚠️  Warning: NaN in input data at batch 3770, skipping...\n",
      "   Batch 3800: Current Loss: 0.036100, Avg(last 100): 0.006906\n",
      "⚠️  Warning: NaN in input data at batch 3889, skipping...\n",
      "   Batch 3900: Current Loss: 0.003338, Avg(last 100): 0.004814\n",
      "⚠️  Warning: NaN in input data at batch 3951, skipping...\n",
      "   Batch 4000: Current Loss: 0.002577, Avg(last 100): 0.004705\n",
      "⚠️  Warning: NaN in input data at batch 4091, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4100, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4142, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4163, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4194, skipping...\n",
      "   Batch 4200: Current Loss: 0.001850, Avg(last 100): 0.004988\n",
      "⚠️  Warning: NaN in input data at batch 4299, skipping...\n",
      "   Batch 4300: Current Loss: 0.001790, Avg(last 100): 0.006007\n",
      "⚠️  Warning: NaN in input data at batch 4321, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4328, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4347, skipping...\n",
      "   Batch 4400: Current Loss: 0.001913, Avg(last 100): 0.004775\n",
      "⚠️  Warning: NaN in input data at batch 4404, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4444, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4449, skipping...\n",
      "   Batch 4500: Current Loss: 0.001005, Avg(last 100): 0.003561\n",
      "⚠️  Warning: NaN in input data at batch 4512, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4533, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4546, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4548, skipping...\n",
      "   Batch 4600: Current Loss: 0.002184, Avg(last 100): 0.005047\n",
      "⚠️  Warning: NaN in input data at batch 4620, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4639, skipping...\n",
      "   Batch 4700: Current Loss: 0.000754, Avg(last 100): 0.004133\n",
      "⚠️  Warning: NaN in input data at batch 4774, skipping...\n",
      "   Batch 4800: Current Loss: 0.002340, Avg(last 100): 0.003500\n",
      "⚠️  Warning: NaN in input data at batch 4870, skipping...\n",
      "   Batch 4900: Current Loss: 0.004484, Avg(last 100): 0.003841\n",
      "   Batch 5000: Current Loss: 0.007121, Avg(last 100): 0.003760\n",
      "⚠️  Warning: NaN in input data at batch 5089, skipping...\n",
      "   Batch 5100: Current Loss: 0.001282, Avg(last 100): 0.003521\n",
      "⚠️  Warning: NaN in input data at batch 5125, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5158, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5181, skipping...\n",
      "   Batch 5200: Current Loss: 0.003536, Avg(last 100): 0.003394\n",
      "⚠️  Warning: NaN in input data at batch 5258, skipping...\n",
      "   Batch 5300: Current Loss: 0.000771, Avg(last 100): 0.004405\n",
      "   Batch 5400: Current Loss: 0.000958, Avg(last 100): 0.002867\n",
      "⚠️  Warning: NaN in input data at batch 5469, skipping...\n",
      "   Batch 5500: Current Loss: 0.001798, Avg(last 100): 0.003114\n",
      "⚠️  Warning: NaN in input data at batch 5514, skipping...\n",
      "   Batch 5600: Current Loss: 0.002123, Avg(last 100): 0.003588\n",
      "   Batch 5700: Current Loss: 0.000712, Avg(last 100): 0.003134\n",
      "⚠️  Warning: NaN in input data at batch 5800, skipping...\n",
      "   Batch 5900: Current Loss: 0.004419, Avg(last 100): 0.003912\n",
      "⚠️  Warning: NaN in input data at batch 5973, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5975, skipping...\n",
      "   Batch 6000: Current Loss: 0.002526, Avg(last 100): 0.002428\n",
      "⚠️  Warning: NaN in input data at batch 6048, skipping...\n",
      "   Batch 6100: Current Loss: 0.011478, Avg(last 100): 0.002714\n",
      "   Batch 6200: Current Loss: 0.005022, Avg(last 100): 0.003485\n",
      "⚠️  Warning: NaN in input data at batch 6205, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6221, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6283, skipping...\n",
      "   Batch 6300: Current Loss: 0.003739, Avg(last 100): 0.003339\n",
      "⚠️  Warning: NaN in input data at batch 6366, skipping...\n",
      "   Batch 6400: Current Loss: 0.001209, Avg(last 100): 0.002557\n",
      "⚠️  Warning: NaN in input data at batch 6435, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6499, skipping...\n",
      "   Batch 6500: Current Loss: 0.004167, Avg(last 100): 0.002549\n",
      "⚠️  Warning: NaN in input data at batch 6522, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6545, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6558, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6575, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6600, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6648, skipping...\n",
      "   Batch 6700: Current Loss: 0.000468, Avg(last 100): 0.002641\n",
      "⚠️  Warning: NaN in input data at batch 6712, skipping...\n",
      "   Batch 6800: Current Loss: 0.008607, Avg(last 100): 0.002110\n",
      "⚠️  Warning: NaN in input data at batch 6803, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6875, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6885, skipping...\n",
      "   Batch 6900: Current Loss: 0.001448, Avg(last 100): 0.003173\n",
      "⚠️  Warning: NaN in input data at batch 6901, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6922, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6975, skipping...\n",
      "   Batch 7000: Current Loss: 0.004451, Avg(last 100): 0.002577\n",
      "⚠️  Warning: NaN in input data at batch 7018, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7025, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7060, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7065, skipping...\n",
      "   Batch 7100: Current Loss: 0.000878, Avg(last 100): 0.002441\n",
      "⚠️  Warning: NaN in input data at batch 7149, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7185, skipping...\n",
      "   Batch 7200: Current Loss: 0.001958, Avg(last 100): 0.002800\n",
      "   Batch 7300: Current Loss: 0.002602, Avg(last 100): 0.002686\n",
      "⚠️  Warning: NaN in input data at batch 7373, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7387, skipping...\n",
      "   Batch 7400: Current Loss: 0.000626, Avg(last 100): 0.002942\n",
      "⚠️  Warning: NaN in input data at batch 7475, skipping...\n",
      "   Batch 7500: Current Loss: 0.003087, Avg(last 100): 0.002710\n",
      "⚠️  Warning: NaN in input data at batch 7516, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7520, skipping...\n",
      "   Batch 7600: Current Loss: 0.016603, Avg(last 100): 0.002369\n",
      "   Batch 7700: Current Loss: 0.001489, Avg(last 100): 0.002133\n",
      "⚠️  Warning: NaN in input data at batch 7779, skipping...\n",
      "   Batch 7800: Current Loss: 0.002060, Avg(last 100): 0.002513\n",
      "⚠️  Warning: NaN in input data at batch 7899, skipping...\n",
      "   Batch 7900: Current Loss: 0.000922, Avg(last 100): 0.002827\n",
      "⚠️  Warning: NaN in input data at batch 7929, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7948, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7955, skipping...\n",
      "   Batch 8000: Current Loss: 0.001339, Avg(last 100): 0.003024\n",
      "⚠️  Warning: NaN in input data at batch 8029, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8054, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8073, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8085, skipping...\n",
      "   Batch 8100: Current Loss: 0.006779, Avg(last 100): 0.002904\n",
      "⚠️  Warning: NaN in input data at batch 8171, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8189, skipping...\n",
      "   Batch 8200: Current Loss: 0.002668, Avg(last 100): 0.002415\n",
      "⚠️  Warning: NaN in input data at batch 8260, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8271, skipping...\n",
      "   Batch 8300: Current Loss: 0.001387, Avg(last 100): 0.002254\n",
      "⚠️  Warning: NaN in input data at batch 8375, skipping...\n",
      "   Batch 8400: Current Loss: 0.000739, Avg(last 100): 0.002140\n",
      "⚠️  Warning: NaN in input data at batch 8438, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8442, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8453, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8454, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8484, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8485, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8491, skipping...\n",
      "   Batch 8500: Current Loss: 0.006790, Avg(last 100): 0.002921\n",
      "⚠️  Warning: NaN in input data at batch 8502, skipping...\n",
      "   Batch 8600: Current Loss: 0.001228, Avg(last 100): 0.002409\n",
      "⚠️  Warning: NaN in input data at batch 8605, skipping...\n",
      "   Batch 8700: Current Loss: 0.004090, Avg(last 100): 0.003039\n",
      "⚠️  Warning: NaN in input data at batch 8796, skipping...\n",
      "   Batch 8800: Current Loss: 0.000922, Avg(last 100): 0.002556\n",
      "⚠️  Warning: NaN in input data at batch 8826, skipping...\n",
      "   Batch 8900: Current Loss: 0.003071, Avg(last 100): 0.002889\n",
      "⚠️  Warning: NaN in input data at batch 8940, skipping...\n",
      "   Batch 9000: Current Loss: 0.002357, Avg(last 100): 0.002209\n",
      "⚠️  Warning: NaN in input data at batch 9005, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 9037, skipping...\n",
      "   Batch 9100: Current Loss: 0.001298, Avg(last 100): 0.002896\n",
      "⚠️  Warning: NaN in input data at batch 9104, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 9132, skipping...\n",
      "   Batch 9200: Current Loss: 0.004772, Avg(last 100): 0.002685\n",
      "⚠️  Warning: NaN in input data at batch 9282, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 9299, skipping...\n",
      "   Batch 9300: Current Loss: 0.000591, Avg(last 100): 0.002390\n",
      "⚠️  Warning: NaN in input data at batch 9360, skipping...\n",
      "   Batch 9400: Current Loss: 0.000702, Avg(last 100): 0.002490\n",
      "⚠️  Warning: NaN in input data at batch 9442, skipping...\n",
      "   Batch 9500: Current Loss: 0.002717, Avg(last 100): 0.002401\n",
      "⚠️  Warning: NaN in input data at batch 9584, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 9591, skipping...\n",
      "   Batch 9600: Current Loss: 0.001664, Avg(last 100): 0.002134\n",
      "✅ Epoch 1 completed:\n",
      "   Average Loss: 0.063963\n",
      "   Processed batches: 9509\n",
      "   Skipped batches (NaN): 164\n",
      "   Current Learning Rate: 1.00e-04\n",
      "✅ Model saved to model_checkpoints/best_model.pth\n",
      "   🏆 New best model! Loss: 0.063963\n",
      "   🎯 Sample prediction: -0.085655 (target: 0.033027)\n",
      "✅ Model saved to model_checkpoints/latest_checkpoint.pth\n",
      "------------------------------------------------------------\n",
      "\n",
      "📊 Epoch 2/10\n",
      "⚠️  Warning: NaN in input data at batch 41, skipping...\n",
      "   Batch 100: Current Loss: 0.001946, Avg(last 100): 0.002338\n",
      "   Batch 200: Current Loss: 0.002474, Avg(last 100): 0.002492\n",
      "⚠️  Warning: NaN in input data at batch 204, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 219, skipping...\n",
      "   Batch 300: Current Loss: 0.000667, Avg(last 100): 0.003434\n",
      "⚠️  Warning: NaN in input data at batch 310, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 399, skipping...\n",
      "   Batch 400: Current Loss: 0.003674, Avg(last 100): 0.002557\n",
      "⚠️  Warning: NaN in input data at batch 427, skipping...\n",
      "   Batch 500: Current Loss: 0.001424, Avg(last 100): 0.002537\n",
      "⚠️  Warning: NaN in input data at batch 520, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 544, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 545, skipping...\n",
      "   Batch 600: Current Loss: 0.002756, Avg(last 100): 0.002799\n",
      "⚠️  Warning: NaN in input data at batch 647, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 654, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 676, skipping...\n",
      "   Batch 700: Current Loss: 0.002618, Avg(last 100): 0.002373\n",
      "⚠️  Warning: NaN in input data at batch 747, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 762, skipping...\n",
      "   Batch 800: Current Loss: 0.000931, Avg(last 100): 0.002065\n",
      "⚠️  Warning: NaN in input data at batch 802, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 817, skipping...\n",
      "   Batch 900: Current Loss: 0.003956, Avg(last 100): 0.002366\n",
      "⚠️  Warning: NaN in input data at batch 952, skipping...\n",
      "   Batch 1000: Current Loss: 0.000924, Avg(last 100): 0.002718\n",
      "⚠️  Warning: NaN in input data at batch 1002, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1025, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1066, skipping...\n",
      "   Batch 1100: Current Loss: 0.000648, Avg(last 100): 0.001954\n",
      "⚠️  Warning: NaN in input data at batch 1177, skipping...\n",
      "   Batch 1200: Current Loss: 0.001940, Avg(last 100): 0.002261\n",
      "⚠️  Warning: NaN in input data at batch 1206, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1244, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1247, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1279, skipping...\n",
      "   Batch 1300: Current Loss: 0.006030, Avg(last 100): 0.002592\n",
      "⚠️  Warning: NaN in input data at batch 1395, skipping...\n",
      "   Batch 1400: Current Loss: 0.001835, Avg(last 100): 0.002102\n",
      "   Batch 1500: Current Loss: 0.000864, Avg(last 100): 0.002034\n",
      "⚠️  Warning: NaN in input data at batch 1534, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1590, skipping...\n",
      "   Batch 1600: Current Loss: 0.003623, Avg(last 100): 0.001943\n",
      "⚠️  Warning: NaN in input data at batch 1642, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1679, skipping...\n",
      "   Batch 1700: Current Loss: 0.002662, Avg(last 100): 0.002043\n",
      "⚠️  Warning: NaN in input data at batch 1744, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1748, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1799, skipping...\n",
      "   Batch 1800: Current Loss: 0.002009, Avg(last 100): 0.001946\n",
      "   Batch 1900: Current Loss: 0.001230, Avg(last 100): 0.002816\n",
      "   Batch 2000: Current Loss: 0.000519, Avg(last 100): 0.002187\n",
      "⚠️  Warning: NaN in input data at batch 2080, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2098, skipping...\n",
      "   Batch 2100: Current Loss: 0.000507, Avg(last 100): 0.002209\n",
      "⚠️  Warning: NaN in input data at batch 2133, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2196, skipping...\n",
      "   Batch 2200: Current Loss: 0.004798, Avg(last 100): 0.002356\n",
      "⚠️  Warning: NaN in input data at batch 2230, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2235, skipping...\n",
      "   Batch 2300: Current Loss: 0.001203, Avg(last 100): 0.002050\n",
      "⚠️  Warning: NaN in input data at batch 2317, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2370, skipping...\n",
      "   Batch 2400: Current Loss: 0.000879, Avg(last 100): 0.002194\n",
      "⚠️  Warning: NaN in input data at batch 2453, skipping...\n",
      "   Batch 2500: Current Loss: 0.001007, Avg(last 100): 0.002595\n",
      "⚠️  Warning: NaN in input data at batch 2554, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2581, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2586, skipping...\n",
      "   Batch 2600: Current Loss: 0.000905, Avg(last 100): 0.002159\n",
      "⚠️  Warning: NaN in input data at batch 2602, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2616, skipping...\n",
      "   Batch 2700: Current Loss: 0.001716, Avg(last 100): 0.002494\n",
      "⚠️  Warning: NaN in input data at batch 2727, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2758, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2765, skipping...\n",
      "   Batch 2800: Current Loss: 0.000585, Avg(last 100): 0.002082\n",
      "   Batch 2900: Current Loss: 0.001218, Avg(last 100): 0.002464\n",
      "   Batch 3000: Current Loss: 0.000650, Avg(last 100): 0.002492\n",
      "   Batch 3100: Current Loss: 0.001976, Avg(last 100): 0.002157\n",
      "⚠️  Warning: NaN in input data at batch 3148, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3186, skipping...\n",
      "   Batch 3200: Current Loss: 0.000552, Avg(last 100): 0.002486\n",
      "⚠️  Warning: NaN in input data at batch 3212, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3232, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3286, skipping...\n",
      "   Batch 3300: Current Loss: 0.000567, Avg(last 100): 0.002416\n",
      "⚠️  Warning: NaN in input data at batch 3322, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3377, skipping...\n",
      "   Batch 3400: Current Loss: 0.001568, Avg(last 100): 0.002442\n",
      "⚠️  Warning: NaN in input data at batch 3433, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3466, skipping...\n",
      "   Batch 3500: Current Loss: 0.000566, Avg(last 100): 0.001928\n",
      "⚠️  Warning: NaN in input data at batch 3546, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3549, skipping...\n",
      "   Batch 3600: Current Loss: 0.000914, Avg(last 100): 0.002130\n",
      "⚠️  Warning: NaN in input data at batch 3671, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3680, skipping...\n",
      "   Batch 3700: Current Loss: 0.001930, Avg(last 100): 0.002098\n",
      "⚠️  Warning: NaN in input data at batch 3713, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3758, skipping...\n",
      "   Batch 3800: Current Loss: 0.000453, Avg(last 100): 0.002337\n",
      "⚠️  Warning: NaN in input data at batch 3803, skipping...\n",
      "   Batch 3900: Current Loss: 0.000574, Avg(last 100): 0.001940\n",
      "⚠️  Warning: NaN in input data at batch 3903, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3910, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3954, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3961, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3994, skipping...\n",
      "   Batch 4000: Current Loss: 0.002304, Avg(last 100): 0.002386\n",
      "⚠️  Warning: NaN in input data at batch 4092, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4097, skipping...\n",
      "   Batch 4100: Current Loss: 0.001351, Avg(last 100): 0.001733\n",
      "⚠️  Warning: NaN in input data at batch 4135, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4159, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4175, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4199, skipping...\n",
      "   Batch 4200: Current Loss: 0.002146, Avg(last 100): 0.002554\n",
      "   Batch 4300: Current Loss: 0.000984, Avg(last 100): 0.002156\n",
      "   Batch 4400: Current Loss: 0.000909, Avg(last 100): 0.002085\n",
      "⚠️  Warning: NaN in input data at batch 4436, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4454, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4469, skipping...\n",
      "   Batch 4500: Current Loss: 0.001848, Avg(last 100): 0.002105\n",
      "   Batch 4600: Current Loss: 0.004015, Avg(last 100): 0.002101\n",
      "⚠️  Warning: NaN in input data at batch 4618, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4619, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4634, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4679, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4681, skipping...\n",
      "   Batch 4700: Current Loss: 0.002209, Avg(last 100): 0.001900\n",
      "⚠️  Warning: NaN in input data at batch 4710, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4759, skipping...\n",
      "   Batch 4800: Current Loss: 0.002686, Avg(last 100): 0.002004\n",
      "⚠️  Warning: NaN in input data at batch 4890, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4900, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4921, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4937, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4950, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4965, skipping...\n",
      "   Batch 5000: Current Loss: 0.000828, Avg(last 100): 0.002187\n",
      "   Batch 5100: Current Loss: 0.006874, Avg(last 100): 0.002595\n",
      "⚠️  Warning: NaN in input data at batch 5118, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5167, skipping...\n",
      "   Batch 5200: Current Loss: 0.000948, Avg(last 100): 0.001882\n",
      "⚠️  Warning: NaN in input data at batch 5222, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5240, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5252, skipping...\n",
      "   Batch 5300: Current Loss: 0.000343, Avg(last 100): 0.002673\n",
      "   Batch 5400: Current Loss: 0.006794, Avg(last 100): 0.001926\n",
      "   Batch 5500: Current Loss: 0.000752, Avg(last 100): 0.002191\n",
      "   Batch 5600: Current Loss: 0.014508, Avg(last 100): 0.002565\n",
      "⚠️  Warning: NaN in input data at batch 5612, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5639, skipping...\n",
      "   Batch 5700: Current Loss: 0.000718, Avg(last 100): 0.002337\n",
      "   Batch 5800: Current Loss: 0.004711, Avg(last 100): 0.002464\n",
      "⚠️  Warning: NaN in input data at batch 5815, skipping...\n",
      "   Batch 5900: Current Loss: 0.001109, Avg(last 100): 0.001969\n",
      "⚠️  Warning: NaN in input data at batch 5958, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5983, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5990, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5994, skipping...\n",
      "   Batch 6000: Current Loss: 0.002581, Avg(last 100): 0.001771\n",
      "   Batch 6100: Current Loss: 0.000439, Avg(last 100): 0.002108\n",
      "⚠️  Warning: NaN in input data at batch 6112, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6133, skipping...\n",
      "   Batch 6200: Current Loss: 0.006240, Avg(last 100): 0.002221\n",
      "⚠️  Warning: NaN in input data at batch 6219, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6235, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6239, skipping...\n",
      "   Batch 6300: Current Loss: 0.000468, Avg(last 100): 0.002365\n",
      "⚠️  Warning: NaN in input data at batch 6371, skipping...\n",
      "   Batch 6400: Current Loss: 0.014622, Avg(last 100): 0.002430\n",
      "⚠️  Warning: NaN in input data at batch 6446, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6462, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6468, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6470, skipping...\n",
      "   Batch 6500: Current Loss: 0.001520, Avg(last 100): 0.002857\n",
      "⚠️  Warning: NaN in input data at batch 6510, skipping...\n",
      "   Batch 6600: Current Loss: 0.000598, Avg(last 100): 0.002472\n",
      "⚠️  Warning: NaN in input data at batch 6691, skipping...\n",
      "   Batch 6700: Current Loss: 0.002829, Avg(last 100): 0.002724\n",
      "⚠️  Warning: NaN in input data at batch 6749, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6759, skipping...\n",
      "   Batch 6800: Current Loss: 0.002608, Avg(last 100): 0.002357\n",
      "⚠️  Warning: NaN in input data at batch 6853, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6863, skipping...\n",
      "   Batch 6900: Current Loss: 0.000980, Avg(last 100): 0.002176\n",
      "⚠️  Warning: NaN in input data at batch 6905, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6911, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6930, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6941, skipping...\n",
      "   Batch 7000: Current Loss: 0.000664, Avg(last 100): 0.002140\n",
      "   Batch 7100: Current Loss: 0.002627, Avg(last 100): 0.001910\n",
      "⚠️  Warning: NaN in input data at batch 7129, skipping...\n",
      "   Batch 7200: Current Loss: 0.000466, Avg(last 100): 0.002514\n",
      "⚠️  Warning: NaN in input data at batch 7224, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7273, skipping...\n",
      "   Batch 7300: Current Loss: 0.002224, Avg(last 100): 0.002445\n",
      "⚠️  Warning: NaN in input data at batch 7346, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7359, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7380, skipping...\n",
      "   Batch 7400: Current Loss: 0.000710, Avg(last 100): 0.001899\n",
      "   Batch 7500: Current Loss: 0.002767, Avg(last 100): 0.002988\n",
      "⚠️  Warning: NaN in input data at batch 7586, skipping...\n",
      "   Batch 7600: Current Loss: 0.000752, Avg(last 100): 0.002846\n",
      "   Batch 7700: Current Loss: 0.000775, Avg(last 100): 0.002514\n",
      "⚠️  Warning: NaN in input data at batch 7724, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7726, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7776, skipping...\n",
      "   Batch 7800: Current Loss: 0.000740, Avg(last 100): 0.002054\n",
      "⚠️  Warning: NaN in input data at batch 7801, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7897, skipping...\n",
      "   Batch 7900: Current Loss: 0.002109, Avg(last 100): 0.002375\n",
      "   Batch 8000: Current Loss: 0.011748, Avg(last 100): 0.002489\n",
      "⚠️  Warning: NaN in input data at batch 8013, skipping...\n",
      "   Batch 8100: Current Loss: 0.002015, Avg(last 100): 0.002229\n",
      "⚠️  Warning: NaN in input data at batch 8126, skipping...\n",
      "   Batch 8200: Current Loss: 0.002297, Avg(last 100): 0.001805\n",
      "⚠️  Warning: NaN in input data at batch 8217, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8292, skipping...\n",
      "   Batch 8300: Current Loss: 0.000845, Avg(last 100): 0.002385\n",
      "⚠️  Warning: NaN in input data at batch 8307, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8339, skipping...\n",
      "   Batch 8400: Current Loss: 0.000962, Avg(last 100): 0.002743\n",
      "⚠️  Warning: NaN in input data at batch 8424, skipping...\n",
      "   Batch 8500: Current Loss: 0.001049, Avg(last 100): 0.002193\n",
      "⚠️  Warning: NaN in input data at batch 8575, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8586, skipping...\n",
      "   Batch 8600: Current Loss: 0.000521, Avg(last 100): 0.002074\n",
      "⚠️  Warning: NaN in input data at batch 8695, skipping...\n",
      "   Batch 8700: Current Loss: 0.001943, Avg(last 100): 0.002452\n",
      "⚠️  Warning: NaN in input data at batch 8759, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8777, skipping...\n",
      "   Batch 8800: Current Loss: 0.004619, Avg(last 100): 0.002657\n",
      "⚠️  Warning: NaN in input data at batch 8839, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8872, skipping...\n",
      "   Batch 8900: Current Loss: 0.001303, Avg(last 100): 0.002505\n",
      "⚠️  Warning: NaN in input data at batch 8908, skipping...\n",
      "   Batch 9000: Current Loss: 0.001112, Avg(last 100): 0.002199\n",
      "⚠️  Warning: NaN in input data at batch 9041, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 9062, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 9067, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 9091, skipping...\n",
      "   Batch 9100: Current Loss: 0.000354, Avg(last 100): 0.002278\n",
      "⚠️  Warning: NaN in input data at batch 9118, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 9130, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 9153, skipping...\n",
      "   Batch 9200: Current Loss: 0.001134, Avg(last 100): 0.002308\n",
      "   Batch 9300: Current Loss: 0.000524, Avg(last 100): 0.002131\n",
      "⚠️  Warning: NaN in input data at batch 9351, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 9357, skipping...\n",
      "   Batch 9400: Current Loss: 0.001366, Avg(last 100): 0.002298\n",
      "⚠️  Warning: NaN in input data at batch 9401, skipping...\n",
      "   Batch 9500: Current Loss: 0.003778, Avg(last 100): 0.002282\n",
      "⚠️  Warning: NaN in input data at batch 9531, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 9540, skipping...\n",
      "   Batch 9600: Current Loss: 0.005194, Avg(last 100): 0.002683\n",
      "✅ Epoch 2 completed:\n",
      "   Average Loss: 0.002312\n",
      "   Processed batches: 9509\n",
      "   Skipped batches (NaN): 164\n",
      "   Current Learning Rate: 1.00e-04\n",
      "✅ Model saved to model_checkpoints/best_model.pth\n",
      "   🏆 New best model! Loss: 0.002312\n",
      "   🎯 Sample prediction: -0.013929 (target: 0.011331)\n",
      "✅ Model saved to model_checkpoints/latest_checkpoint.pth\n",
      "------------------------------------------------------------\n",
      "\n",
      "📊 Epoch 3/10\n",
      "⚠️  Warning: NaN in input data at batch 45, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 68, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 90, skipping...\n",
      "   Batch 100: Current Loss: 0.000668, Avg(last 100): 0.002063\n",
      "⚠️  Warning: NaN in input data at batch 168, skipping...\n",
      "   Batch 200: Current Loss: 0.004140, Avg(last 100): 0.002234\n",
      "⚠️  Warning: NaN in input data at batch 267, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 271, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 299, skipping...\n",
      "   Batch 300: Current Loss: 0.000646, Avg(last 100): 0.002512\n",
      "   Batch 400: Current Loss: 0.005544, Avg(last 100): 0.002042\n",
      "⚠️  Warning: NaN in input data at batch 453, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 454, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 484, skipping...\n",
      "   Batch 500: Current Loss: 0.000619, Avg(last 100): 0.002499\n",
      "   Batch 600: Current Loss: 0.001653, Avg(last 100): 0.002572\n",
      "   Batch 700: Current Loss: 0.002267, Avg(last 100): 0.002170\n",
      "   Batch 800: Current Loss: 0.001953, Avg(last 100): 0.002027\n",
      "⚠️  Warning: NaN in input data at batch 893, skipping...\n",
      "   Batch 900: Current Loss: 0.002265, Avg(last 100): 0.002826\n",
      "⚠️  Warning: NaN in input data at batch 938, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 947, skipping...\n",
      "   Batch 1000: Current Loss: 0.002430, Avg(last 100): 0.002235\n",
      "⚠️  Warning: NaN in input data at batch 1028, skipping...\n",
      "   Batch 1100: Current Loss: 0.001744, Avg(last 100): 0.002654\n",
      "⚠️  Warning: NaN in input data at batch 1166, skipping...\n",
      "   Batch 1200: Current Loss: 0.001131, Avg(last 100): 0.002414\n",
      "   Batch 1300: Current Loss: 0.000555, Avg(last 100): 0.002058\n",
      "⚠️  Warning: NaN in input data at batch 1333, skipping...\n",
      "   Batch 1400: Current Loss: 0.000729, Avg(last 100): 0.002367\n",
      "⚠️  Warning: NaN in input data at batch 1492, skipping...\n",
      "   Batch 1500: Current Loss: 0.005172, Avg(last 100): 0.002389\n",
      "⚠️  Warning: NaN in input data at batch 1541, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1542, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1588, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1599, skipping...\n",
      "   Batch 1600: Current Loss: 0.001930, Avg(last 100): 0.002380\n",
      "⚠️  Warning: NaN in input data at batch 1628, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1673, skipping...\n",
      "   Batch 1700: Current Loss: 0.002743, Avg(last 100): 0.002448\n",
      "⚠️  Warning: NaN in input data at batch 1759, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1799, skipping...\n",
      "   Batch 1800: Current Loss: 0.000967, Avg(last 100): 0.001609\n",
      "⚠️  Warning: NaN in input data at batch 1818, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1832, skipping...\n",
      "   Batch 1900: Current Loss: 0.002053, Avg(last 100): 0.002170\n",
      "⚠️  Warning: NaN in input data at batch 1916, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1976, skipping...\n",
      "   Batch 2000: Current Loss: 0.000568, Avg(last 100): 0.002885\n",
      "⚠️  Warning: NaN in input data at batch 2004, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2084, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2092, skipping...\n",
      "   Batch 2100: Current Loss: 0.000843, Avg(last 100): 0.001657\n",
      "⚠️  Warning: NaN in input data at batch 2115, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2180, skipping...\n",
      "   Batch 2200: Current Loss: 0.001491, Avg(last 100): 0.002141\n",
      "⚠️  Warning: NaN in input data at batch 2205, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2230, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2276, skipping...\n",
      "   Batch 2300: Current Loss: 0.001370, Avg(last 100): 0.002446\n",
      "⚠️  Warning: NaN in input data at batch 2304, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2365, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2374, skipping...\n",
      "   Batch 2400: Current Loss: 0.000882, Avg(last 100): 0.002262\n",
      "⚠️  Warning: NaN in input data at batch 2426, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2495, skipping...\n",
      "   Batch 2500: Current Loss: 0.000654, Avg(last 100): 0.002418\n",
      "⚠️  Warning: NaN in input data at batch 2546, skipping...\n",
      "   Batch 2600: Current Loss: 0.000383, Avg(last 100): 0.002138\n",
      "⚠️  Warning: NaN in input data at batch 2619, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2697, skipping...\n",
      "   Batch 2700: Current Loss: 0.001008, Avg(last 100): 0.002064\n",
      "⚠️  Warning: NaN in input data at batch 2702, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2706, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2765, skipping...\n",
      "   Batch 2800: Current Loss: 0.002312, Avg(last 100): 0.001951\n",
      "⚠️  Warning: NaN in input data at batch 2822, skipping...\n",
      "   Batch 2900: Current Loss: 0.002626, Avg(last 100): 0.002186\n",
      "⚠️  Warning: NaN in input data at batch 2930, skipping...\n",
      "   Batch 3000: Current Loss: 0.005831, Avg(last 100): 0.002306\n",
      "⚠️  Warning: NaN in input data at batch 3015, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3087, skipping...\n",
      "   Batch 3100: Current Loss: 0.001519, Avg(last 100): 0.002236\n",
      "⚠️  Warning: NaN in input data at batch 3118, skipping...\n",
      "   Batch 3200: Current Loss: 0.005836, Avg(last 100): 0.002095\n",
      "⚠️  Warning: NaN in input data at batch 3205, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3218, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3265, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3288, skipping...\n",
      "   Batch 3300: Current Loss: 0.002649, Avg(last 100): 0.002433\n",
      "   Batch 3400: Current Loss: 0.001345, Avg(last 100): 0.001738\n",
      "⚠️  Warning: NaN in input data at batch 3435, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3446, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3487, skipping...\n",
      "   Batch 3500: Current Loss: 0.001691, Avg(last 100): 0.002355\n",
      "⚠️  Warning: NaN in input data at batch 3517, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3539, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3568, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3588, skipping...\n",
      "   Batch 3600: Current Loss: 0.000888, Avg(last 100): 0.002426\n",
      "   Batch 3700: Current Loss: 0.006046, Avg(last 100): 0.002147\n",
      "⚠️  Warning: NaN in input data at batch 3722, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3733, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3758, skipping...\n",
      "   Batch 3800: Current Loss: 0.001921, Avg(last 100): 0.002431\n",
      "   Batch 3900: Current Loss: 0.001315, Avg(last 100): 0.002668\n",
      "⚠️  Warning: NaN in input data at batch 3931, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3948, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3949, skipping...\n",
      "   Batch 4000: Current Loss: 0.003257, Avg(last 100): 0.002008\n",
      "⚠️  Warning: NaN in input data at batch 4025, skipping...\n",
      "   Batch 4100: Current Loss: 0.000721, Avg(last 100): 0.002302\n",
      "⚠️  Warning: NaN in input data at batch 4113, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4141, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4193, skipping...\n",
      "   Batch 4200: Current Loss: 0.007080, Avg(last 100): 0.002270\n",
      "⚠️  Warning: NaN in input data at batch 4257, skipping...\n",
      "   Batch 4300: Current Loss: 0.000456, Avg(last 100): 0.002644\n",
      "⚠️  Warning: NaN in input data at batch 4388, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4399, skipping...\n",
      "   Batch 4400: Current Loss: 0.003461, Avg(last 100): 0.001949\n",
      "⚠️  Warning: NaN in input data at batch 4449, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4495, skipping...\n",
      "   Batch 4500: Current Loss: 0.000742, Avg(last 100): 0.002502\n",
      "⚠️  Warning: NaN in input data at batch 4513, skipping...\n",
      "   Batch 4600: Current Loss: 0.000700, Avg(last 100): 0.001676\n",
      "⚠️  Warning: NaN in input data at batch 4615, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4668, skipping...\n",
      "   Batch 4700: Current Loss: 0.000976, Avg(last 100): 0.001655\n",
      "⚠️  Warning: NaN in input data at batch 4703, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4714, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4719, skipping...\n",
      "   Batch 4800: Current Loss: 0.000830, Avg(last 100): 0.002135\n",
      "⚠️  Warning: NaN in input data at batch 4849, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4861, skipping...\n",
      "   Batch 4900: Current Loss: 0.000706, Avg(last 100): 0.001912\n",
      "⚠️  Warning: NaN in input data at batch 4919, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4968, skipping...\n",
      "   Batch 5000: Current Loss: 0.000502, Avg(last 100): 0.001874\n",
      "⚠️  Warning: NaN in input data at batch 5003, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5059, skipping...\n",
      "   Batch 5100: Current Loss: 0.002461, Avg(last 100): 0.001890\n",
      "⚠️  Warning: NaN in input data at batch 5165, skipping...\n",
      "   Batch 5200: Current Loss: 0.010658, Avg(last 100): 0.002240\n",
      "⚠️  Warning: NaN in input data at batch 5202, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5240, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5248, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5291, skipping...\n",
      "   Batch 5300: Current Loss: 0.000733, Avg(last 100): 0.001892\n",
      "   Batch 5400: Current Loss: 0.000580, Avg(last 100): 0.002172\n",
      "⚠️  Warning: NaN in input data at batch 5435, skipping...\n",
      "   Batch 5500: Current Loss: 0.000407, Avg(last 100): 0.002323\n",
      "⚠️  Warning: NaN in input data at batch 5533, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5583, skipping...\n",
      "   Batch 5600: Current Loss: 0.000883, Avg(last 100): 0.002058\n",
      "   Batch 5700: Current Loss: 0.001712, Avg(last 100): 0.002414\n",
      "⚠️  Warning: NaN in input data at batch 5722, skipping...\n",
      "   Batch 5800: Current Loss: 0.001173, Avg(last 100): 0.002138\n",
      "⚠️  Warning: NaN in input data at batch 5803, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5821, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5830, skipping...\n",
      "   Batch 5900: Current Loss: 0.008878, Avg(last 100): 0.002452\n",
      "⚠️  Warning: NaN in input data at batch 5913, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5919, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5921, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5942, skipping...\n",
      "   Batch 6000: Current Loss: 0.000550, Avg(last 100): 0.002301\n",
      "⚠️  Warning: NaN in input data at batch 6032, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6034, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6063, skipping...\n",
      "   Batch 6100: Current Loss: 0.002667, Avg(last 100): 0.003151\n",
      "⚠️  Warning: NaN in input data at batch 6109, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6146, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6191, skipping...\n",
      "   Batch 6200: Current Loss: 0.000442, Avg(last 100): 0.002058\n",
      "⚠️  Warning: NaN in input data at batch 6277, skipping...\n",
      "   Batch 6300: Current Loss: 0.002766, Avg(last 100): 0.001869\n",
      "⚠️  Warning: NaN in input data at batch 6305, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6315, skipping...\n",
      "   Batch 6400: Current Loss: 0.003709, Avg(last 100): 0.002525\n",
      "⚠️  Warning: NaN in input data at batch 6418, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6481, skipping...\n",
      "   Batch 6500: Current Loss: 0.000697, Avg(last 100): 0.002888\n",
      "⚠️  Warning: NaN in input data at batch 6571, skipping...\n",
      "   Batch 6600: Current Loss: 0.000849, Avg(last 100): 0.001813\n",
      "⚠️  Warning: NaN in input data at batch 6604, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6661, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6687, skipping...\n",
      "   Batch 6700: Current Loss: 0.000547, Avg(last 100): 0.002812\n",
      "⚠️  Warning: NaN in input data at batch 6707, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6788, skipping...\n",
      "   Batch 6800: Current Loss: 0.000491, Avg(last 100): 0.002150\n",
      "   Batch 6900: Current Loss: 0.001929, Avg(last 100): 0.002342\n",
      "⚠️  Warning: NaN in input data at batch 6990, skipping...\n",
      "   Batch 7000: Current Loss: 0.000469, Avg(last 100): 0.002193\n",
      "⚠️  Warning: NaN in input data at batch 7070, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7076, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7081, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7097, skipping...\n",
      "   Batch 7100: Current Loss: 0.001083, Avg(last 100): 0.002321\n",
      "⚠️  Warning: NaN in input data at batch 7110, skipping...\n",
      "   Batch 7200: Current Loss: 0.002282, Avg(last 100): 0.001949\n",
      "   Batch 7300: Current Loss: 0.000670, Avg(last 100): 0.002319\n",
      "⚠️  Warning: NaN in input data at batch 7367, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7385, skipping...\n",
      "   Batch 7400: Current Loss: 0.011377, Avg(last 100): 0.002847\n",
      "⚠️  Warning: NaN in input data at batch 7455, skipping...\n",
      "   Batch 7500: Current Loss: 0.000741, Avg(last 100): 0.002421\n",
      "⚠️  Warning: NaN in input data at batch 7532, skipping...\n",
      "   Batch 7600: Current Loss: 0.009207, Avg(last 100): 0.002137\n",
      "   Batch 7700: Current Loss: 0.001205, Avg(last 100): 0.001900\n",
      "   Batch 7800: Current Loss: 0.002890, Avg(last 100): 0.002623\n",
      "   Batch 7900: Current Loss: 0.002049, Avg(last 100): 0.002258\n",
      "⚠️  Warning: NaN in input data at batch 7926, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7947, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7969, skipping...\n",
      "   Batch 8000: Current Loss: 0.000666, Avg(last 100): 0.002139\n",
      "⚠️  Warning: NaN in input data at batch 8018, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8085, skipping...\n",
      "   Batch 8100: Current Loss: 0.000586, Avg(last 100): 0.002826\n",
      "⚠️  Warning: NaN in input data at batch 8199, skipping...\n",
      "   Batch 8200: Current Loss: 0.000720, Avg(last 100): 0.002084\n",
      "⚠️  Warning: NaN in input data at batch 8293, skipping...\n",
      "   Batch 8300: Current Loss: 0.002688, Avg(last 100): 0.002330\n",
      "⚠️  Warning: NaN in input data at batch 8356, skipping...\n",
      "   Batch 8400: Current Loss: 0.000424, Avg(last 100): 0.002252\n",
      "   Batch 8500: Current Loss: 0.000771, Avg(last 100): 0.002037\n",
      "⚠️  Warning: NaN in input data at batch 8526, skipping...\n",
      "   Batch 8600: Current Loss: 0.001011, Avg(last 100): 0.002139\n",
      "⚠️  Warning: NaN in input data at batch 8614, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8630, skipping...\n",
      "   Batch 8700: Current Loss: 0.000527, Avg(last 100): 0.002213\n",
      "⚠️  Warning: NaN in input data at batch 8791, skipping...\n",
      "   Batch 8800: Current Loss: 0.000548, Avg(last 100): 0.002191\n",
      "   Batch 8900: Current Loss: 0.002111, Avg(last 100): 0.002221\n",
      "⚠️  Warning: NaN in input data at batch 8964, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8970, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8975, skipping...\n",
      "   Batch 9000: Current Loss: 0.002493, Avg(last 100): 0.002716\n",
      "⚠️  Warning: NaN in input data at batch 9020, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 9034, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 9091, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 9094, skipping...\n",
      "   Batch 9100: Current Loss: 0.000815, Avg(last 100): 0.002016\n",
      "⚠️  Warning: NaN in input data at batch 9151, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 9155, skipping...\n",
      "   Batch 9200: Current Loss: 0.000485, Avg(last 100): 0.002340\n",
      "⚠️  Warning: NaN in input data at batch 9265, skipping...\n",
      "   Batch 9300: Current Loss: 0.002261, Avg(last 100): 0.002593\n",
      "⚠️  Warning: NaN in input data at batch 9363, skipping...\n",
      "   Batch 9400: Current Loss: 0.005469, Avg(last 100): 0.002295\n",
      "⚠️  Warning: NaN in input data at batch 9402, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 9453, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 9462, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 9479, skipping...\n",
      "   Batch 9500: Current Loss: 0.002164, Avg(last 100): 0.002040\n",
      "⚠️  Warning: NaN in input data at batch 9573, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 9594, skipping...\n",
      "   Batch 9600: Current Loss: 0.001532, Avg(last 100): 0.002559\n",
      "✅ Epoch 3 completed:\n",
      "   Average Loss: 0.002260\n",
      "   Processed batches: 9510\n",
      "   Skipped batches (NaN): 163\n",
      "   Current Learning Rate: 1.00e-04\n",
      "✅ Model saved to model_checkpoints/best_model.pth\n",
      "   🏆 New best model! Loss: 0.002260\n",
      "   🎯 Sample prediction: 0.016533 (target: 0.039557)\n",
      "✅ Model saved to model_checkpoints/latest_checkpoint.pth\n",
      "------------------------------------------------------------\n",
      "\n",
      "📊 Epoch 4/10\n",
      "   Batch 100: Current Loss: 0.001968, Avg(last 100): 0.002436\n",
      "⚠️  Warning: NaN in input data at batch 118, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 130, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 162, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 188, skipping...\n",
      "   Batch 200: Current Loss: 0.003657, Avg(last 100): 0.002756\n",
      "⚠️  Warning: NaN in input data at batch 203, skipping...\n",
      "   Batch 300: Current Loss: 0.002773, Avg(last 100): 0.002097\n",
      "⚠️  Warning: NaN in input data at batch 311, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 314, skipping...\n",
      "   Batch 400: Current Loss: 0.007006, Avg(last 100): 0.002634\n",
      "⚠️  Warning: NaN in input data at batch 489, skipping...\n",
      "   Batch 500: Current Loss: 0.000504, Avg(last 100): 0.002332\n",
      "⚠️  Warning: NaN in input data at batch 571, skipping...\n",
      "   Batch 600: Current Loss: 0.001000, Avg(last 100): 0.002087\n",
      "⚠️  Warning: NaN in input data at batch 620, skipping...\n",
      "   Batch 700: Current Loss: 0.000823, Avg(last 100): 0.002556\n",
      "⚠️  Warning: NaN in input data at batch 730, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 761, skipping...\n",
      "   Batch 800: Current Loss: 0.002584, Avg(last 100): 0.002210\n",
      "   Batch 900: Current Loss: 0.000424, Avg(last 100): 0.002401\n",
      "⚠️  Warning: NaN in input data at batch 978, skipping...\n",
      "   Batch 1000: Current Loss: 0.001523, Avg(last 100): 0.002076\n",
      "⚠️  Warning: NaN in input data at batch 1025, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1059, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1072, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1095, skipping...\n",
      "   Batch 1100: Current Loss: 0.000648, Avg(last 100): 0.002286\n",
      "⚠️  Warning: NaN in input data at batch 1116, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1185, skipping...\n",
      "   Batch 1200: Current Loss: 0.000752, Avg(last 100): 0.002166\n",
      "⚠️  Warning: NaN in input data at batch 1212, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1231, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1249, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1283, skipping...\n",
      "   Batch 1300: Current Loss: 0.001681, Avg(last 100): 0.002641\n",
      "⚠️  Warning: NaN in input data at batch 1304, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1324, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1325, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1378, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1381, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1387, skipping...\n",
      "   Batch 1400: Current Loss: 0.000479, Avg(last 100): 0.002157\n",
      "⚠️  Warning: NaN in input data at batch 1476, skipping...\n",
      "   Batch 1500: Current Loss: 0.000492, Avg(last 100): 0.001789\n",
      "   Batch 1600: Current Loss: 0.001013, Avg(last 100): 0.002772\n",
      "⚠️  Warning: NaN in input data at batch 1603, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1646, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1654, skipping...\n",
      "   Batch 1700: Current Loss: 0.006258, Avg(last 100): 0.002193\n",
      "   Batch 1800: Current Loss: 0.000906, Avg(last 100): 0.001917\n",
      "⚠️  Warning: NaN in input data at batch 1859, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1882, skipping...\n",
      "   Batch 1900: Current Loss: 0.003652, Avg(last 100): 0.002672\n",
      "   Batch 2000: Current Loss: 0.000752, Avg(last 100): 0.002703\n",
      "⚠️  Warning: NaN in input data at batch 2008, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2019, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2028, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2031, skipping...\n",
      "   Batch 2100: Current Loss: 0.002723, Avg(last 100): 0.002321\n",
      "⚠️  Warning: NaN in input data at batch 2146, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2157, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2176, skipping...\n",
      "   Batch 2200: Current Loss: 0.001037, Avg(last 100): 0.002508\n",
      "   Batch 2300: Current Loss: 0.005572, Avg(last 100): 0.002616\n",
      "⚠️  Warning: NaN in input data at batch 2310, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2320, skipping...\n",
      "   Batch 2400: Current Loss: 0.000971, Avg(last 100): 0.002111\n",
      "⚠️  Warning: NaN in input data at batch 2410, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2424, skipping...\n",
      "   Batch 2500: Current Loss: 0.002715, Avg(last 100): 0.002399\n",
      "⚠️  Warning: NaN in input data at batch 2530, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2541, skipping...\n",
      "   Batch 2600: Current Loss: 0.000563, Avg(last 100): 0.002522\n",
      "⚠️  Warning: NaN in input data at batch 2657, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2665, skipping...\n",
      "   Batch 2700: Current Loss: 0.002766, Avg(last 100): 0.002394\n",
      "⚠️  Warning: NaN in input data at batch 2703, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2719, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2728, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2754, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2760, skipping...\n",
      "   Batch 2800: Current Loss: 0.001478, Avg(last 100): 0.002090\n",
      "⚠️  Warning: NaN in input data at batch 2833, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2882, skipping...\n",
      "   Batch 2900: Current Loss: 0.001391, Avg(last 100): 0.001735\n",
      "⚠️  Warning: NaN in input data at batch 2948, skipping...\n",
      "   Batch 3000: Current Loss: 0.002675, Avg(last 100): 0.002290\n",
      "⚠️  Warning: NaN in input data at batch 3052, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3077, skipping...\n",
      "   Batch 3100: Current Loss: 0.001964, Avg(last 100): 0.002168\n",
      "⚠️  Warning: NaN in input data at batch 3116, skipping...\n",
      "   Batch 3200: Current Loss: 0.000763, Avg(last 100): 0.002319\n",
      "⚠️  Warning: NaN in input data at batch 3207, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3255, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3261, skipping...\n",
      "   Batch 3300: Current Loss: 0.003558, Avg(last 100): 0.002375\n",
      "⚠️  Warning: NaN in input data at batch 3368, skipping...\n",
      "   Batch 3400: Current Loss: 0.004828, Avg(last 100): 0.002137\n",
      "⚠️  Warning: NaN in input data at batch 3466, skipping...\n",
      "   Batch 3500: Current Loss: 0.007274, Avg(last 100): 0.002108\n",
      "⚠️  Warning: NaN in input data at batch 3548, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3568, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3592, skipping...\n",
      "   Batch 3600: Current Loss: 0.001311, Avg(last 100): 0.001664\n",
      "⚠️  Warning: NaN in input data at batch 3603, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3650, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3672, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3684, skipping...\n",
      "   Batch 3700: Current Loss: 0.001313, Avg(last 100): 0.002531\n",
      "   Batch 3800: Current Loss: 0.001950, Avg(last 100): 0.002415\n",
      "⚠️  Warning: NaN in input data at batch 3826, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3875, skipping...\n",
      "   Batch 3900: Current Loss: 0.000981, Avg(last 100): 0.002153\n",
      "   Batch 4000: Current Loss: 0.000616, Avg(last 100): 0.002265\n",
      "⚠️  Warning: NaN in input data at batch 4005, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4075, skipping...\n",
      "   Batch 4100: Current Loss: 0.002124, Avg(last 100): 0.001983\n",
      "⚠️  Warning: NaN in input data at batch 4156, skipping...\n",
      "   Batch 4200: Current Loss: 0.001558, Avg(last 100): 0.002423\n",
      "⚠️  Warning: NaN in input data at batch 4274, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4299, skipping...\n",
      "   Batch 4300: Current Loss: 0.000798, Avg(last 100): 0.001720\n",
      "   Batch 4400: Current Loss: 0.000684, Avg(last 100): 0.002102\n",
      "   Batch 4500: Current Loss: 0.000755, Avg(last 100): 0.002485\n",
      "⚠️  Warning: NaN in input data at batch 4554, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4589, skipping...\n",
      "   Batch 4600: Current Loss: 0.000495, Avg(last 100): 0.002389\n",
      "⚠️  Warning: NaN in input data at batch 4602, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4646, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4688, skipping...\n",
      "   Batch 4700: Current Loss: 0.000494, Avg(last 100): 0.002420\n",
      "⚠️  Warning: NaN in input data at batch 4777, skipping...\n",
      "   Batch 4800: Current Loss: 0.000983, Avg(last 100): 0.002146\n",
      "⚠️  Warning: NaN in input data at batch 4824, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4866, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4880, skipping...\n",
      "   Batch 4900: Current Loss: 0.003274, Avg(last 100): 0.002751\n",
      "   Batch 5000: Current Loss: 0.002592, Avg(last 100): 0.002355\n",
      "   Batch 5100: Current Loss: 0.004028, Avg(last 100): 0.002392\n",
      "   Batch 5200: Current Loss: 0.002095, Avg(last 100): 0.002513\n",
      "⚠️  Warning: NaN in input data at batch 5257, skipping...\n",
      "   Batch 5300: Current Loss: 0.001392, Avg(last 100): 0.002149\n",
      "⚠️  Warning: NaN in input data at batch 5316, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5318, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5320, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5379, skipping...\n",
      "   Batch 5400: Current Loss: 0.003089, Avg(last 100): 0.002615\n",
      "⚠️  Warning: NaN in input data at batch 5459, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5461, skipping...\n",
      "   Batch 5500: Current Loss: 0.000641, Avg(last 100): 0.002427\n",
      "⚠️  Warning: NaN in input data at batch 5525, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5573, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5595, skipping...\n",
      "   Batch 5600: Current Loss: 0.001807, Avg(last 100): 0.002363\n",
      "⚠️  Warning: NaN in input data at batch 5626, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5664, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5671, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5699, skipping...\n",
      "   Batch 5700: Current Loss: 0.002336, Avg(last 100): 0.002307\n",
      "⚠️  Warning: NaN in input data at batch 5771, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5780, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5799, skipping...\n",
      "   Batch 5800: Current Loss: 0.001251, Avg(last 100): 0.002208\n",
      "   Batch 5900: Current Loss: 0.005270, Avg(last 100): 0.002145\n",
      "   Batch 6000: Current Loss: 0.000690, Avg(last 100): 0.002127\n",
      "⚠️  Warning: NaN in input data at batch 6055, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6068, skipping...\n",
      "   Batch 6100: Current Loss: 0.000976, Avg(last 100): 0.002086\n",
      "   Batch 6200: Current Loss: 0.002993, Avg(last 100): 0.002801\n",
      "⚠️  Warning: NaN in input data at batch 6240, skipping...\n",
      "   Batch 6300: Current Loss: 0.000882, Avg(last 100): 0.002231\n",
      "⚠️  Warning: NaN in input data at batch 6315, skipping...\n",
      "   Batch 6400: Current Loss: 0.002100, Avg(last 100): 0.001695\n",
      "⚠️  Warning: NaN in input data at batch 6412, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6449, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6483, skipping...\n",
      "   Batch 6500: Current Loss: 0.000548, Avg(last 100): 0.001982\n",
      "⚠️  Warning: NaN in input data at batch 6554, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6556, skipping...\n",
      "   Batch 6600: Current Loss: 0.002190, Avg(last 100): 0.001928\n",
      "⚠️  Warning: NaN in input data at batch 6611, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6637, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6645, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6664, skipping...\n",
      "   Batch 6700: Current Loss: 0.000654, Avg(last 100): 0.002460\n",
      "⚠️  Warning: NaN in input data at batch 6755, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6787, skipping...\n",
      "   Batch 6800: Current Loss: 0.000530, Avg(last 100): 0.001728\n",
      "   Batch 6900: Current Loss: 0.000997, Avg(last 100): 0.002061\n",
      "   Batch 7000: Current Loss: 0.000823, Avg(last 100): 0.002542\n",
      "⚠️  Warning: NaN in input data at batch 7005, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7034, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7049, skipping...\n",
      "   Batch 7100: Current Loss: 0.000725, Avg(last 100): 0.002404\n",
      "⚠️  Warning: NaN in input data at batch 7119, skipping...\n",
      "   Batch 7200: Current Loss: 0.000976, Avg(last 100): 0.001832\n",
      "⚠️  Warning: NaN in input data at batch 7204, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7258, skipping...\n",
      "   Batch 7300: Current Loss: 0.000418, Avg(last 100): 0.002130\n",
      "⚠️  Warning: NaN in input data at batch 7340, skipping...\n",
      "   Batch 7400: Current Loss: 0.001726, Avg(last 100): 0.002458\n",
      "⚠️  Warning: NaN in input data at batch 7433, skipping...\n",
      "   Batch 7500: Current Loss: 0.001997, Avg(last 100): 0.002255\n",
      "⚠️  Warning: NaN in input data at batch 7531, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7553, skipping...\n",
      "   Batch 7600: Current Loss: 0.000683, Avg(last 100): 0.002075\n",
      "⚠️  Warning: NaN in input data at batch 7608, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7687, skipping...\n",
      "   Batch 7700: Current Loss: 0.002337, Avg(last 100): 0.002021\n",
      "⚠️  Warning: NaN in input data at batch 7752, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7792, skipping...\n",
      "   Batch 7800: Current Loss: 0.000469, Avg(last 100): 0.002046\n",
      "⚠️  Warning: NaN in input data at batch 7856, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7884, skipping...\n",
      "   Batch 7900: Current Loss: 0.000610, Avg(last 100): 0.002113\n",
      "⚠️  Warning: NaN in input data at batch 7902, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7911, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7936, skipping...\n",
      "   Batch 8000: Current Loss: 0.004190, Avg(last 100): 0.002470\n",
      "⚠️  Warning: NaN in input data at batch 8026, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8031, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8064, skipping...\n",
      "   Batch 8100: Current Loss: 0.005010, Avg(last 100): 0.002243\n",
      "⚠️  Warning: NaN in input data at batch 8158, skipping...\n",
      "   Batch 8200: Current Loss: 0.000617, Avg(last 100): 0.001712\n",
      "   Batch 8300: Current Loss: 0.000695, Avg(last 100): 0.002219\n",
      "⚠️  Warning: NaN in input data at batch 8324, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8350, skipping...\n",
      "   Batch 8400: Current Loss: 0.002791, Avg(last 100): 0.001603\n",
      "⚠️  Warning: NaN in input data at batch 8427, skipping...\n",
      "   Batch 8500: Current Loss: 0.000755, Avg(last 100): 0.002141\n",
      "⚠️  Warning: NaN in input data at batch 8505, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8515, skipping...\n",
      "   Batch 8600: Current Loss: 0.001641, Avg(last 100): 0.002151\n",
      "⚠️  Warning: NaN in input data at batch 8611, skipping...\n",
      "   Batch 8700: Current Loss: 0.001010, Avg(last 100): 0.002012\n",
      "⚠️  Warning: NaN in input data at batch 8710, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8730, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8742, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8753, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8795, skipping...\n",
      "   Batch 8800: Current Loss: 0.010120, Avg(last 100): 0.002516\n",
      "⚠️  Warning: NaN in input data at batch 8874, skipping...\n",
      "   Batch 8900: Current Loss: 0.001079, Avg(last 100): 0.002614\n",
      "   Batch 9000: Current Loss: 0.002087, Avg(last 100): 0.002324\n",
      "⚠️  Warning: NaN in input data at batch 9031, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 9082, skipping...\n",
      "   Batch 9100: Current Loss: 0.002376, Avg(last 100): 0.002860\n",
      "   Batch 9200: Current Loss: 0.000769, Avg(last 100): 0.002074\n",
      "⚠️  Warning: NaN in input data at batch 9204, skipping...\n",
      "   Batch 9300: Current Loss: 0.000581, Avg(last 100): 0.002148\n",
      "⚠️  Warning: NaN in input data at batch 9307, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 9311, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 9392, skipping...\n",
      "   Batch 9400: Current Loss: 0.006486, Avg(last 100): 0.002007\n",
      "⚠️  Warning: NaN in input data at batch 9425, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 9444, skipping...\n",
      "   Batch 9500: Current Loss: 0.002398, Avg(last 100): 0.002334\n",
      "   Batch 9600: Current Loss: 0.002492, Avg(last 100): 0.002598\n",
      "⚠️  Warning: NaN in input data at batch 9640, skipping...\n",
      "✅ Epoch 4 completed:\n",
      "   Average Loss: 0.002263\n",
      "   Processed batches: 9508\n",
      "   Skipped batches (NaN): 165\n",
      "   Current Learning Rate: 1.00e-04\n",
      "✅ Model saved to model_checkpoints/latest_checkpoint.pth\n",
      "------------------------------------------------------------\n",
      "\n",
      "📊 Epoch 5/10\n",
      "⚠️  Warning: NaN in input data at batch 21, skipping...\n",
      "   Batch 100: Current Loss: 0.004217, Avg(last 100): 0.002475\n",
      "⚠️  Warning: NaN in input data at batch 200, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 236, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 286, skipping...\n",
      "   Batch 300: Current Loss: 0.002499, Avg(last 100): 0.001803\n",
      "   Batch 400: Current Loss: 0.001375, Avg(last 100): 0.002347\n",
      "⚠️  Warning: NaN in input data at batch 493, skipping...\n",
      "   Batch 500: Current Loss: 0.006440, Avg(last 100): 0.002218\n",
      "⚠️  Warning: NaN in input data at batch 517, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 539, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 542, skipping...\n",
      "   Batch 600: Current Loss: 0.001112, Avg(last 100): 0.002430\n",
      "   Batch 700: Current Loss: 0.000964, Avg(last 100): 0.002485\n",
      "⚠️  Warning: NaN in input data at batch 714, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 795, skipping...\n",
      "   Batch 800: Current Loss: 0.000798, Avg(last 100): 0.002296\n",
      "⚠️  Warning: NaN in input data at batch 826, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 840, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 859, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 888, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 896, skipping...\n",
      "   Batch 900: Current Loss: 0.000332, Avg(last 100): 0.002151\n",
      "⚠️  Warning: NaN in input data at batch 918, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 965, skipping...\n",
      "   Batch 1000: Current Loss: 0.002980, Avg(last 100): 0.002067\n",
      "⚠️  Warning: NaN in input data at batch 1018, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1096, skipping...\n",
      "   Batch 1100: Current Loss: 0.002997, Avg(last 100): 0.002488\n",
      "⚠️  Warning: NaN in input data at batch 1132, skipping...\n",
      "   Batch 1200: Current Loss: 0.000441, Avg(last 100): 0.002102\n",
      "⚠️  Warning: NaN in input data at batch 1276, skipping...\n",
      "   Batch 1300: Current Loss: 0.005688, Avg(last 100): 0.002121\n",
      "⚠️  Warning: NaN in input data at batch 1323, skipping...\n",
      "   Batch 1400: Current Loss: 0.000521, Avg(last 100): 0.002279\n",
      "⚠️  Warning: NaN in input data at batch 1464, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1467, skipping...\n",
      "   Batch 1500: Current Loss: 0.003331, Avg(last 100): 0.002241\n",
      "⚠️  Warning: NaN in input data at batch 1539, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1553, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1587, skipping...\n",
      "   Batch 1600: Current Loss: 0.000688, Avg(last 100): 0.002423\n",
      "   Batch 1700: Current Loss: 0.001420, Avg(last 100): 0.002518\n",
      "⚠️  Warning: NaN in input data at batch 1800, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1804, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1842, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1856, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1873, skipping...\n",
      "   Batch 1900: Current Loss: 0.000717, Avg(last 100): 0.002748\n",
      "⚠️  Warning: NaN in input data at batch 1939, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1997, skipping...\n",
      "   Batch 2000: Current Loss: 0.000742, Avg(last 100): 0.001979\n",
      "⚠️  Warning: NaN in input data at batch 2020, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2066, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2075, skipping...\n",
      "   Batch 2100: Current Loss: 0.000805, Avg(last 100): 0.002402\n",
      "⚠️  Warning: NaN in input data at batch 2114, skipping...\n",
      "   Batch 2200: Current Loss: 0.000514, Avg(last 100): 0.002204\n",
      "⚠️  Warning: NaN in input data at batch 2205, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2215, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2219, skipping...\n",
      "   Batch 2300: Current Loss: 0.001293, Avg(last 100): 0.002236\n",
      "   Batch 2400: Current Loss: 0.006489, Avg(last 100): 0.002269\n",
      "⚠️  Warning: NaN in input data at batch 2429, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2451, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2490, skipping...\n",
      "   Batch 2500: Current Loss: 0.002803, Avg(last 100): 0.002661\n",
      "⚠️  Warning: NaN in input data at batch 2555, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2573, skipping...\n",
      "   Batch 2600: Current Loss: 0.000703, Avg(last 100): 0.002276\n",
      "⚠️  Warning: NaN in input data at batch 2666, skipping...\n",
      "   Batch 2700: Current Loss: 0.006478, Avg(last 100): 0.002232\n",
      "⚠️  Warning: NaN in input data at batch 2781, skipping...\n",
      "   Batch 2800: Current Loss: 0.004890, Avg(last 100): 0.002465\n",
      "⚠️  Warning: NaN in input data at batch 2811, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2818, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2883, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2888, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2894, skipping...\n",
      "   Batch 2900: Current Loss: 0.008307, Avg(last 100): 0.002442\n",
      "⚠️  Warning: NaN in input data at batch 2905, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2980, skipping...\n",
      "   Batch 3000: Current Loss: 0.001795, Avg(last 100): 0.002569\n",
      "⚠️  Warning: NaN in input data at batch 3031, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3055, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3069, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3071, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3079, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3092, skipping...\n",
      "   Batch 3100: Current Loss: 0.001300, Avg(last 100): 0.002281\n",
      "   Batch 3200: Current Loss: 0.004520, Avg(last 100): 0.002482\n",
      "⚠️  Warning: NaN in input data at batch 3223, skipping...\n",
      "   Batch 3300: Current Loss: 0.026621, Avg(last 100): 0.003118\n",
      "   Batch 3400: Current Loss: 0.000676, Avg(last 100): 0.002190\n",
      "⚠️  Warning: NaN in input data at batch 3469, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3473, skipping...\n",
      "   Batch 3500: Current Loss: 0.001490, Avg(last 100): 0.002068\n",
      "⚠️  Warning: NaN in input data at batch 3536, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3583, skipping...\n",
      "   Batch 3600: Current Loss: 0.013467, Avg(last 100): 0.002036\n",
      "⚠️  Warning: NaN in input data at batch 3614, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3638, skipping...\n",
      "   Batch 3700: Current Loss: 0.004572, Avg(last 100): 0.002804\n",
      "⚠️  Warning: NaN in input data at batch 3724, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3779, skipping...\n",
      "   Batch 3800: Current Loss: 0.000656, Avg(last 100): 0.002305\n",
      "⚠️  Warning: NaN in input data at batch 3845, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3878, skipping...\n",
      "   Batch 3900: Current Loss: 0.003266, Avg(last 100): 0.002015\n",
      "⚠️  Warning: NaN in input data at batch 3911, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3927, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3969, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3994, skipping...\n",
      "   Batch 4000: Current Loss: 0.004627, Avg(last 100): 0.002289\n",
      "⚠️  Warning: NaN in input data at batch 4049, skipping...\n",
      "   Batch 4100: Current Loss: 0.003361, Avg(last 100): 0.002324\n",
      "⚠️  Warning: NaN in input data at batch 4106, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4178, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4181, skipping...\n",
      "   Batch 4200: Current Loss: 0.001795, Avg(last 100): 0.002334\n",
      "⚠️  Warning: NaN in input data at batch 4208, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4217, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4298, skipping...\n",
      "   Batch 4300: Current Loss: 0.000619, Avg(last 100): 0.002715\n",
      "⚠️  Warning: NaN in input data at batch 4390, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4399, skipping...\n",
      "   Batch 4400: Current Loss: 0.000987, Avg(last 100): 0.002425\n",
      "⚠️  Warning: NaN in input data at batch 4407, skipping...\n",
      "   Batch 4500: Current Loss: 0.000542, Avg(last 100): 0.002531\n",
      "⚠️  Warning: NaN in input data at batch 4555, skipping...\n",
      "   Batch 4600: Current Loss: 0.000980, Avg(last 100): 0.002288\n",
      "⚠️  Warning: NaN in input data at batch 4625, skipping...\n",
      "   Batch 4700: Current Loss: 0.000301, Avg(last 100): 0.002075\n",
      "   Batch 4800: Current Loss: 0.000903, Avg(last 100): 0.002416\n",
      "⚠️  Warning: NaN in input data at batch 4866, skipping...\n",
      "   Batch 4900: Current Loss: 0.000612, Avg(last 100): 0.002156\n",
      "⚠️  Warning: NaN in input data at batch 4929, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4963, skipping...\n",
      "   Batch 5000: Current Loss: 0.001980, Avg(last 100): 0.002242\n",
      "⚠️  Warning: NaN in input data at batch 5060, skipping...\n",
      "   Batch 5100: Current Loss: 0.000370, Avg(last 100): 0.001874\n",
      "⚠️  Warning: NaN in input data at batch 5162, skipping...\n",
      "   Batch 5200: Current Loss: 0.000972, Avg(last 100): 0.001805\n",
      "⚠️  Warning: NaN in input data at batch 5252, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5268, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5283, skipping...\n",
      "   Batch 5300: Current Loss: 0.001025, Avg(last 100): 0.002050\n",
      "⚠️  Warning: NaN in input data at batch 5317, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5398, skipping...\n",
      "   Batch 5400: Current Loss: 0.012717, Avg(last 100): 0.002577\n",
      "⚠️  Warning: NaN in input data at batch 5461, skipping...\n",
      "   Batch 5500: Current Loss: 0.006907, Avg(last 100): 0.002275\n",
      "⚠️  Warning: NaN in input data at batch 5501, skipping...\n",
      "   Batch 5600: Current Loss: 0.002056, Avg(last 100): 0.002585\n",
      "⚠️  Warning: NaN in input data at batch 5610, skipping...\n",
      "   Batch 5700: Current Loss: 0.001808, Avg(last 100): 0.002200\n",
      "   Batch 5800: Current Loss: 0.002304, Avg(last 100): 0.002213\n",
      "⚠️  Warning: NaN in input data at batch 5817, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5823, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5838, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5865, skipping...\n",
      "   Batch 5900: Current Loss: 0.003063, Avg(last 100): 0.002712\n",
      "⚠️  Warning: NaN in input data at batch 5935, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5989, skipping...\n",
      "   Batch 6000: Current Loss: 0.000917, Avg(last 100): 0.002125\n",
      "⚠️  Warning: NaN in input data at batch 6036, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6063, skipping...\n",
      "   Batch 6100: Current Loss: 0.004033, Avg(last 100): 0.002191\n",
      "   Batch 6200: Current Loss: 0.000623, Avg(last 100): 0.002401\n",
      "⚠️  Warning: NaN in input data at batch 6214, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6215, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6279, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6299, skipping...\n",
      "   Batch 6300: Current Loss: 0.000786, Avg(last 100): 0.002439\n",
      "⚠️  Warning: NaN in input data at batch 6340, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6363, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6374, skipping...\n",
      "   Batch 6400: Current Loss: 0.001068, Avg(last 100): 0.002240\n",
      "⚠️  Warning: NaN in input data at batch 6493, skipping...\n",
      "   Batch 6500: Current Loss: 0.000561, Avg(last 100): 0.002207\n",
      "⚠️  Warning: NaN in input data at batch 6503, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6575, skipping...\n",
      "   Batch 6600: Current Loss: 0.000458, Avg(last 100): 0.002107\n",
      "⚠️  Warning: NaN in input data at batch 6608, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6628, skipping...\n",
      "   Batch 6700: Current Loss: 0.000408, Avg(last 100): 0.002212\n",
      "⚠️  Warning: NaN in input data at batch 6712, skipping...\n",
      "   Batch 6800: Current Loss: 0.004566, Avg(last 100): 0.002082\n",
      "⚠️  Warning: NaN in input data at batch 6836, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6880, skipping...\n",
      "   Batch 6900: Current Loss: 0.009156, Avg(last 100): 0.002637\n",
      "⚠️  Warning: NaN in input data at batch 6938, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6976, skipping...\n",
      "   Batch 7000: Current Loss: 0.001720, Avg(last 100): 0.002058\n",
      "⚠️  Warning: NaN in input data at batch 7042, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7048, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7052, skipping...\n",
      "   Batch 7100: Current Loss: 0.000425, Avg(last 100): 0.001935\n",
      "   Batch 7200: Current Loss: 0.001468, Avg(last 100): 0.001886\n",
      "   Batch 7300: Current Loss: 0.001897, Avg(last 100): 0.002485\n",
      "   Batch 7400: Current Loss: 0.005910, Avg(last 100): 0.001832\n",
      "⚠️  Warning: NaN in input data at batch 7462, skipping...\n",
      "   Batch 7500: Current Loss: 0.001105, Avg(last 100): 0.001756\n",
      "⚠️  Warning: NaN in input data at batch 7510, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7532, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7533, skipping...\n",
      "   Batch 7600: Current Loss: 0.002479, Avg(last 100): 0.002112\n",
      "⚠️  Warning: NaN in input data at batch 7640, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7692, skipping...\n",
      "   Batch 7700: Current Loss: 0.000544, Avg(last 100): 0.002221\n",
      "   Batch 7800: Current Loss: 0.001519, Avg(last 100): 0.002271\n",
      "⚠️  Warning: NaN in input data at batch 7835, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7884, skipping...\n",
      "   Batch 7900: Current Loss: 0.000866, Avg(last 100): 0.002063\n",
      "⚠️  Warning: NaN in input data at batch 7931, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7939, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7940, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7941, skipping...\n",
      "   Batch 8000: Current Loss: 0.001987, Avg(last 100): 0.002287\n",
      "⚠️  Warning: NaN in input data at batch 8006, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8021, skipping...\n",
      "   Batch 8100: Current Loss: 0.000438, Avg(last 100): 0.001695\n",
      "   Batch 8200: Current Loss: 0.006063, Avg(last 100): 0.002454\n",
      "⚠️  Warning: NaN in input data at batch 8252, skipping...\n",
      "   Batch 8300: Current Loss: 0.004256, Avg(last 100): 0.002453\n",
      "⚠️  Warning: NaN in input data at batch 8323, skipping...\n",
      "   Batch 8400: Current Loss: 0.003090, Avg(last 100): 0.002307\n",
      "⚠️  Warning: NaN in input data at batch 8447, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8454, skipping...\n",
      "   Batch 8500: Current Loss: 0.003587, Avg(last 100): 0.002181\n",
      "   Batch 8600: Current Loss: 0.000794, Avg(last 100): 0.001908\n",
      "   Batch 8700: Current Loss: 0.001355, Avg(last 100): 0.002204\n",
      "⚠️  Warning: NaN in input data at batch 8709, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8734, skipping...\n",
      "   Batch 8800: Current Loss: 0.000764, Avg(last 100): 0.002200\n",
      "⚠️  Warning: NaN in input data at batch 8848, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8849, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8853, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8866, skipping...\n",
      "   Batch 8900: Current Loss: 0.002263, Avg(last 100): 0.002341\n",
      "⚠️  Warning: NaN in input data at batch 8983, skipping...\n",
      "   Batch 9000: Current Loss: 0.004583, Avg(last 100): 0.001917\n",
      "⚠️  Warning: NaN in input data at batch 9004, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 9027, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 9064, skipping...\n",
      "   Batch 9100: Current Loss: 0.004394, Avg(last 100): 0.001843\n",
      "⚠️  Warning: NaN in input data at batch 9155, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 9156, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 9175, skipping...\n",
      "   Batch 9200: Current Loss: 0.002014, Avg(last 100): 0.002480\n",
      "⚠️  Warning: NaN in input data at batch 9251, skipping...\n",
      "   Batch 9300: Current Loss: 0.006296, Avg(last 100): 0.002585\n",
      "⚠️  Warning: NaN in input data at batch 9304, skipping...\n",
      "   Batch 9400: Current Loss: 0.000599, Avg(last 100): 0.002219\n",
      "   Batch 9500: Current Loss: 0.000416, Avg(last 100): 0.002433\n",
      "   Batch 9600: Current Loss: 0.000555, Avg(last 100): 0.002426\n",
      "⚠️  Warning: NaN in input data at batch 9607, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 9661, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 9668, skipping...\n",
      "✅ Epoch 5 completed:\n",
      "   Average Loss: 0.002263\n",
      "   Processed batches: 9508\n",
      "   Skipped batches (NaN): 165\n",
      "   Current Learning Rate: 1.00e-04\n",
      "✅ Model saved to model_checkpoints/latest_checkpoint.pth\n",
      "------------------------------------------------------------\n",
      "\n",
      "📊 Epoch 6/10\n",
      "⚠️  Warning: NaN in input data at batch 41, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 100, skipping...\n",
      "   Batch 200: Current Loss: 0.001636, Avg(last 100): 0.002313\n",
      "⚠️  Warning: NaN in input data at batch 276, skipping...\n",
      "   Batch 300: Current Loss: 0.001339, Avg(last 100): 0.001888\n",
      "⚠️  Warning: NaN in input data at batch 313, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 321, skipping...\n",
      "   Batch 400: Current Loss: 0.000407, Avg(last 100): 0.002181\n",
      "   Batch 500: Current Loss: 0.000758, Avg(last 100): 0.001895\n",
      "⚠️  Warning: NaN in input data at batch 524, skipping...\n",
      "   Batch 600: Current Loss: 0.000659, Avg(last 100): 0.002418\n",
      "⚠️  Warning: NaN in input data at batch 689, skipping...\n",
      "   Batch 700: Current Loss: 0.002072, Avg(last 100): 0.003041\n",
      "⚠️  Warning: NaN in input data at batch 729, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 749, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 766, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 781, skipping...\n",
      "   Batch 800: Current Loss: 0.001304, Avg(last 100): 0.002036\n",
      "⚠️  Warning: NaN in input data at batch 801, skipping...\n",
      "   Batch 900: Current Loss: 0.000488, Avg(last 100): 0.001959\n",
      "⚠️  Warning: NaN in input data at batch 935, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 950, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 975, skipping...\n",
      "   Batch 1000: Current Loss: 0.002482, Avg(last 100): 0.002432\n",
      "⚠️  Warning: NaN in input data at batch 1065, skipping...\n",
      "   Batch 1100: Current Loss: 0.001168, Avg(last 100): 0.002126\n",
      "⚠️  Warning: NaN in input data at batch 1104, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1125, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1133, skipping...\n",
      "   Batch 1200: Current Loss: 0.002151, Avg(last 100): 0.002237\n",
      "   Batch 1300: Current Loss: 0.002391, Avg(last 100): 0.002101\n",
      "   Batch 1400: Current Loss: 0.001274, Avg(last 100): 0.002385\n",
      "⚠️  Warning: NaN in input data at batch 1448, skipping...\n",
      "   Batch 1500: Current Loss: 0.003313, Avg(last 100): 0.002662\n",
      "   Batch 1600: Current Loss: 0.002543, Avg(last 100): 0.002095\n",
      "⚠️  Warning: NaN in input data at batch 1609, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1670, skipping...\n",
      "   Batch 1700: Current Loss: 0.000442, Avg(last 100): 0.002121\n",
      "⚠️  Warning: NaN in input data at batch 1718, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1794, skipping...\n",
      "   Batch 1800: Current Loss: 0.001802, Avg(last 100): 0.002375\n",
      "⚠️  Warning: NaN in input data at batch 1860, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1877, skipping...\n",
      "   Batch 1900: Current Loss: 0.001259, Avg(last 100): 0.002029\n",
      "⚠️  Warning: NaN in input data at batch 1912, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1931, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1943, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1947, skipping...\n",
      "   Batch 2000: Current Loss: 0.007804, Avg(last 100): 0.001897\n",
      "   Batch 2100: Current Loss: 0.000454, Avg(last 100): 0.002311\n",
      "   Batch 2200: Current Loss: 0.004910, Avg(last 100): 0.002155\n",
      "⚠️  Warning: NaN in input data at batch 2223, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2225, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2246, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2247, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2254, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2264, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2280, skipping...\n",
      "   Batch 2300: Current Loss: 0.000453, Avg(last 100): 0.002656\n",
      "⚠️  Warning: NaN in input data at batch 2303, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2347, skipping...\n",
      "   Batch 2400: Current Loss: 0.000929, Avg(last 100): 0.002468\n",
      "   Batch 2500: Current Loss: 0.022294, Avg(last 100): 0.002488\n",
      "⚠️  Warning: NaN in input data at batch 2540, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2550, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2555, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2575, skipping...\n",
      "   Batch 2600: Current Loss: 0.001263, Avg(last 100): 0.003047\n",
      "⚠️  Warning: NaN in input data at batch 2647, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2651, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2652, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2656, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2700, skipping...\n",
      "   Batch 2800: Current Loss: 0.000922, Avg(last 100): 0.001939\n",
      "⚠️  Warning: NaN in input data at batch 2872, skipping...\n",
      "   Batch 2900: Current Loss: 0.005328, Avg(last 100): 0.002550\n",
      "   Batch 3000: Current Loss: 0.000661, Avg(last 100): 0.002174\n",
      "⚠️  Warning: NaN in input data at batch 3010, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3021, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3079, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3099, skipping...\n",
      "   Batch 3100: Current Loss: 0.003122, Avg(last 100): 0.002492\n",
      "⚠️  Warning: NaN in input data at batch 3187, skipping...\n",
      "   Batch 3200: Current Loss: 0.005117, Avg(last 100): 0.002276\n",
      "⚠️  Warning: NaN in input data at batch 3212, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3242, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3248, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3270, skipping...\n",
      "   Batch 3300: Current Loss: 0.000553, Avg(last 100): 0.002594\n",
      "   Batch 3400: Current Loss: 0.001172, Avg(last 100): 0.002345\n",
      "⚠️  Warning: NaN in input data at batch 3424, skipping...\n",
      "   Batch 3500: Current Loss: 0.000806, Avg(last 100): 0.002088\n",
      "⚠️  Warning: NaN in input data at batch 3592, skipping...\n",
      "   Batch 3600: Current Loss: 0.000786, Avg(last 100): 0.002217\n",
      "⚠️  Warning: NaN in input data at batch 3610, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3657, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3695, skipping...\n",
      "   Batch 3700: Current Loss: 0.000534, Avg(last 100): 0.002107\n",
      "⚠️  Warning: NaN in input data at batch 3710, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3779, skipping...\n",
      "   Batch 3800: Current Loss: 0.002253, Avg(last 100): 0.001842\n",
      "⚠️  Warning: NaN in input data at batch 3828, skipping...\n",
      "   Batch 3900: Current Loss: 0.002020, Avg(last 100): 0.002372\n",
      "⚠️  Warning: NaN in input data at batch 3981, skipping...\n",
      "   Batch 4000: Current Loss: 0.001875, Avg(last 100): 0.002114\n",
      "⚠️  Warning: NaN in input data at batch 4038, skipping...\n",
      "   Batch 4100: Current Loss: 0.006075, Avg(last 100): 0.002383\n",
      "⚠️  Warning: NaN in input data at batch 4143, skipping...\n",
      "   Batch 4200: Current Loss: 0.000964, Avg(last 100): 0.002485\n",
      "⚠️  Warning: NaN in input data at batch 4249, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4250, skipping...\n",
      "   Batch 4300: Current Loss: 0.004049, Avg(last 100): 0.002434\n",
      "⚠️  Warning: NaN in input data at batch 4356, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4374, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4387, skipping...\n",
      "   Batch 4400: Current Loss: 0.000627, Avg(last 100): 0.001681\n",
      "⚠️  Warning: NaN in input data at batch 4439, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4451, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4457, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4458, skipping...\n",
      "   Batch 4500: Current Loss: 0.000539, Avg(last 100): 0.002360\n",
      "⚠️  Warning: NaN in input data at batch 4503, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4547, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4562, skipping...\n",
      "   Batch 4600: Current Loss: 0.005232, Avg(last 100): 0.002231\n",
      "⚠️  Warning: NaN in input data at batch 4621, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4685, skipping...\n",
      "   Batch 4700: Current Loss: 0.000688, Avg(last 100): 0.001924\n",
      "⚠️  Warning: NaN in input data at batch 4710, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4732, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4744, skipping...\n",
      "   Batch 4800: Current Loss: 0.001136, Avg(last 100): 0.002060\n",
      "⚠️  Warning: NaN in input data at batch 4896, skipping...\n",
      "   Batch 4900: Current Loss: 0.000817, Avg(last 100): 0.002219\n",
      "⚠️  Warning: NaN in input data at batch 4984, skipping...\n",
      "   Batch 5000: Current Loss: 0.000485, Avg(last 100): 0.002187\n",
      "   Batch 5100: Current Loss: 0.000917, Avg(last 100): 0.002926\n",
      "⚠️  Warning: NaN in input data at batch 5159, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5186, skipping...\n",
      "   Batch 5200: Current Loss: 0.000704, Avg(last 100): 0.002110\n",
      "⚠️  Warning: NaN in input data at batch 5202, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5241, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5265, skipping...\n",
      "   Batch 5300: Current Loss: 0.000492, Avg(last 100): 0.002425\n",
      "⚠️  Warning: NaN in input data at batch 5316, skipping...\n",
      "   Batch 5400: Current Loss: 0.003708, Avg(last 100): 0.002162\n",
      "   Batch 5500: Current Loss: 0.004949, Avg(last 100): 0.002282\n",
      "⚠️  Warning: NaN in input data at batch 5544, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5595, skipping...\n",
      "   Batch 5600: Current Loss: 0.000721, Avg(last 100): 0.002048\n",
      "⚠️  Warning: NaN in input data at batch 5685, skipping...\n",
      "   Batch 5700: Current Loss: 0.000603, Avg(last 100): 0.002020\n",
      "⚠️  Warning: NaN in input data at batch 5729, skipping...\n",
      "   Batch 5800: Current Loss: 0.001285, Avg(last 100): 0.002155\n",
      "⚠️  Warning: NaN in input data at batch 5828, skipping...\n",
      "   Batch 5900: Current Loss: 0.001618, Avg(last 100): 0.002886\n",
      "   Batch 6000: Current Loss: 0.003305, Avg(last 100): 0.002614\n",
      "⚠️  Warning: NaN in input data at batch 6014, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6084, skipping...\n",
      "   Batch 6100: Current Loss: 0.000611, Avg(last 100): 0.002261\n",
      "⚠️  Warning: NaN in input data at batch 6187, skipping...\n",
      "   Batch 6200: Current Loss: 0.003677, Avg(last 100): 0.002081\n",
      "⚠️  Warning: NaN in input data at batch 6262, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6283, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6298, skipping...\n",
      "   Batch 6300: Current Loss: 0.004178, Avg(last 100): 0.001780\n",
      "⚠️  Warning: NaN in input data at batch 6316, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6346, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6360, skipping...\n",
      "   Batch 6400: Current Loss: 0.004812, Avg(last 100): 0.002448\n",
      "   Batch 6500: Current Loss: 0.000469, Avg(last 100): 0.002344\n",
      "⚠️  Warning: NaN in input data at batch 6556, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6584, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6596, skipping...\n",
      "   Batch 6600: Current Loss: 0.003894, Avg(last 100): 0.002545\n",
      "⚠️  Warning: NaN in input data at batch 6655, skipping...\n",
      "   Batch 6700: Current Loss: 0.003176, Avg(last 100): 0.002267\n",
      "⚠️  Warning: NaN in input data at batch 6736, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6777, skipping...\n",
      "   Batch 6800: Current Loss: 0.000426, Avg(last 100): 0.002381\n",
      "⚠️  Warning: NaN in input data at batch 6818, skipping...\n",
      "   Batch 6900: Current Loss: 0.006267, Avg(last 100): 0.002175\n",
      "   Batch 7000: Current Loss: 0.001952, Avg(last 100): 0.001944\n",
      "⚠️  Warning: NaN in input data at batch 7058, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7084, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7100, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7195, skipping...\n",
      "   Batch 7200: Current Loss: 0.004500, Avg(last 100): 0.002131\n",
      "⚠️  Warning: NaN in input data at batch 7234, skipping...\n",
      "   Batch 7300: Current Loss: 0.000940, Avg(last 100): 0.002229\n",
      "⚠️  Warning: NaN in input data at batch 7309, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7355, skipping...\n",
      "   Batch 7400: Current Loss: 0.000846, Avg(last 100): 0.002322\n",
      "⚠️  Warning: NaN in input data at batch 7485, skipping...\n",
      "   Batch 7500: Current Loss: 0.000393, Avg(last 100): 0.002213\n",
      "⚠️  Warning: NaN in input data at batch 7505, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7557, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7573, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7592, skipping...\n",
      "   Batch 7600: Current Loss: 0.000870, Avg(last 100): 0.002216\n",
      "⚠️  Warning: NaN in input data at batch 7683, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7691, skipping...\n",
      "   Batch 7700: Current Loss: 0.006737, Avg(last 100): 0.002075\n",
      "⚠️  Warning: NaN in input data at batch 7748, skipping...\n",
      "   Batch 7800: Current Loss: 0.002143, Avg(last 100): 0.002324\n",
      "⚠️  Warning: NaN in input data at batch 7871, skipping...\n",
      "   Batch 7900: Current Loss: 0.006096, Avg(last 100): 0.001877\n",
      "⚠️  Warning: NaN in input data at batch 7914, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7963, skipping...\n",
      "   Batch 8000: Current Loss: 0.001437, Avg(last 100): 0.002380\n",
      "⚠️  Warning: NaN in input data at batch 8037, skipping...\n",
      "   Batch 8100: Current Loss: 0.000768, Avg(last 100): 0.002447\n",
      "⚠️  Warning: NaN in input data at batch 8131, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8184, skipping...\n",
      "   Batch 8200: Current Loss: 0.000876, Avg(last 100): 0.002390\n",
      "⚠️  Warning: NaN in input data at batch 8202, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8228, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8237, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8256, skipping...\n",
      "   Batch 8300: Current Loss: 0.000653, Avg(last 100): 0.002009\n",
      "⚠️  Warning: NaN in input data at batch 8328, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8343, skipping...\n",
      "   Batch 8400: Current Loss: 0.002113, Avg(last 100): 0.001854\n",
      "⚠️  Warning: NaN in input data at batch 8426, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8453, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8462, skipping...\n",
      "   Batch 8500: Current Loss: 0.000795, Avg(last 100): 0.002767\n",
      "   Batch 8600: Current Loss: 0.000905, Avg(last 100): 0.002443\n",
      "⚠️  Warning: NaN in input data at batch 8637, skipping...\n",
      "   Batch 8700: Current Loss: 0.002175, Avg(last 100): 0.002402\n",
      "⚠️  Warning: NaN in input data at batch 8788, skipping...\n",
      "   Batch 8800: Current Loss: 0.003164, Avg(last 100): 0.002089\n",
      "   Batch 8900: Current Loss: 0.001579, Avg(last 100): 0.002317\n",
      "⚠️  Warning: NaN in input data at batch 8933, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8957, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8964, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8992, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8998, skipping...\n",
      "   Batch 9000: Current Loss: 0.004103, Avg(last 100): 0.001766\n",
      "⚠️  Warning: NaN in input data at batch 9023, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 9065, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 9084, skipping...\n",
      "   Batch 9100: Current Loss: 0.001838, Avg(last 100): 0.002451\n",
      "⚠️  Warning: NaN in input data at batch 9171, skipping...\n",
      "   Batch 9200: Current Loss: 0.001233, Avg(last 100): 0.002329\n",
      "⚠️  Warning: NaN in input data at batch 9216, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 9237, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 9256, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 9275, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 9286, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 9289, skipping...\n",
      "   Batch 9300: Current Loss: 0.000655, Avg(last 100): 0.002568\n",
      "⚠️  Warning: NaN in input data at batch 9334, skipping...\n",
      "   Batch 9400: Current Loss: 0.000894, Avg(last 100): 0.002077\n",
      "   Batch 9500: Current Loss: 0.001540, Avg(last 100): 0.002825\n",
      "⚠️  Warning: NaN in input data at batch 9522, skipping...\n",
      "   Batch 9600: Current Loss: 0.002391, Avg(last 100): 0.002170\n",
      "✅ Epoch 6 completed:\n",
      "   Average Loss: 0.002270\n",
      "   Processed batches: 9509\n",
      "   Skipped batches (NaN): 164\n",
      "   Current Learning Rate: 1.00e-04\n",
      "✅ Model saved to model_checkpoints/latest_checkpoint.pth\n",
      "------------------------------------------------------------\n",
      "\n",
      "📊 Epoch 7/10\n",
      "⚠️  Warning: NaN in input data at batch 67, skipping...\n",
      "   Batch 100: Current Loss: 0.000812, Avg(last 100): 0.001794\n",
      "   Batch 200: Current Loss: 0.008246, Avg(last 100): 0.002365\n",
      "⚠️  Warning: NaN in input data at batch 252, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 259, skipping...\n",
      "   Batch 300: Current Loss: 0.000415, Avg(last 100): 0.002342\n",
      "⚠️  Warning: NaN in input data at batch 356, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 375, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 400, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 413, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 414, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 490, skipping...\n",
      "   Batch 500: Current Loss: 0.000522, Avg(last 100): 0.001664\n",
      "⚠️  Warning: NaN in input data at batch 519, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 557, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 594, skipping...\n",
      "   Batch 600: Current Loss: 0.001305, Avg(last 100): 0.002195\n",
      "⚠️  Warning: NaN in input data at batch 601, skipping...\n",
      "   Batch 700: Current Loss: 0.000620, Avg(last 100): 0.001947\n",
      "   Batch 800: Current Loss: 0.000649, Avg(last 100): 0.001677\n",
      "⚠️  Warning: NaN in input data at batch 864, skipping...\n",
      "   Batch 900: Current Loss: 0.001786, Avg(last 100): 0.001993\n",
      "⚠️  Warning: NaN in input data at batch 907, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 923, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 961, skipping...\n",
      "   Batch 1000: Current Loss: 0.000644, Avg(last 100): 0.002416\n",
      "⚠️  Warning: NaN in input data at batch 1044, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1071, skipping...\n",
      "   Batch 1100: Current Loss: 0.000367, Avg(last 100): 0.002182\n",
      "⚠️  Warning: NaN in input data at batch 1111, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1124, skipping...\n",
      "   Batch 1200: Current Loss: 0.001446, Avg(last 100): 0.002415\n",
      "⚠️  Warning: NaN in input data at batch 1205, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1258, skipping...\n",
      "   Batch 1300: Current Loss: 0.000749, Avg(last 100): 0.002283\n",
      "⚠️  Warning: NaN in input data at batch 1302, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1323, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1342, skipping...\n",
      "   Batch 1400: Current Loss: 0.018674, Avg(last 100): 0.002520\n",
      "⚠️  Warning: NaN in input data at batch 1406, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1416, skipping...\n",
      "   Batch 1500: Current Loss: 0.001107, Avg(last 100): 0.002668\n",
      "   Batch 1600: Current Loss: 0.000522, Avg(last 100): 0.002057\n",
      "⚠️  Warning: NaN in input data at batch 1690, skipping...\n",
      "   Batch 1700: Current Loss: 0.004521, Avg(last 100): 0.002457\n",
      "⚠️  Warning: NaN in input data at batch 1702, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1727, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1745, skipping...\n",
      "   Batch 1800: Current Loss: 0.000854, Avg(last 100): 0.002055\n",
      "⚠️  Warning: NaN in input data at batch 1831, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1840, skipping...\n",
      "   Batch 1900: Current Loss: 0.001504, Avg(last 100): 0.002595\n",
      "⚠️  Warning: NaN in input data at batch 1972, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1995, skipping...\n",
      "   Batch 2000: Current Loss: 0.002144, Avg(last 100): 0.002509\n",
      "⚠️  Warning: NaN in input data at batch 2064, skipping...\n",
      "   Batch 2100: Current Loss: 0.002506, Avg(last 100): 0.002404\n",
      "⚠️  Warning: NaN in input data at batch 2160, skipping...\n",
      "   Batch 2200: Current Loss: 0.001244, Avg(last 100): 0.002390\n",
      "⚠️  Warning: NaN in input data at batch 2287, skipping...\n",
      "   Batch 2300: Current Loss: 0.001148, Avg(last 100): 0.002211\n",
      "⚠️  Warning: NaN in input data at batch 2303, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2313, skipping...\n",
      "   Batch 2400: Current Loss: 0.001694, Avg(last 100): 0.001852\n",
      "⚠️  Warning: NaN in input data at batch 2403, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2410, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2489, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2497, skipping...\n",
      "   Batch 2500: Current Loss: 0.001452, Avg(last 100): 0.001977\n",
      "   Batch 2600: Current Loss: 0.000700, Avg(last 100): 0.002121\n",
      "⚠️  Warning: NaN in input data at batch 2659, skipping...\n",
      "   Batch 2700: Current Loss: 0.001045, Avg(last 100): 0.002292\n",
      "⚠️  Warning: NaN in input data at batch 2726, skipping...\n",
      "   Batch 2800: Current Loss: 0.000811, Avg(last 100): 0.002489\n",
      "⚠️  Warning: NaN in input data at batch 2811, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2831, skipping...\n",
      "   Batch 2900: Current Loss: 0.000538, Avg(last 100): 0.002521\n",
      "⚠️  Warning: NaN in input data at batch 2958, skipping...\n",
      "   Batch 3000: Current Loss: 0.001142, Avg(last 100): 0.001959\n",
      "⚠️  Warning: NaN in input data at batch 3088, skipping...\n",
      "   Batch 3100: Current Loss: 0.000674, Avg(last 100): 0.002379\n",
      "   Batch 3200: Current Loss: 0.000569, Avg(last 100): 0.002301\n",
      "   Batch 3300: Current Loss: 0.000551, Avg(last 100): 0.001800\n",
      "⚠️  Warning: NaN in input data at batch 3309, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3328, skipping...\n",
      "   Batch 3400: Current Loss: 0.012601, Avg(last 100): 0.002084\n",
      "   Batch 3500: Current Loss: 0.000438, Avg(last 100): 0.002199\n",
      "   Batch 3600: Current Loss: 0.000709, Avg(last 100): 0.002398\n",
      "⚠️  Warning: NaN in input data at batch 3664, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3675, skipping...\n",
      "   Batch 3700: Current Loss: 0.001734, Avg(last 100): 0.002122\n",
      "   Batch 3800: Current Loss: 0.011144, Avg(last 100): 0.002154\n",
      "⚠️  Warning: NaN in input data at batch 3889, skipping...\n",
      "   Batch 3900: Current Loss: 0.001460, Avg(last 100): 0.001829\n",
      "⚠️  Warning: NaN in input data at batch 3963, skipping...\n",
      "   Batch 4000: Current Loss: 0.000853, Avg(last 100): 0.002060\n",
      "⚠️  Warning: NaN in input data at batch 4015, skipping...\n",
      "   Batch 4100: Current Loss: 0.000823, Avg(last 100): 0.002341\n",
      "⚠️  Warning: NaN in input data at batch 4171, skipping...\n",
      "   Batch 4200: Current Loss: 0.000987, Avg(last 100): 0.002487\n",
      "⚠️  Warning: NaN in input data at batch 4234, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4255, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4281, skipping...\n",
      "   Batch 4300: Current Loss: 0.009050, Avg(last 100): 0.002202\n",
      "⚠️  Warning: NaN in input data at batch 4366, skipping...\n",
      "   Batch 4400: Current Loss: 0.000695, Avg(last 100): 0.002572\n",
      "⚠️  Warning: NaN in input data at batch 4411, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4414, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4433, skipping...\n",
      "   Batch 4500: Current Loss: 0.001427, Avg(last 100): 0.002454\n",
      "⚠️  Warning: NaN in input data at batch 4541, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4550, skipping...\n",
      "   Batch 4600: Current Loss: 0.004141, Avg(last 100): 0.002587\n",
      "⚠️  Warning: NaN in input data at batch 4621, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4682, skipping...\n",
      "   Batch 4700: Current Loss: 0.001073, Avg(last 100): 0.001516\n",
      "⚠️  Warning: NaN in input data at batch 4707, skipping...\n",
      "   Batch 4800: Current Loss: 0.000574, Avg(last 100): 0.002443\n",
      "⚠️  Warning: NaN in input data at batch 4805, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4900, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4904, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4912, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4918, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4967, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4996, skipping...\n",
      "   Batch 5000: Current Loss: 0.001462, Avg(last 100): 0.002162\n",
      "⚠️  Warning: NaN in input data at batch 5021, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5026, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5031, skipping...\n",
      "   Batch 5100: Current Loss: 0.000950, Avg(last 100): 0.002556\n",
      "⚠️  Warning: NaN in input data at batch 5197, skipping...\n",
      "   Batch 5200: Current Loss: 0.001160, Avg(last 100): 0.002645\n",
      "⚠️  Warning: NaN in input data at batch 5234, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5273, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5275, skipping...\n",
      "   Batch 5300: Current Loss: 0.000530, Avg(last 100): 0.001819\n",
      "   Batch 5400: Current Loss: 0.004031, Avg(last 100): 0.002045\n",
      "⚠️  Warning: NaN in input data at batch 5474, skipping...\n",
      "   Batch 5500: Current Loss: 0.007199, Avg(last 100): 0.002528\n",
      "   Batch 5600: Current Loss: 0.004198, Avg(last 100): 0.002064\n",
      "⚠️  Warning: NaN in input data at batch 5637, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5638, skipping...\n",
      "   Batch 5700: Current Loss: 0.011308, Avg(last 100): 0.002302\n",
      "⚠️  Warning: NaN in input data at batch 5715, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5728, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5752, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5774, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5775, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5799, skipping...\n",
      "   Batch 5800: Current Loss: 0.001245, Avg(last 100): 0.002234\n",
      "   Batch 5900: Current Loss: 0.006046, Avg(last 100): 0.002542\n",
      "⚠️  Warning: NaN in input data at batch 5968, skipping...\n",
      "   Batch 6000: Current Loss: 0.000351, Avg(last 100): 0.002205\n",
      "   Batch 6100: Current Loss: 0.019155, Avg(last 100): 0.001960\n",
      "   Batch 6200: Current Loss: 0.002459, Avg(last 100): 0.001877\n",
      "⚠️  Warning: NaN in input data at batch 6204, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6294, skipping...\n",
      "   Batch 6300: Current Loss: 0.001832, Avg(last 100): 0.002308\n",
      "⚠️  Warning: NaN in input data at batch 6301, skipping...\n",
      "   Batch 6400: Current Loss: 0.000687, Avg(last 100): 0.002328\n",
      "⚠️  Warning: NaN in input data at batch 6435, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6480, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6483, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6491, skipping...\n",
      "   Batch 6500: Current Loss: 0.005639, Avg(last 100): 0.002060\n",
      "⚠️  Warning: NaN in input data at batch 6542, skipping...\n",
      "   Batch 6600: Current Loss: 0.002255, Avg(last 100): 0.001974\n",
      "⚠️  Warning: NaN in input data at batch 6602, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6614, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6642, skipping...\n",
      "   Batch 6700: Current Loss: 0.001141, Avg(last 100): 0.002275\n",
      "⚠️  Warning: NaN in input data at batch 6747, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6758, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6797, skipping...\n",
      "   Batch 6800: Current Loss: 0.000676, Avg(last 100): 0.001967\n",
      "⚠️  Warning: NaN in input data at batch 6872, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6885, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6892, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6895, skipping...\n",
      "   Batch 6900: Current Loss: 0.001072, Avg(last 100): 0.002295\n",
      "⚠️  Warning: NaN in input data at batch 6917, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6959, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6964, skipping...\n",
      "   Batch 7000: Current Loss: 0.000412, Avg(last 100): 0.002032\n",
      "⚠️  Warning: NaN in input data at batch 7100, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7112, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7175, skipping...\n",
      "   Batch 7200: Current Loss: 0.000362, Avg(last 100): 0.002526\n",
      "⚠️  Warning: NaN in input data at batch 7214, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7220, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7262, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7275, skipping...\n",
      "   Batch 7300: Current Loss: 0.017916, Avg(last 100): 0.002229\n",
      "⚠️  Warning: NaN in input data at batch 7315, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7316, skipping...\n",
      "   Batch 7400: Current Loss: 0.000526, Avg(last 100): 0.002661\n",
      "⚠️  Warning: NaN in input data at batch 7478, skipping...\n",
      "   Batch 7500: Current Loss: 0.000943, Avg(last 100): 0.002052\n",
      "⚠️  Warning: NaN in input data at batch 7512, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7530, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7575, skipping...\n",
      "   Batch 7600: Current Loss: 0.005596, Avg(last 100): 0.001765\n",
      "⚠️  Warning: NaN in input data at batch 7631, skipping...\n",
      "   Batch 7700: Current Loss: 0.000509, Avg(last 100): 0.002803\n",
      "   Batch 7800: Current Loss: 0.002207, Avg(last 100): 0.002280\n",
      "⚠️  Warning: NaN in input data at batch 7817, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7835, skipping...\n",
      "   Batch 7900: Current Loss: 0.000686, Avg(last 100): 0.002548\n",
      "⚠️  Warning: NaN in input data at batch 7912, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7966, skipping...\n",
      "   Batch 8000: Current Loss: 0.002214, Avg(last 100): 0.002176\n",
      "⚠️  Warning: NaN in input data at batch 8010, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8023, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8031, skipping...\n",
      "   Batch 8100: Current Loss: 0.002128, Avg(last 100): 0.002168\n",
      "   Batch 8200: Current Loss: 0.001024, Avg(last 100): 0.001879\n",
      "⚠️  Warning: NaN in input data at batch 8225, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8265, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8268, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8278, skipping...\n",
      "   Batch 8300: Current Loss: 0.001209, Avg(last 100): 0.002483\n",
      "⚠️  Warning: NaN in input data at batch 8318, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8320, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8396, skipping...\n",
      "   Batch 8400: Current Loss: 0.006052, Avg(last 100): 0.002245\n",
      "⚠️  Warning: NaN in input data at batch 8470, skipping...\n",
      "   Batch 8500: Current Loss: 0.012341, Avg(last 100): 0.002333\n",
      "   Batch 8600: Current Loss: 0.003742, Avg(last 100): 0.002160\n",
      "⚠️  Warning: NaN in input data at batch 8647, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8653, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8665, skipping...\n",
      "   Batch 8700: Current Loss: 0.000420, Avg(last 100): 0.002045\n",
      "⚠️  Warning: NaN in input data at batch 8723, skipping...\n",
      "   Batch 8800: Current Loss: 0.004241, Avg(last 100): 0.002544\n",
      "   Batch 8900: Current Loss: 0.005890, Avg(last 100): 0.002774\n",
      "⚠️  Warning: NaN in input data at batch 8908, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8929, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8953, skipping...\n",
      "   Batch 9000: Current Loss: 0.003232, Avg(last 100): 0.002646\n",
      "⚠️  Warning: NaN in input data at batch 9019, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 9054, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 9080, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 9092, skipping...\n",
      "   Batch 9100: Current Loss: 0.000498, Avg(last 100): 0.002449\n",
      "   Batch 9200: Current Loss: 0.001078, Avg(last 100): 0.002694\n",
      "⚠️  Warning: NaN in input data at batch 9208, skipping...\n",
      "   Batch 9300: Current Loss: 0.000546, Avg(last 100): 0.002039\n",
      "⚠️  Warning: NaN in input data at batch 9313, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 9398, skipping...\n",
      "   Batch 9400: Current Loss: 0.000917, Avg(last 100): 0.001999\n",
      "⚠️  Warning: NaN in input data at batch 9416, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 9417, skipping...\n",
      "   Batch 9500: Current Loss: 0.001676, Avg(last 100): 0.002741\n",
      "⚠️  Warning: NaN in input data at batch 9508, skipping...\n",
      "   Batch 9600: Current Loss: 0.000474, Avg(last 100): 0.003000\n",
      "✅ Epoch 7 completed:\n",
      "   Average Loss: 0.002261\n",
      "   Processed batches: 9511\n",
      "   Skipped batches (NaN): 162\n",
      "   Current Learning Rate: 5.00e-05\n",
      "✅ Model saved to model_checkpoints/latest_checkpoint.pth\n",
      "------------------------------------------------------------\n",
      "\n",
      "📊 Epoch 8/10\n",
      "⚠️  Warning: NaN in input data at batch 0, skipping...\n",
      "   Batch 100: Current Loss: 0.000423, Avg(last 100): 0.002199\n",
      "⚠️  Warning: NaN in input data at batch 156, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 174, skipping...\n",
      "   Batch 200: Current Loss: 0.001123, Avg(last 100): 0.001770\n",
      "⚠️  Warning: NaN in input data at batch 262, skipping...\n",
      "   Batch 300: Current Loss: 0.002352, Avg(last 100): 0.002188\n",
      "⚠️  Warning: NaN in input data at batch 347, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 358, skipping...\n",
      "   Batch 400: Current Loss: 0.012016, Avg(last 100): 0.002688\n",
      "   Batch 500: Current Loss: 0.001443, Avg(last 100): 0.002307\n",
      "⚠️  Warning: NaN in input data at batch 549, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 558, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 566, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 568, skipping...\n",
      "   Batch 600: Current Loss: 0.000715, Avg(last 100): 0.002136\n",
      "⚠️  Warning: NaN in input data at batch 648, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 655, skipping...\n",
      "   Batch 700: Current Loss: 0.000381, Avg(last 100): 0.002279\n",
      "⚠️  Warning: NaN in input data at batch 762, skipping...\n",
      "   Batch 800: Current Loss: 0.001152, Avg(last 100): 0.001978\n",
      "⚠️  Warning: NaN in input data at batch 864, skipping...\n",
      "   Batch 900: Current Loss: 0.005119, Avg(last 100): 0.001978\n",
      "⚠️  Warning: NaN in input data at batch 905, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 983, skipping...\n",
      "   Batch 1000: Current Loss: 0.000668, Avg(last 100): 0.002951\n",
      "⚠️  Warning: NaN in input data at batch 1003, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1010, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1038, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1044, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1088, skipping...\n",
      "   Batch 1100: Current Loss: 0.002392, Avg(last 100): 0.002800\n",
      "⚠️  Warning: NaN in input data at batch 1152, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1153, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1178, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1188, skipping...\n",
      "   Batch 1200: Current Loss: 0.001415, Avg(last 100): 0.002725\n",
      "⚠️  Warning: NaN in input data at batch 1226, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1297, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1298, skipping...\n",
      "   Batch 1300: Current Loss: 0.000616, Avg(last 100): 0.002449\n",
      "⚠️  Warning: NaN in input data at batch 1345, skipping...\n",
      "   Batch 1400: Current Loss: 0.002037, Avg(last 100): 0.001986\n",
      "⚠️  Warning: NaN in input data at batch 1418, skipping...\n",
      "   Batch 1500: Current Loss: 0.002134, Avg(last 100): 0.002202\n",
      "⚠️  Warning: NaN in input data at batch 1541, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1576, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1579, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1580, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1595, skipping...\n",
      "   Batch 1600: Current Loss: 0.004513, Avg(last 100): 0.002206\n",
      "⚠️  Warning: NaN in input data at batch 1648, skipping...\n",
      "   Batch 1700: Current Loss: 0.005132, Avg(last 100): 0.002678\n",
      "   Batch 1800: Current Loss: 0.001222, Avg(last 100): 0.002535\n",
      "⚠️  Warning: NaN in input data at batch 1840, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1882, skipping...\n",
      "   Batch 1900: Current Loss: 0.000481, Avg(last 100): 0.002003\n",
      "⚠️  Warning: NaN in input data at batch 1987, skipping...\n",
      "   Batch 2000: Current Loss: 0.002198, Avg(last 100): 0.001904\n",
      "⚠️  Warning: NaN in input data at batch 2069, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2073, skipping...\n",
      "   Batch 2100: Current Loss: 0.000563, Avg(last 100): 0.002580\n",
      "⚠️  Warning: NaN in input data at batch 2154, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2195, skipping...\n",
      "   Batch 2200: Current Loss: 0.001547, Avg(last 100): 0.002356\n",
      "   Batch 2300: Current Loss: 0.000751, Avg(last 100): 0.002039\n",
      "   Batch 2400: Current Loss: 0.004667, Avg(last 100): 0.002321\n",
      "⚠️  Warning: NaN in input data at batch 2427, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2431, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2479, skipping...\n",
      "   Batch 2500: Current Loss: 0.003160, Avg(last 100): 0.002200\n",
      "   Batch 2600: Current Loss: 0.002119, Avg(last 100): 0.002234\n",
      "   Batch 2700: Current Loss: 0.000567, Avg(last 100): 0.002073\n",
      "⚠️  Warning: NaN in input data at batch 2707, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2722, skipping...\n",
      "   Batch 2800: Current Loss: 0.003823, Avg(last 100): 0.001771\n",
      "⚠️  Warning: NaN in input data at batch 2895, skipping...\n",
      "   Batch 2900: Current Loss: 0.000899, Avg(last 100): 0.001914\n",
      "⚠️  Warning: NaN in input data at batch 2994, skipping...\n",
      "   Batch 3000: Current Loss: 0.002208, Avg(last 100): 0.002049\n",
      "⚠️  Warning: NaN in input data at batch 3016, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3026, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3038, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3039, skipping...\n",
      "   Batch 3100: Current Loss: 0.002818, Avg(last 100): 0.002329\n",
      "⚠️  Warning: NaN in input data at batch 3139, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3152, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3158, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3197, skipping...\n",
      "   Batch 3200: Current Loss: 0.001356, Avg(last 100): 0.001851\n",
      "⚠️  Warning: NaN in input data at batch 3240, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3281, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3298, skipping...\n",
      "   Batch 3300: Current Loss: 0.001726, Avg(last 100): 0.002397\n",
      "⚠️  Warning: NaN in input data at batch 3339, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3349, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3400, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3490, skipping...\n",
      "   Batch 3500: Current Loss: 0.007079, Avg(last 100): 0.002061\n",
      "⚠️  Warning: NaN in input data at batch 3518, skipping...\n",
      "   Batch 3600: Current Loss: 0.002470, Avg(last 100): 0.002719\n",
      "⚠️  Warning: NaN in input data at batch 3619, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3700, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3710, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3722, skipping...\n",
      "   Batch 3800: Current Loss: 0.000911, Avg(last 100): 0.002261\n",
      "⚠️  Warning: NaN in input data at batch 3883, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3889, skipping...\n",
      "   Batch 3900: Current Loss: 0.014306, Avg(last 100): 0.002322\n",
      "⚠️  Warning: NaN in input data at batch 3980, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3985, skipping...\n",
      "   Batch 4000: Current Loss: 0.001549, Avg(last 100): 0.002355\n",
      "   Batch 4100: Current Loss: 0.001090, Avg(last 100): 0.002520\n",
      "⚠️  Warning: NaN in input data at batch 4112, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4143, skipping...\n",
      "   Batch 4200: Current Loss: 0.000691, Avg(last 100): 0.002159\n",
      "⚠️  Warning: NaN in input data at batch 4282, skipping...\n",
      "   Batch 4300: Current Loss: 0.000851, Avg(last 100): 0.002010\n",
      "⚠️  Warning: NaN in input data at batch 4332, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4337, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4350, skipping...\n",
      "   Batch 4400: Current Loss: 0.001398, Avg(last 100): 0.002224\n",
      "   Batch 4500: Current Loss: 0.000569, Avg(last 100): 0.002868\n",
      "   Batch 4600: Current Loss: 0.000790, Avg(last 100): 0.001797\n",
      "⚠️  Warning: NaN in input data at batch 4627, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4665, skipping...\n",
      "   Batch 4700: Current Loss: 0.000917, Avg(last 100): 0.002587\n",
      "⚠️  Warning: NaN in input data at batch 4788, skipping...\n",
      "   Batch 4800: Current Loss: 0.004444, Avg(last 100): 0.002589\n",
      "⚠️  Warning: NaN in input data at batch 4846, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4852, skipping...\n",
      "   Batch 4900: Current Loss: 0.006525, Avg(last 100): 0.002802\n",
      "⚠️  Warning: NaN in input data at batch 4959, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4977, skipping...\n",
      "   Batch 5000: Current Loss: 0.001879, Avg(last 100): 0.001977\n",
      "   Batch 5100: Current Loss: 0.002142, Avg(last 100): 0.002243\n",
      "⚠️  Warning: NaN in input data at batch 5102, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5131, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5149, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5172, skipping...\n",
      "   Batch 5200: Current Loss: 0.001055, Avg(last 100): 0.002568\n",
      "   Batch 5300: Current Loss: 0.003213, Avg(last 100): 0.002060\n",
      "⚠️  Warning: NaN in input data at batch 5324, skipping...\n",
      "   Batch 5400: Current Loss: 0.003344, Avg(last 100): 0.002024\n",
      "⚠️  Warning: NaN in input data at batch 5445, skipping...\n",
      "   Batch 5500: Current Loss: 0.001414, Avg(last 100): 0.002204\n",
      "⚠️  Warning: NaN in input data at batch 5510, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5512, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5513, skipping...\n",
      "   Batch 5600: Current Loss: 0.000943, Avg(last 100): 0.002174\n",
      "   Batch 5700: Current Loss: 0.000731, Avg(last 100): 0.002285\n",
      "⚠️  Warning: NaN in input data at batch 5763, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5775, skipping...\n",
      "   Batch 5800: Current Loss: 0.003806, Avg(last 100): 0.002411\n",
      "⚠️  Warning: NaN in input data at batch 5818, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5822, skipping...\n",
      "   Batch 5900: Current Loss: 0.000431, Avg(last 100): 0.002355\n",
      "   Batch 6000: Current Loss: 0.000471, Avg(last 100): 0.002627\n",
      "⚠️  Warning: NaN in input data at batch 6090, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6097, skipping...\n",
      "   Batch 6100: Current Loss: 0.006221, Avg(last 100): 0.002359\n",
      "⚠️  Warning: NaN in input data at batch 6101, skipping...\n",
      "   Batch 6200: Current Loss: 0.002404, Avg(last 100): 0.002303\n",
      "⚠️  Warning: NaN in input data at batch 6285, skipping...\n",
      "   Batch 6300: Current Loss: 0.000443, Avg(last 100): 0.002374\n",
      "⚠️  Warning: NaN in input data at batch 6375, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6395, skipping...\n",
      "   Batch 6400: Current Loss: 0.000560, Avg(last 100): 0.001953\n",
      "⚠️  Warning: NaN in input data at batch 6439, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6446, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6498, skipping...\n",
      "   Batch 6500: Current Loss: 0.000998, Avg(last 100): 0.002649\n",
      "⚠️  Warning: NaN in input data at batch 6570, skipping...\n",
      "   Batch 6600: Current Loss: 0.002553, Avg(last 100): 0.002815\n",
      "⚠️  Warning: NaN in input data at batch 6606, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6625, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6649, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6672, skipping...\n",
      "   Batch 6700: Current Loss: 0.000941, Avg(last 100): 0.002434\n",
      "⚠️  Warning: NaN in input data at batch 6750, skipping...\n",
      "   Batch 6800: Current Loss: 0.001165, Avg(last 100): 0.002029\n",
      "⚠️  Warning: NaN in input data at batch 6866, skipping...\n",
      "   Batch 6900: Current Loss: 0.002565, Avg(last 100): 0.002052\n",
      "⚠️  Warning: NaN in input data at batch 6967, skipping...\n",
      "   Batch 7000: Current Loss: 0.002526, Avg(last 100): 0.002673\n",
      "   Batch 7100: Current Loss: 0.004035, Avg(last 100): 0.002321\n",
      "⚠️  Warning: NaN in input data at batch 7171, skipping...\n",
      "   Batch 7200: Current Loss: 0.000742, Avg(last 100): 0.002234\n",
      "   Batch 7300: Current Loss: 0.002733, Avg(last 100): 0.001744\n",
      "⚠️  Warning: NaN in input data at batch 7309, skipping...\n",
      "   Batch 7400: Current Loss: 0.000632, Avg(last 100): 0.002421\n",
      "⚠️  Warning: NaN in input data at batch 7499, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7500, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7546, skipping...\n",
      "   Batch 7600: Current Loss: 0.001118, Avg(last 100): 0.001941\n",
      "⚠️  Warning: NaN in input data at batch 7680, skipping...\n",
      "   Batch 7700: Current Loss: 0.001127, Avg(last 100): 0.002070\n",
      "⚠️  Warning: NaN in input data at batch 7710, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7714, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7789, skipping...\n",
      "   Batch 7800: Current Loss: 0.000429, Avg(last 100): 0.001891\n",
      "⚠️  Warning: NaN in input data at batch 7849, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7857, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7872, skipping...\n",
      "   Batch 7900: Current Loss: 0.000496, Avg(last 100): 0.002020\n",
      "⚠️  Warning: NaN in input data at batch 7907, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7931, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7940, skipping...\n",
      "   Batch 8000: Current Loss: 0.000417, Avg(last 100): 0.002448\n",
      "⚠️  Warning: NaN in input data at batch 8030, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8068, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8082, skipping...\n",
      "   Batch 8100: Current Loss: 0.001932, Avg(last 100): 0.002302\n",
      "⚠️  Warning: NaN in input data at batch 8193, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8199, skipping...\n",
      "   Batch 8200: Current Loss: 0.001977, Avg(last 100): 0.002064\n",
      "⚠️  Warning: NaN in input data at batch 8202, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8249, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8253, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8257, skipping...\n",
      "   Batch 8300: Current Loss: 0.003619, Avg(last 100): 0.002186\n",
      "⚠️  Warning: NaN in input data at batch 8370, skipping...\n",
      "   Batch 8400: Current Loss: 0.002588, Avg(last 100): 0.002156\n",
      "   Batch 8500: Current Loss: 0.007568, Avg(last 100): 0.002050\n",
      "⚠️  Warning: NaN in input data at batch 8595, skipping...\n",
      "   Batch 8600: Current Loss: 0.002564, Avg(last 100): 0.002790\n",
      "⚠️  Warning: NaN in input data at batch 8680, skipping...\n",
      "   Batch 8700: Current Loss: 0.002524, Avg(last 100): 0.002492\n",
      "⚠️  Warning: NaN in input data at batch 8702, skipping...\n",
      "   Batch 8800: Current Loss: 0.000355, Avg(last 100): 0.002210\n",
      "⚠️  Warning: NaN in input data at batch 8804, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8820, skipping...\n",
      "   Batch 8900: Current Loss: 0.000661, Avg(last 100): 0.001804\n",
      "⚠️  Warning: NaN in input data at batch 8915, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8926, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8977, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8979, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8986, skipping...\n",
      "   Batch 9000: Current Loss: 0.002498, Avg(last 100): 0.002122\n",
      "⚠️  Warning: NaN in input data at batch 9057, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 9096, skipping...\n",
      "   Batch 9100: Current Loss: 0.000582, Avg(last 100): 0.002479\n",
      "⚠️  Warning: NaN in input data at batch 9113, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 9148, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 9167, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 9189, skipping...\n",
      "   Batch 9200: Current Loss: 0.001724, Avg(last 100): 0.002313\n",
      "⚠️  Warning: NaN in input data at batch 9244, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 9254, skipping...\n",
      "   Batch 9300: Current Loss: 0.001265, Avg(last 100): 0.002212\n",
      "   Batch 9400: Current Loss: 0.006528, Avg(last 100): 0.002175\n",
      "⚠️  Warning: NaN in input data at batch 9468, skipping...\n",
      "   Batch 9500: Current Loss: 0.001196, Avg(last 100): 0.002415\n",
      "⚠️  Warning: NaN in input data at batch 9568, skipping...\n",
      "   Batch 9600: Current Loss: 0.002089, Avg(last 100): 0.002008\n",
      "⚠️  Warning: NaN in input data at batch 9612, skipping...\n",
      "✅ Epoch 8 completed:\n",
      "   Average Loss: 0.002259\n",
      "   Processed batches: 9510\n",
      "   Skipped batches (NaN): 163\n",
      "   Current Learning Rate: 5.00e-05\n",
      "✅ Model saved to model_checkpoints/best_model.pth\n",
      "   🏆 New best model! Loss: 0.002259\n",
      "   🎯 Sample prediction: 0.035059 (target: 0.032397)\n",
      "✅ Model saved to model_checkpoints/latest_checkpoint.pth\n",
      "------------------------------------------------------------\n",
      "\n",
      "📊 Epoch 9/10\n",
      "⚠️  Warning: NaN in input data at batch 37, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 72, skipping...\n",
      "   Batch 100: Current Loss: 0.000987, Avg(last 100): 0.002073\n",
      "   Batch 200: Current Loss: 0.001017, Avg(last 100): 0.002238\n",
      "   Batch 300: Current Loss: 0.000720, Avg(last 100): 0.001963\n",
      "   Batch 400: Current Loss: 0.013052, Avg(last 100): 0.002631\n",
      "⚠️  Warning: NaN in input data at batch 495, skipping...\n",
      "   Batch 500: Current Loss: 0.000744, Avg(last 100): 0.002093\n",
      "⚠️  Warning: NaN in input data at batch 525, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 561, skipping...\n",
      "   Batch 600: Current Loss: 0.001118, Avg(last 100): 0.002264\n",
      "⚠️  Warning: NaN in input data at batch 688, skipping...\n",
      "   Batch 700: Current Loss: 0.001097, Avg(last 100): 0.002028\n",
      "⚠️  Warning: NaN in input data at batch 709, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 730, skipping...\n",
      "   Batch 800: Current Loss: 0.000548, Avg(last 100): 0.002318\n",
      "   Batch 900: Current Loss: 0.000892, Avg(last 100): 0.002015\n",
      "   Batch 1000: Current Loss: 0.000785, Avg(last 100): 0.001825\n",
      "⚠️  Warning: NaN in input data at batch 1092, skipping...\n",
      "   Batch 1100: Current Loss: 0.001585, Avg(last 100): 0.002517\n",
      "   Batch 1200: Current Loss: 0.000789, Avg(last 100): 0.001924\n",
      "⚠️  Warning: NaN in input data at batch 1263, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1271, skipping...\n",
      "   Batch 1300: Current Loss: 0.003851, Avg(last 100): 0.002162\n",
      "⚠️  Warning: NaN in input data at batch 1325, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1349, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1372, skipping...\n",
      "   Batch 1400: Current Loss: 0.001257, Avg(last 100): 0.002393\n",
      "⚠️  Warning: NaN in input data at batch 1407, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1419, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1429, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1434, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1452, skipping...\n",
      "   Batch 1500: Current Loss: 0.002131, Avg(last 100): 0.001994\n",
      "⚠️  Warning: NaN in input data at batch 1550, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1573, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1588, skipping...\n",
      "   Batch 1600: Current Loss: 0.002567, Avg(last 100): 0.002497\n",
      "⚠️  Warning: NaN in input data at batch 1601, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1640, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1644, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1652, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1677, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1679, skipping...\n",
      "   Batch 1700: Current Loss: 0.002230, Avg(last 100): 0.002162\n",
      "⚠️  Warning: NaN in input data at batch 1740, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1742, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1789, skipping...\n",
      "   Batch 1800: Current Loss: 0.000878, Avg(last 100): 0.002108\n",
      "⚠️  Warning: NaN in input data at batch 1811, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1823, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1895, skipping...\n",
      "   Batch 1900: Current Loss: 0.005364, Avg(last 100): 0.002408\n",
      "⚠️  Warning: NaN in input data at batch 1956, skipping...\n",
      "   Batch 2000: Current Loss: 0.000484, Avg(last 100): 0.002163\n",
      "⚠️  Warning: NaN in input data at batch 2054, skipping...\n",
      "   Batch 2100: Current Loss: 0.000561, Avg(last 100): 0.002003\n",
      "⚠️  Warning: NaN in input data at batch 2176, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2191, skipping...\n",
      "   Batch 2200: Current Loss: 0.000660, Avg(last 100): 0.002545\n",
      "⚠️  Warning: NaN in input data at batch 2293, skipping...\n",
      "   Batch 2300: Current Loss: 0.000685, Avg(last 100): 0.002453\n",
      "⚠️  Warning: NaN in input data at batch 2387, skipping...\n",
      "   Batch 2400: Current Loss: 0.004625, Avg(last 100): 0.002166\n",
      "   Batch 2500: Current Loss: 0.004324, Avg(last 100): 0.002880\n",
      "⚠️  Warning: NaN in input data at batch 2585, skipping...\n",
      "   Batch 2600: Current Loss: 0.007678, Avg(last 100): 0.002579\n",
      "⚠️  Warning: NaN in input data at batch 2617, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2624, skipping...\n",
      "   Batch 2700: Current Loss: 0.000541, Avg(last 100): 0.001815\n",
      "⚠️  Warning: NaN in input data at batch 2721, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2732, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2747, skipping...\n",
      "   Batch 2800: Current Loss: 0.011386, Avg(last 100): 0.002681\n",
      "⚠️  Warning: NaN in input data at batch 2828, skipping...\n",
      "   Batch 2900: Current Loss: 0.006815, Avg(last 100): 0.002569\n",
      "⚠️  Warning: NaN in input data at batch 2908, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2918, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2971, skipping...\n",
      "   Batch 3000: Current Loss: 0.002704, Avg(last 100): 0.002590\n",
      "⚠️  Warning: NaN in input data at batch 3070, skipping...\n",
      "   Batch 3100: Current Loss: 0.000676, Avg(last 100): 0.002625\n",
      "⚠️  Warning: NaN in input data at batch 3129, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3199, skipping...\n",
      "   Batch 3200: Current Loss: 0.000400, Avg(last 100): 0.002253\n",
      "⚠️  Warning: NaN in input data at batch 3244, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3278, skipping...\n",
      "   Batch 3300: Current Loss: 0.000951, Avg(last 100): 0.002834\n",
      "   Batch 3400: Current Loss: 0.001602, Avg(last 100): 0.002115\n",
      "   Batch 3500: Current Loss: 0.002097, Avg(last 100): 0.001763\n",
      "⚠️  Warning: NaN in input data at batch 3514, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3577, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3578, skipping...\n",
      "   Batch 3600: Current Loss: 0.000832, Avg(last 100): 0.002438\n",
      "⚠️  Warning: NaN in input data at batch 3609, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3665, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3692, skipping...\n",
      "   Batch 3700: Current Loss: 0.000875, Avg(last 100): 0.002463\n",
      "⚠️  Warning: NaN in input data at batch 3733, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3762, skipping...\n",
      "   Batch 3800: Current Loss: 0.003636, Avg(last 100): 0.002269\n",
      "⚠️  Warning: NaN in input data at batch 3815, skipping...\n",
      "   Batch 3900: Current Loss: 0.002149, Avg(last 100): 0.001935\n",
      "⚠️  Warning: NaN in input data at batch 3932, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3984, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3987, skipping...\n",
      "   Batch 4000: Current Loss: 0.002946, Avg(last 100): 0.002198\n",
      "⚠️  Warning: NaN in input data at batch 4026, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4086, skipping...\n",
      "   Batch 4100: Current Loss: 0.001374, Avg(last 100): 0.002182\n",
      "⚠️  Warning: NaN in input data at batch 4122, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4167, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4197, skipping...\n",
      "   Batch 4200: Current Loss: 0.000600, Avg(last 100): 0.002368\n",
      "⚠️  Warning: NaN in input data at batch 4226, skipping...\n",
      "   Batch 4300: Current Loss: 0.001811, Avg(last 100): 0.002560\n",
      "⚠️  Warning: NaN in input data at batch 4368, skipping...\n",
      "   Batch 4400: Current Loss: 0.001711, Avg(last 100): 0.002473\n",
      "⚠️  Warning: NaN in input data at batch 4417, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4423, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4444, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4487, skipping...\n",
      "   Batch 4500: Current Loss: 0.000880, Avg(last 100): 0.002286\n",
      "⚠️  Warning: NaN in input data at batch 4521, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4597, skipping...\n",
      "   Batch 4600: Current Loss: 0.010842, Avg(last 100): 0.002802\n",
      "⚠️  Warning: NaN in input data at batch 4640, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4651, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4687, skipping...\n",
      "   Batch 4700: Current Loss: 0.000458, Avg(last 100): 0.002009\n",
      "   Batch 4800: Current Loss: 0.000716, Avg(last 100): 0.002306\n",
      "⚠️  Warning: NaN in input data at batch 4810, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4820, skipping...\n",
      "   Batch 4900: Current Loss: 0.006655, Avg(last 100): 0.002052\n",
      "⚠️  Warning: NaN in input data at batch 4991, skipping...\n",
      "   Batch 5000: Current Loss: 0.000790, Avg(last 100): 0.002192\n",
      "⚠️  Warning: NaN in input data at batch 5018, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5020, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5028, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5085, skipping...\n",
      "   Batch 5100: Current Loss: 0.001200, Avg(last 100): 0.002116\n",
      "⚠️  Warning: NaN in input data at batch 5164, skipping...\n",
      "   Batch 5200: Current Loss: 0.001086, Avg(last 100): 0.002032\n",
      "⚠️  Warning: NaN in input data at batch 5242, skipping...\n",
      "   Batch 5300: Current Loss: 0.001897, Avg(last 100): 0.001985\n",
      "⚠️  Warning: NaN in input data at batch 5308, skipping...\n",
      "   Batch 5400: Current Loss: 0.000975, Avg(last 100): 0.002427\n",
      "   Batch 5500: Current Loss: 0.003030, Avg(last 100): 0.002665\n",
      "⚠️  Warning: NaN in input data at batch 5553, skipping...\n",
      "   Batch 5600: Current Loss: 0.000899, Avg(last 100): 0.002007\n",
      "⚠️  Warning: NaN in input data at batch 5613, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5664, skipping...\n",
      "   Batch 5700: Current Loss: 0.000622, Avg(last 100): 0.002251\n",
      "⚠️  Warning: NaN in input data at batch 5701, skipping...\n",
      "   Batch 5800: Current Loss: 0.002787, Avg(last 100): 0.001798\n",
      "⚠️  Warning: NaN in input data at batch 5802, skipping...\n",
      "   Batch 5900: Current Loss: 0.000496, Avg(last 100): 0.002414\n",
      "⚠️  Warning: NaN in input data at batch 5910, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5927, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5947, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5988, skipping...\n",
      "   Batch 6000: Current Loss: 0.001003, Avg(last 100): 0.002022\n",
      "⚠️  Warning: NaN in input data at batch 6032, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6088, skipping...\n",
      "   Batch 6100: Current Loss: 0.005279, Avg(last 100): 0.002267\n",
      "   Batch 6200: Current Loss: 0.000415, Avg(last 100): 0.002269\n",
      "⚠️  Warning: NaN in input data at batch 6254, skipping...\n",
      "   Batch 6300: Current Loss: 0.000753, Avg(last 100): 0.002265\n",
      "⚠️  Warning: NaN in input data at batch 6363, skipping...\n",
      "   Batch 6400: Current Loss: 0.001368, Avg(last 100): 0.002628\n",
      "⚠️  Warning: NaN in input data at batch 6429, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6442, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6493, skipping...\n",
      "   Batch 6500: Current Loss: 0.000536, Avg(last 100): 0.002662\n",
      "⚠️  Warning: NaN in input data at batch 6545, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6563, skipping...\n",
      "   Batch 6600: Current Loss: 0.000969, Avg(last 100): 0.001760\n",
      "⚠️  Warning: NaN in input data at batch 6606, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6643, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6670, skipping...\n",
      "   Batch 6700: Current Loss: 0.000508, Avg(last 100): 0.002105\n",
      "   Batch 6800: Current Loss: 0.000766, Avg(last 100): 0.002071\n",
      "   Batch 6900: Current Loss: 0.001169, Avg(last 100): 0.002428\n",
      "⚠️  Warning: NaN in input data at batch 6968, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6991, skipping...\n",
      "   Batch 7000: Current Loss: 0.001477, Avg(last 100): 0.002478\n",
      "⚠️  Warning: NaN in input data at batch 7010, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7078, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7087, skipping...\n",
      "   Batch 7100: Current Loss: 0.000997, Avg(last 100): 0.002175\n",
      "   Batch 7200: Current Loss: 0.000882, Avg(last 100): 0.001667\n",
      "⚠️  Warning: NaN in input data at batch 7220, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7240, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7273, skipping...\n",
      "   Batch 7300: Current Loss: 0.008141, Avg(last 100): 0.002381\n",
      "⚠️  Warning: NaN in input data at batch 7360, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7368, skipping...\n",
      "   Batch 7400: Current Loss: 0.000908, Avg(last 100): 0.001755\n",
      "⚠️  Warning: NaN in input data at batch 7414, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7423, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7459, skipping...\n",
      "   Batch 7500: Current Loss: 0.002222, Avg(last 100): 0.002380\n",
      "⚠️  Warning: NaN in input data at batch 7551, skipping...\n",
      "   Batch 7600: Current Loss: 0.000657, Avg(last 100): 0.002373\n",
      "⚠️  Warning: NaN in input data at batch 7631, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7663, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7664, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7684, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7686, skipping...\n",
      "   Batch 7700: Current Loss: 0.001086, Avg(last 100): 0.001845\n",
      "⚠️  Warning: NaN in input data at batch 7723, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7771, skipping...\n",
      "   Batch 7800: Current Loss: 0.002646, Avg(last 100): 0.002793\n",
      "⚠️  Warning: NaN in input data at batch 7818, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7819, skipping...\n",
      "   Batch 7900: Current Loss: 0.001358, Avg(last 100): 0.002354\n",
      "⚠️  Warning: NaN in input data at batch 7906, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7926, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7992, skipping...\n",
      "   Batch 8000: Current Loss: 0.000976, Avg(last 100): 0.001801\n",
      "⚠️  Warning: NaN in input data at batch 8030, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8055, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8078, skipping...\n",
      "   Batch 8100: Current Loss: 0.000672, Avg(last 100): 0.002155\n",
      "⚠️  Warning: NaN in input data at batch 8164, skipping...\n",
      "   Batch 8200: Current Loss: 0.000639, Avg(last 100): 0.001888\n",
      "   Batch 8300: Current Loss: 0.004893, Avg(last 100): 0.002327\n",
      "⚠️  Warning: NaN in input data at batch 8383, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8398, skipping...\n",
      "   Batch 8400: Current Loss: 0.002212, Avg(last 100): 0.002419\n",
      "⚠️  Warning: NaN in input data at batch 8441, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8479, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8484, skipping...\n",
      "   Batch 8500: Current Loss: 0.001970, Avg(last 100): 0.002258\n",
      "   Batch 8600: Current Loss: 0.006184, Avg(last 100): 0.001912\n",
      "⚠️  Warning: NaN in input data at batch 8640, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8671, skipping...\n",
      "   Batch 8700: Current Loss: 0.001245, Avg(last 100): 0.002348\n",
      "⚠️  Warning: NaN in input data at batch 8739, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8745, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8770, skipping...\n",
      "   Batch 8800: Current Loss: 0.005702, Avg(last 100): 0.002088\n",
      "⚠️  Warning: NaN in input data at batch 8856, skipping...\n",
      "   Batch 8900: Current Loss: 0.000453, Avg(last 100): 0.002257\n",
      "⚠️  Warning: NaN in input data at batch 8947, skipping...\n",
      "   Batch 9000: Current Loss: 0.000895, Avg(last 100): 0.003287\n",
      "   Batch 9100: Current Loss: 0.000922, Avg(last 100): 0.002409\n",
      "⚠️  Warning: NaN in input data at batch 9121, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 9192, skipping...\n",
      "   Batch 9200: Current Loss: 0.002898, Avg(last 100): 0.002095\n",
      "⚠️  Warning: NaN in input data at batch 9205, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 9292, skipping...\n",
      "   Batch 9300: Current Loss: 0.001303, Avg(last 100): 0.002031\n",
      "⚠️  Warning: NaN in input data at batch 9351, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 9354, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 9366, skipping...\n",
      "   Batch 9400: Current Loss: 0.002924, Avg(last 100): 0.002610\n",
      "⚠️  Warning: NaN in input data at batch 9421, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 9479, skipping...\n",
      "   Batch 9500: Current Loss: 0.002220, Avg(last 100): 0.001951\n",
      "   Batch 9600: Current Loss: 0.003679, Avg(last 100): 0.002286\n",
      "✅ Epoch 9 completed:\n",
      "   Average Loss: 0.002260\n",
      "   Processed batches: 9508\n",
      "   Skipped batches (NaN): 165\n",
      "   Current Learning Rate: 5.00e-05\n",
      "✅ Model saved to model_checkpoints/latest_checkpoint.pth\n",
      "------------------------------------------------------------\n",
      "\n",
      "📊 Epoch 10/10\n",
      "⚠️  Warning: NaN in input data at batch 25, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 60, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 69, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 91, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 93, skipping...\n",
      "   Batch 100: Current Loss: 0.006525, Avg(last 100): 0.002057\n",
      "⚠️  Warning: NaN in input data at batch 143, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 145, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 157, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 166, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 193, skipping...\n",
      "   Batch 200: Current Loss: 0.000632, Avg(last 100): 0.002487\n",
      "⚠️  Warning: NaN in input data at batch 220, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 254, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 259, skipping...\n",
      "   Batch 300: Current Loss: 0.001321, Avg(last 100): 0.002023\n",
      "   Batch 400: Current Loss: 0.005651, Avg(last 100): 0.002005\n",
      "   Batch 500: Current Loss: 0.001854, Avg(last 100): 0.002070\n",
      "⚠️  Warning: NaN in input data at batch 598, skipping...\n",
      "   Batch 600: Current Loss: 0.000468, Avg(last 100): 0.002586\n",
      "   Batch 700: Current Loss: 0.002435, Avg(last 100): 0.002387\n",
      "⚠️  Warning: NaN in input data at batch 771, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 792, skipping...\n",
      "   Batch 800: Current Loss: 0.005873, Avg(last 100): 0.002079\n",
      "⚠️  Warning: NaN in input data at batch 876, skipping...\n",
      "   Batch 900: Current Loss: 0.002387, Avg(last 100): 0.002251\n",
      "⚠️  Warning: NaN in input data at batch 967, skipping...\n",
      "   Batch 1000: Current Loss: 0.002409, Avg(last 100): 0.002155\n",
      "⚠️  Warning: NaN in input data at batch 1025, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1030, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1044, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1056, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1090, skipping...\n",
      "   Batch 1100: Current Loss: 0.000458, Avg(last 100): 0.002037\n",
      "⚠️  Warning: NaN in input data at batch 1105, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1161, skipping...\n",
      "   Batch 1200: Current Loss: 0.002297, Avg(last 100): 0.002616\n",
      "⚠️  Warning: NaN in input data at batch 1262, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1277, skipping...\n",
      "   Batch 1300: Current Loss: 0.002485, Avg(last 100): 0.002353\n",
      "⚠️  Warning: NaN in input data at batch 1389, skipping...\n",
      "   Batch 1400: Current Loss: 0.009985, Avg(last 100): 0.002091\n",
      "⚠️  Warning: NaN in input data at batch 1423, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1435, skipping...\n",
      "   Batch 1500: Current Loss: 0.000914, Avg(last 100): 0.002351\n",
      "⚠️  Warning: NaN in input data at batch 1531, skipping...\n",
      "   Batch 1600: Current Loss: 0.011762, Avg(last 100): 0.002334\n",
      "⚠️  Warning: NaN in input data at batch 1680, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1681, skipping...\n",
      "   Batch 1700: Current Loss: 0.001516, Avg(last 100): 0.002022\n",
      "⚠️  Warning: NaN in input data at batch 1737, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1753, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1770, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1774, skipping...\n",
      "   Batch 1800: Current Loss: 0.000862, Avg(last 100): 0.001974\n",
      "⚠️  Warning: NaN in input data at batch 1809, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1858, skipping...\n",
      "   Batch 1900: Current Loss: 0.005140, Avg(last 100): 0.002394\n",
      "⚠️  Warning: NaN in input data at batch 1930, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 1996, skipping...\n",
      "   Batch 2000: Current Loss: 0.004931, Avg(last 100): 0.002332\n",
      "⚠️  Warning: NaN in input data at batch 2066, skipping...\n",
      "   Batch 2100: Current Loss: 0.002471, Avg(last 100): 0.002546\n",
      "⚠️  Warning: NaN in input data at batch 2161, skipping...\n",
      "   Batch 2200: Current Loss: 0.000501, Avg(last 100): 0.002329\n",
      "⚠️  Warning: NaN in input data at batch 2208, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2282, skipping...\n",
      "   Batch 2300: Current Loss: 0.000969, Avg(last 100): 0.002142\n",
      "⚠️  Warning: NaN in input data at batch 2301, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2369, skipping...\n",
      "   Batch 2400: Current Loss: 0.001102, Avg(last 100): 0.002295\n",
      "   Batch 2500: Current Loss: 0.009651, Avg(last 100): 0.002460\n",
      "⚠️  Warning: NaN in input data at batch 2518, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2538, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2564, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 2592, skipping...\n",
      "   Batch 2600: Current Loss: 0.002683, Avg(last 100): 0.002377\n",
      "   Batch 2700: Current Loss: 0.000754, Avg(last 100): 0.002362\n",
      "⚠️  Warning: NaN in input data at batch 2790, skipping...\n",
      "   Batch 2800: Current Loss: 0.003323, Avg(last 100): 0.002176\n",
      "⚠️  Warning: NaN in input data at batch 2866, skipping...\n",
      "   Batch 2900: Current Loss: 0.000640, Avg(last 100): 0.001942\n",
      "⚠️  Warning: NaN in input data at batch 2977, skipping...\n",
      "   Batch 3000: Current Loss: 0.000607, Avg(last 100): 0.002490\n",
      "⚠️  Warning: NaN in input data at batch 3022, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3026, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3085, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3090, skipping...\n",
      "   Batch 3100: Current Loss: 0.001237, Avg(last 100): 0.002316\n",
      "   Batch 3200: Current Loss: 0.000721, Avg(last 100): 0.002097\n",
      "⚠️  Warning: NaN in input data at batch 3250, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3298, skipping...\n",
      "   Batch 3300: Current Loss: 0.000831, Avg(last 100): 0.002433\n",
      "⚠️  Warning: NaN in input data at batch 3311, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3344, skipping...\n",
      "   Batch 3400: Current Loss: 0.000452, Avg(last 100): 0.002675\n",
      "⚠️  Warning: NaN in input data at batch 3408, skipping...\n",
      "   Batch 3500: Current Loss: 0.002345, Avg(last 100): 0.002680\n",
      "⚠️  Warning: NaN in input data at batch 3511, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3513, skipping...\n",
      "   Batch 3600: Current Loss: 0.000464, Avg(last 100): 0.002239\n",
      "⚠️  Warning: NaN in input data at batch 3623, skipping...\n",
      "   Batch 3700: Current Loss: 0.006304, Avg(last 100): 0.002706\n",
      "⚠️  Warning: NaN in input data at batch 3705, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3745, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3763, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 3766, skipping...\n",
      "   Batch 3800: Current Loss: 0.002130, Avg(last 100): 0.002491\n",
      "⚠️  Warning: NaN in input data at batch 3831, skipping...\n",
      "   Batch 3900: Current Loss: 0.006410, Avg(last 100): 0.002257\n",
      "   Batch 4000: Current Loss: 0.004214, Avg(last 100): 0.002412\n",
      "⚠️  Warning: NaN in input data at batch 4029, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4056, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4057, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4081, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4084, skipping...\n",
      "   Batch 4100: Current Loss: 0.000606, Avg(last 100): 0.002238\n",
      "   Batch 4200: Current Loss: 0.001152, Avg(last 100): 0.002222\n",
      "   Batch 4300: Current Loss: 0.001533, Avg(last 100): 0.002160\n",
      "⚠️  Warning: NaN in input data at batch 4304, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4317, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4333, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4363, skipping...\n",
      "   Batch 4400: Current Loss: 0.000676, Avg(last 100): 0.002039\n",
      "⚠️  Warning: NaN in input data at batch 4463, skipping...\n",
      "   Batch 4500: Current Loss: 0.001359, Avg(last 100): 0.002209\n",
      "⚠️  Warning: NaN in input data at batch 4539, skipping...\n",
      "   Batch 4600: Current Loss: 0.000555, Avg(last 100): 0.002042\n",
      "⚠️  Warning: NaN in input data at batch 4661, skipping...\n",
      "   Batch 4700: Current Loss: 0.000601, Avg(last 100): 0.002246\n",
      "⚠️  Warning: NaN in input data at batch 4730, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4796, skipping...\n",
      "   Batch 4800: Current Loss: 0.001793, Avg(last 100): 0.001673\n",
      "⚠️  Warning: NaN in input data at batch 4825, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 4887, skipping...\n",
      "   Batch 4900: Current Loss: 0.000483, Avg(last 100): 0.001990\n",
      "⚠️  Warning: NaN in input data at batch 4982, skipping...\n",
      "   Batch 5000: Current Loss: 0.003494, Avg(last 100): 0.002916\n",
      "⚠️  Warning: NaN in input data at batch 5050, skipping...\n",
      "   Batch 5100: Current Loss: 0.004476, Avg(last 100): 0.002455\n",
      "⚠️  Warning: NaN in input data at batch 5123, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5182, skipping...\n",
      "   Batch 5200: Current Loss: 0.005781, Avg(last 100): 0.002260\n",
      "   Batch 5300: Current Loss: 0.000721, Avg(last 100): 0.002342\n",
      "⚠️  Warning: NaN in input data at batch 5332, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5356, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5380, skipping...\n",
      "   Batch 5400: Current Loss: 0.003699, Avg(last 100): 0.002405\n",
      "⚠️  Warning: NaN in input data at batch 5428, skipping...\n",
      "   Batch 5500: Current Loss: 0.005647, Avg(last 100): 0.002387\n",
      "⚠️  Warning: NaN in input data at batch 5549, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5594, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5597, skipping...\n",
      "   Batch 5600: Current Loss: 0.000336, Avg(last 100): 0.002356\n",
      "⚠️  Warning: NaN in input data at batch 5612, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5630, skipping...\n",
      "   Batch 5700: Current Loss: 0.000427, Avg(last 100): 0.001741\n",
      "⚠️  Warning: NaN in input data at batch 5767, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 5772, skipping...\n",
      "   Batch 5800: Current Loss: 0.000515, Avg(last 100): 0.002105\n",
      "   Batch 5900: Current Loss: 0.007729, Avg(last 100): 0.002333\n",
      "   Batch 6000: Current Loss: 0.009670, Avg(last 100): 0.002238\n",
      "   Batch 6100: Current Loss: 0.003216, Avg(last 100): 0.002708\n",
      "⚠️  Warning: NaN in input data at batch 6110, skipping...\n",
      "   Batch 6200: Current Loss: 0.000721, Avg(last 100): 0.002253\n",
      "⚠️  Warning: NaN in input data at batch 6260, skipping...\n",
      "   Batch 6300: Current Loss: 0.001047, Avg(last 100): 0.002469\n",
      "⚠️  Warning: NaN in input data at batch 6310, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6362, skipping...\n",
      "   Batch 6400: Current Loss: 0.000574, Avg(last 100): 0.002137\n",
      "⚠️  Warning: NaN in input data at batch 6464, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6477, skipping...\n",
      "   Batch 6500: Current Loss: 0.000675, Avg(last 100): 0.002333\n",
      "⚠️  Warning: NaN in input data at batch 6558, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6562, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6588, skipping...\n",
      "   Batch 6600: Current Loss: 0.003957, Avg(last 100): 0.001963\n",
      "⚠️  Warning: NaN in input data at batch 6641, skipping...\n",
      "   Batch 6700: Current Loss: 0.000495, Avg(last 100): 0.002382\n",
      "⚠️  Warning: NaN in input data at batch 6708, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6722, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6787, skipping...\n",
      "   Batch 6800: Current Loss: 0.000474, Avg(last 100): 0.002301\n",
      "⚠️  Warning: NaN in input data at batch 6848, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6862, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6900, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6958, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6971, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 6980, skipping...\n",
      "   Batch 7000: Current Loss: 0.000482, Avg(last 100): 0.001950\n",
      "⚠️  Warning: NaN in input data at batch 7078, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7083, skipping...\n",
      "   Batch 7100: Current Loss: 0.000720, Avg(last 100): 0.002364\n",
      "⚠️  Warning: NaN in input data at batch 7132, skipping...\n",
      "   Batch 7200: Current Loss: 0.000526, Avg(last 100): 0.001518\n",
      "   Batch 7300: Current Loss: 0.001567, Avg(last 100): 0.002233\n",
      "⚠️  Warning: NaN in input data at batch 7324, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7334, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7361, skipping...\n",
      "   Batch 7400: Current Loss: 0.002695, Avg(last 100): 0.002145\n",
      "   Batch 7500: Current Loss: 0.004694, Avg(last 100): 0.002251\n",
      "⚠️  Warning: NaN in input data at batch 7516, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7534, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7541, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7572, skipping...\n",
      "   Batch 7600: Current Loss: 0.001285, Avg(last 100): 0.002433\n",
      "⚠️  Warning: NaN in input data at batch 7657, skipping...\n",
      "   Batch 7700: Current Loss: 0.001981, Avg(last 100): 0.002000\n",
      "⚠️  Warning: NaN in input data at batch 7714, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 7732, skipping...\n",
      "   Batch 7800: Current Loss: 0.000590, Avg(last 100): 0.002362\n",
      "   Batch 7900: Current Loss: 0.006060, Avg(last 100): 0.001867\n",
      "   Batch 8000: Current Loss: 0.002524, Avg(last 100): 0.002306\n",
      "⚠️  Warning: NaN in input data at batch 8032, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8041, skipping...\n",
      "   Batch 8100: Current Loss: 0.004705, Avg(last 100): 0.002806\n",
      "⚠️  Warning: NaN in input data at batch 8129, skipping...\n",
      "   Batch 8200: Current Loss: 0.000671, Avg(last 100): 0.002263\n",
      "⚠️  Warning: NaN in input data at batch 8209, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8250, skipping...\n",
      "   Batch 8300: Current Loss: 0.000341, Avg(last 100): 0.002358\n",
      "   Batch 8400: Current Loss: 0.000951, Avg(last 100): 0.002071\n",
      "⚠️  Warning: NaN in input data at batch 8462, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8473, skipping...\n",
      "   Batch 8500: Current Loss: 0.001296, Avg(last 100): 0.002307\n",
      "⚠️  Warning: NaN in input data at batch 8521, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8559, skipping...\n",
      "   Batch 8600: Current Loss: 0.001733, Avg(last 100): 0.002155\n",
      "   Batch 8700: Current Loss: 0.006641, Avg(last 100): 0.002302\n",
      "⚠️  Warning: NaN in input data at batch 8731, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8760, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8773, skipping...\n",
      "   Batch 8800: Current Loss: 0.000368, Avg(last 100): 0.002029\n",
      "⚠️  Warning: NaN in input data at batch 8834, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8847, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8850, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 8882, skipping...\n",
      "   Batch 8900: Current Loss: 0.005184, Avg(last 100): 0.002607\n",
      "⚠️  Warning: NaN in input data at batch 8902, skipping...\n",
      "   Batch 9000: Current Loss: 0.000396, Avg(last 100): 0.002346\n",
      "⚠️  Warning: NaN in input data at batch 9017, skipping...\n",
      "   Batch 9100: Current Loss: 0.000824, Avg(last 100): 0.002070\n",
      "⚠️  Warning: NaN in input data at batch 9103, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 9115, skipping...\n",
      "   Batch 9200: Current Loss: 0.006003, Avg(last 100): 0.002271\n",
      "⚠️  Warning: NaN in input data at batch 9229, skipping...\n",
      "   Batch 9300: Current Loss: 0.000840, Avg(last 100): 0.002237\n",
      "⚠️  Warning: NaN in input data at batch 9314, skipping...\n",
      "   Batch 9400: Current Loss: 0.001007, Avg(last 100): 0.002050\n",
      "⚠️  Warning: NaN in input data at batch 9459, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 9482, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 9493, skipping...\n",
      "   Batch 9500: Current Loss: 0.002095, Avg(last 100): 0.002313\n",
      "⚠️  Warning: NaN in input data at batch 9509, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 9524, skipping...\n",
      "   Batch 9600: Current Loss: 0.000641, Avg(last 100): 0.002223\n",
      "⚠️  Warning: NaN in input data at batch 9640, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 9654, skipping...\n",
      "⚠️  Warning: NaN in input data at batch 9657, skipping...\n",
      "✅ Epoch 10 completed:\n",
      "   Average Loss: 0.002259\n",
      "   Processed batches: 9509\n",
      "   Skipped batches (NaN): 164\n",
      "   Current Learning Rate: 5.00e-05\n",
      "✅ Model saved to model_checkpoints/best_model.pth\n",
      "   🏆 New best model! Loss: 0.002259\n",
      "   🎯 Sample prediction: 0.037434 (target: 0.023586)\n",
      "✅ Model saved to model_checkpoints/latest_checkpoint.pth\n",
      "\n",
      "🛑 Early stopping triggered after epoch 10\n",
      "   Best loss achieved: 0.002259\n",
      "\n",
      "🎉 Training completed!\n",
      "\n",
      "🔄 Loading best model for final evaluation...\n",
      "✅ Model loaded from model_checkpoints/best_model.pth\n",
      "   Resumed from epoch 9, loss: 0.002259\n",
      "\n",
      "📈 Final model evaluation...\n",
      "✅ Final average validation loss: 0.002267\n",
      "   Evaluated on 9510 batches\n",
      "\n",
      "📊 Sample Predictions vs Targets:\n",
      "   Sample 1: Pred=0.039701, Target=0.015796, Diff=0.023906\n",
      "   Sample 2: Pred=0.039705, Target=0.012328, Diff=0.027377\n",
      "   Sample 3: Pred=0.039706, Target=0.017554, Diff=0.022152\n",
      "   Sample 4: Pred=0.039707, Target=0.036510, Diff=0.003198\n",
      "   Sample 5: Pred=0.039707, Target=0.071168, Diff=0.031461\n",
      "✅ Final model weights are healthy\n",
      "\n",
      "📋 Training Summary:\n",
      "   Total epochs completed: 10\n",
      "   Best training loss: 0.002259\n",
      "   Final training loss: 0.002259\n",
      "   Models saved in: model_checkpoints\n",
      "✅ Model saved to model_checkpoints/final_model.pth\n",
      "\n",
      "💾 Saved models:\n",
      "   • Best model: model_checkpoints/best_model.pth\n",
      "   • Final model: model_checkpoints/final_model.pth\n",
      "   • Latest checkpoint: model_checkpoints/latest_checkpoint.pth\n",
      "\n",
      "🎯 To make predictions later, use:\n",
      "   prediction = make_prediction_with_saved_model('model_checkpoints/best_model.pth', your_input_data)\n",
      "   # Or with inverse scaling:\n",
      "   actual_price, norm_pred = make_prediction_with_saved_model('model_checkpoints/best_model.pth', your_input_data, scaler, features)\n"
     ]
    }
   ],
   "execution_count": 173
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T14:45:21.286274Z",
     "start_time": "2025-06-02T14:45:20.932603Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. Create your model\n",
    "model = LSTMModel(input_size=len(features), output_size=1, hidden_size=64, num_layers=2)\n",
    "\n",
    "# 2. Train with all enhancements\n",
    "train_losses, val_losses, best_loss = train_model_with_enhancements(\n",
    "    model=model,\n",
    "    train_loader=train_dataloader,\n",
    "    val_loader=val_dataloader,\n",
    "    X_data=X,  # Your full feature array\n",
    "    scaler=scaler,  # Your fitted scaler\n",
    "    features=features,  # List of feature names\n",
    "    num_epochs=100,\n",
    "    patience=10,  # Stop if no improvement for 10 epochs\n",
    "    save_dir='models'  # Where to save models\n",
    ")\n",
    "\n",
    "# 3. Make predictions\n",
    "predicted_price, norm_pred = make_prediction(model, X, scaler, features)\n",
    "print(f\"Predicted close price for the next day: ${predicted_price:.2f}\")"
   ],
   "id": "53c1c55053763554",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[172]\u001B[39m\u001B[32m, line 7\u001B[39m\n\u001B[32m      2\u001B[39m model = LSTMModel(input_size=\u001B[38;5;28mlen\u001B[39m(features), output_size=\u001B[32m1\u001B[39m, hidden_size=\u001B[32m64\u001B[39m, num_layers=\u001B[32m2\u001B[39m)\n\u001B[32m      4\u001B[39m \u001B[38;5;66;03m# 2. Train with all enhancements\u001B[39;00m\n\u001B[32m      5\u001B[39m train_losses, val_losses, best_loss = train_model_with_enhancements(\n\u001B[32m      6\u001B[39m     model=model,\n\u001B[32m----> \u001B[39m\u001B[32m7\u001B[39m     train_loader=\u001B[43mtrain_dataloader\u001B[49m,\n\u001B[32m      8\u001B[39m     val_loader=val_dataloader,\n\u001B[32m      9\u001B[39m     X_data=X,  \u001B[38;5;66;03m# Your full feature array\u001B[39;00m\n\u001B[32m     10\u001B[39m     scaler=scaler,  \u001B[38;5;66;03m# Your fitted scaler\u001B[39;00m\n\u001B[32m     11\u001B[39m     features=features,  \u001B[38;5;66;03m# List of feature names\u001B[39;00m\n\u001B[32m     12\u001B[39m     num_epochs=\u001B[32m100\u001B[39m,\n\u001B[32m     13\u001B[39m     patience=\u001B[32m10\u001B[39m,  \u001B[38;5;66;03m# Stop if no improvement for 10 epochs\u001B[39;00m\n\u001B[32m     14\u001B[39m     save_dir=\u001B[33m'\u001B[39m\u001B[33mmodels\u001B[39m\u001B[33m'\u001B[39m  \u001B[38;5;66;03m# Where to save models\u001B[39;00m\n\u001B[32m     15\u001B[39m )\n\u001B[32m     17\u001B[39m \u001B[38;5;66;03m# 3. Make predictions\u001B[39;00m\n\u001B[32m     18\u001B[39m predicted_price, norm_pred = make_prediction(model, X, scaler, features)\n",
      "\u001B[31mNameError\u001B[39m: name 'train_dataloader' is not defined"
     ]
    }
   ],
   "execution_count": 172
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n"
   ],
   "id": "752dce02565ca2dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T03:26:01.636782Z",
     "start_time": "2025-06-03T03:26:01.614761Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input = [0.0623, 0.0359, 0.0427, 0.0682]\n",
    "input = pd.DataFrame(input)\n",
    "\n",
    "\n",
    "input_scaled = scaler.fit_transform(input)\n",
    "input_scaled.shape\n"
   ],
   "id": "91a394f978c743a6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 189
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Predict with trained model",
   "id": "9b575df1d17879ce"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T18:48:58.150849Z",
     "start_time": "2025-06-02T18:48:58.078538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load and predict\n",
    "actual_price, norm_pred = make_prediction_with_saved_model(\n",
    "    'model_checkpoints/best_model.pth',\n",
    "    input_sequence,\n",
    "    scaler,\n",
    "    features\n",
    ")\n",
    "print(f\"Predicted close price for the next day: ${actual_price:.2f}\")"
   ],
   "id": "c01f21649a13fce3",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for LSTMModel:\n\tsize mismatch for lstm1.weight_ih_l0: copying a param with shape torch.Size([256, 4]) from checkpoint, the shape in current model is torch.Size([256, 1]).",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[186]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# Load and predict\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m actual_price, norm_pred = \u001B[43mmake_prediction_with_saved_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      3\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mmodel_checkpoints/best_model.pth\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      4\u001B[39m \u001B[43m    \u001B[49m\u001B[43minput_sequence\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      5\u001B[39m \u001B[43m    \u001B[49m\u001B[43mscaler\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      6\u001B[39m \u001B[43m    \u001B[49m\u001B[43mfeatures\u001B[49m\n\u001B[32m      7\u001B[39m \u001B[43m)\u001B[49m\n\u001B[32m      8\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mPredicted close price for the next day: $\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mactual_price\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[173]\u001B[39m\u001B[32m, line 295\u001B[39m, in \u001B[36mmake_prediction_with_saved_model\u001B[39m\u001B[34m(model_path, input_data, scaler, features)\u001B[39m\n\u001B[32m    286\u001B[39m \u001B[38;5;66;03m# Create new model instance (you'll need to adjust parameters as needed)\u001B[39;00m\n\u001B[32m    287\u001B[39m pred_model = LSTMModel(\n\u001B[32m    288\u001B[39m     input_size=input_data.shape[-\u001B[32m1\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(input_data.shape) > \u001B[32m1\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(features),\n\u001B[32m    289\u001B[39m     output_size=\u001B[32m1\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m    292\u001B[39m     dropout=\u001B[32m0.2\u001B[39m\n\u001B[32m    293\u001B[39m )\n\u001B[32m--> \u001B[39m\u001B[32m295\u001B[39m \u001B[43mpred_model\u001B[49m\u001B[43m.\u001B[49m\u001B[43mload_state_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcheckpoint\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43mmodel_state_dict\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    296\u001B[39m pred_model.eval()\n\u001B[32m    298\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m torch.no_grad():\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject2/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2593\u001B[39m, in \u001B[36mModule.load_state_dict\u001B[39m\u001B[34m(self, state_dict, strict, assign)\u001B[39m\n\u001B[32m   2585\u001B[39m         error_msgs.insert(\n\u001B[32m   2586\u001B[39m             \u001B[32m0\u001B[39m,\n\u001B[32m   2587\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mMissing key(s) in state_dict: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[33m. \u001B[39m\u001B[33m\"\u001B[39m.format(\n\u001B[32m   2588\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33m, \u001B[39m\u001B[33m\"\u001B[39m.join(\u001B[33mf\u001B[39m\u001B[33m'\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mk\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\u001B[33m'\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m missing_keys)\n\u001B[32m   2589\u001B[39m             ),\n\u001B[32m   2590\u001B[39m         )\n\u001B[32m   2592\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(error_msgs) > \u001B[32m0\u001B[39m:\n\u001B[32m-> \u001B[39m\u001B[32m2593\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[32m   2594\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mError(s) in loading state_dict for \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[33m:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[33m\"\u001B[39m.format(\n\u001B[32m   2595\u001B[39m             \u001B[38;5;28mself\u001B[39m.\u001B[34m__class__\u001B[39m.\u001B[34m__name__\u001B[39m, \u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[33m\"\u001B[39m.join(error_msgs)\n\u001B[32m   2596\u001B[39m         )\n\u001B[32m   2597\u001B[39m     )\n\u001B[32m   2598\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001B[31mRuntimeError\u001B[39m: Error(s) in loading state_dict for LSTMModel:\n\tsize mismatch for lstm1.weight_ih_l0: copying a param with shape torch.Size([256, 4]) from checkpoint, the shape in current model is torch.Size([256, 1])."
     ]
    }
   ],
   "execution_count": 186
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Make Predictions of the next day",
   "id": "69d17821adbe7f64"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Predict the next day using the last 30 days of data\n",
    "last_sequence = torch.tensor(X[-1], dtype=torch.float32)\n",
    "predicted_norm = model(last_sequence).item()\n",
    "\n",
    "# Inverse normalize the predicted value\n",
    "dummy = np.zeros((1, len(features) + 1))\n",
    "dummy[0, -1] = predicted_norm\n",
    "predicted_close = scaler.inverse_transform(dummy)[0, -1]\n",
    "print(f\"Predicted close price for the next day: {predicted_close:.2f}\")"
   ],
   "id": "a8d0552bb24dd3b9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T13:10:36.121628Z",
     "start_time": "2025-05-30T13:10:34.933062Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "from collections import defaultdict\n",
    "\n",
    "POLYGON_APIKEY = \"SnsJtDM9NI3nlgJXmYpcWdeHo7g_u2Xb\"\n",
    "exchange_to_country = {\n",
    "    \"XNAS\": \"US\",\n",
    "    \"XNYS\": \"US\",\n",
    "    \"XTKS\": \"Japan\",\n",
    "    \"XTAI\": \"Taiwan\",\n",
    "    \"XSHG\": \"China\",\n",
    "    \"XHKG\": \"Hong Kong\"\n",
    "}\n",
    "\n",
    "# Step 1: Get tickers\n",
    "url = \"https://api.polygon.io/v3/reference/tickers\"\n",
    "params = {\n",
    "    \"apiKey\": POLYGON_APIKEY,\n",
    "    \"limit\": 1000,  # increase this or use pagination\n",
    "    \"active\": \"true\",\n",
    "    \"market\": \"stocks\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, params=params)\n",
    "tickers = response.json().get(\"results\", [])\n",
    "\n",
    "# Step 2: Organize by region\n",
    "region_dict = defaultdict(list)\n",
    "\n",
    "for ticker in tickers:\n",
    "    ex = ticker.get(\"primary_exchange\", \"\")\n",
    "    country = exchange_to_country.get(ex)\n",
    "    if country:\n",
    "        region_dict[country].append(ticker)\n",
    "\n",
    "# Step 3: Sort by market cap and select top 10\n",
    "top_10_per_region = {}\n",
    "for region, companies in region_dict.items():\n",
    "    sorted_companies = sorted(companies, key=lambda x: x.get(\"market_cap\", 0), reverse=True)\n",
    "    top_10_per_region[region] = sorted_companies[:10]\n",
    "\n",
    "# Step 4: Print results\n",
    "for region, top_10 in top_10_per_region.items():\n",
    "    print(f\"Top 10 in {region}:\")\n",
    "    for company in top_10:\n",
    "        print(f\"{company['ticker']} - {company.get('name', '')} - Market Cap: {company.get('market_cap')}\")\n",
    "    print(\"-\" * 40)\n"
   ],
   "id": "67af4a644dd4a26b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 in US:\n",
      "A - Agilent Technologies Inc. - Market Cap: None\n",
      "AA - Alcoa Corporation - Market Cap: None\n",
      "AACB - Artius II Acquisition Inc. Class A Ordinary Shares - Market Cap: None\n",
      "AACBR - Artius II Acquisition Inc. Rights - Market Cap: None\n",
      "AACBU - Artius II Acquisition Inc. Units - Market Cap: None\n",
      "AACG - ATA Creativity Global American Depositary Shares - Market Cap: None\n",
      "AACIU - Armada Acquisition Corp. II Units - Market Cap: None\n",
      "AACT - Ares Acquisition Corporation II - Market Cap: None\n",
      "AACT.U - Ares Acquisition Corporation II Units, each consisting of one Class A ordinary share and one-half of one redeemable warrant - Market Cap: None\n",
      "AACT.WS - Ares Acquisition Corporation II Redeemable Warrants, each whole warrant exercisable for one Class A ordinary share at an exercise price of $11.50 - Market Cap: None\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T13:10:48.988536Z",
     "start_time": "2025-05-30T13:10:48.982608Z"
    }
   },
   "cell_type": "code",
   "source": "top_10_per_region",
   "id": "c024208c8c0ab922",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'US': [{'ticker': 'A',\n",
       "   'name': 'Agilent Technologies Inc.',\n",
       "   'market': 'stocks',\n",
       "   'locale': 'us',\n",
       "   'primary_exchange': 'XNYS',\n",
       "   'type': 'CS',\n",
       "   'active': True,\n",
       "   'currency_name': 'usd',\n",
       "   'cik': '0001090872',\n",
       "   'composite_figi': 'BBG000C2V3D6',\n",
       "   'share_class_figi': 'BBG001SCTQY4',\n",
       "   'last_updated_utc': '2025-05-21T00:00:00Z'},\n",
       "  {'ticker': 'AA',\n",
       "   'name': 'Alcoa Corporation',\n",
       "   'market': 'stocks',\n",
       "   'locale': 'us',\n",
       "   'primary_exchange': 'XNYS',\n",
       "   'type': 'CS',\n",
       "   'active': True,\n",
       "   'currency_name': 'usd',\n",
       "   'cik': '0001675149',\n",
       "   'composite_figi': 'BBG00B3T3HD3',\n",
       "   'share_class_figi': 'BBG00B3T3HF1',\n",
       "   'last_updated_utc': '2025-05-29T00:00:00Z'},\n",
       "  {'ticker': 'AACB',\n",
       "   'name': 'Artius II Acquisition Inc. Class A Ordinary Shares',\n",
       "   'market': 'stocks',\n",
       "   'locale': 'us',\n",
       "   'primary_exchange': 'XNAS',\n",
       "   'type': 'CS',\n",
       "   'active': True,\n",
       "   'currency_name': 'usd',\n",
       "   'cik': '0002034334',\n",
       "   'last_updated_utc': '2025-05-29T00:00:00Z'},\n",
       "  {'ticker': 'AACBR',\n",
       "   'name': 'Artius II Acquisition Inc. Rights',\n",
       "   'market': 'stocks',\n",
       "   'locale': 'us',\n",
       "   'primary_exchange': 'XNAS',\n",
       "   'type': 'RIGHT',\n",
       "   'active': True,\n",
       "   'currency_name': 'usd',\n",
       "   'cik': '0002034334',\n",
       "   'last_updated_utc': '2025-05-29T00:00:00Z'},\n",
       "  {'ticker': 'AACBU',\n",
       "   'name': 'Artius II Acquisition Inc. Units',\n",
       "   'market': 'stocks',\n",
       "   'locale': 'us',\n",
       "   'primary_exchange': 'XNAS',\n",
       "   'type': 'UNIT',\n",
       "   'active': True,\n",
       "   'currency_name': 'usd',\n",
       "   'cik': '0002034334',\n",
       "   'last_updated_utc': '2025-05-29T00:00:00Z'},\n",
       "  {'ticker': 'AACG',\n",
       "   'name': 'ATA Creativity Global American Depositary Shares',\n",
       "   'market': 'stocks',\n",
       "   'locale': 'us',\n",
       "   'primary_exchange': 'XNAS',\n",
       "   'type': 'ADRC',\n",
       "   'active': True,\n",
       "   'currency_name': 'usd',\n",
       "   'cik': '0001420529',\n",
       "   'composite_figi': 'BBG000V2S3P6',\n",
       "   'share_class_figi': 'BBG001T125S9',\n",
       "   'last_updated_utc': '2025-05-23T00:00:00Z'},\n",
       "  {'ticker': 'AACIU',\n",
       "   'name': 'Armada Acquisition Corp. II Units',\n",
       "   'market': 'stocks',\n",
       "   'locale': 'us',\n",
       "   'primary_exchange': 'XNAS',\n",
       "   'type': 'UNIT',\n",
       "   'active': True,\n",
       "   'currency_name': 'usd',\n",
       "   'cik': '0002044009',\n",
       "   'last_updated_utc': '2025-05-29T00:00:00Z'},\n",
       "  {'ticker': 'AACT',\n",
       "   'name': 'Ares Acquisition Corporation II',\n",
       "   'market': 'stocks',\n",
       "   'locale': 'us',\n",
       "   'primary_exchange': 'XNYS',\n",
       "   'type': 'CS',\n",
       "   'active': True,\n",
       "   'currency_name': 'usd',\n",
       "   'cik': '0001853138',\n",
       "   'last_updated_utc': '2025-05-29T00:00:00Z'},\n",
       "  {'ticker': 'AACT.U',\n",
       "   'name': 'Ares Acquisition Corporation II Units, each consisting of one Class A ordinary share and one-half of one redeemable warrant',\n",
       "   'market': 'stocks',\n",
       "   'locale': 'us',\n",
       "   'primary_exchange': 'XNYS',\n",
       "   'type': 'UNIT',\n",
       "   'active': True,\n",
       "   'currency_name': 'usd',\n",
       "   'cik': '0001853138',\n",
       "   'last_updated_utc': '2025-05-29T00:00:00Z'},\n",
       "  {'ticker': 'AACT.WS',\n",
       "   'name': 'Ares Acquisition Corporation II Redeemable Warrants, each whole warrant exercisable for one Class A ordinary share at an exercise price of $11.50',\n",
       "   'market': 'stocks',\n",
       "   'locale': 'us',\n",
       "   'primary_exchange': 'XNYS',\n",
       "   'type': 'WARRANT',\n",
       "   'active': True,\n",
       "   'currency_name': 'usd',\n",
       "   'cik': '0001853138',\n",
       "   'last_updated_utc': '2025-05-29T00:00:00Z'}]}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T13:31:45.601449Z",
     "start_time": "2025-05-30T13:31:45.595795Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Fetch OHLCV time series for a given ticker\n",
    "def fetch_ohlcv(ticker, api_key, timespan=\"day\", from_date=\"2024-01-01\", to_date=\"2025-05-28\"):\n",
    "    url = f\"https://api.polygon.io/v2/aggs/ticker/{ticker}/range/1/{timespan}/{from_date}/{to_date}\"\n",
    "    params = {\"adjusted\": \"true\", \"sort\": \"asc\", \"limit\": 50000, \"apiKey\": api_key}\n",
    "    res = requests.get(url, params=params).json()\n",
    "    return res[\"results\"] if \"results\" in res else []\n"
   ],
   "id": "63a2c344c9b94a76",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T13:33:22.209559Z",
     "start_time": "2025-05-30T13:32:13.337691Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "api_key = POLYGON_APIKEY\n",
    "\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for region, companies in top_10_per_region.items():\n",
    "    for company in companies:\n",
    "        ticker\n",
    "\n",
    "for ticker in tqdm(tickers):\n",
    "    data = fetch_ohlcv(ticker, api_key)\n",
    "    for d in data:\n",
    "        d[\"ticker\"] = ticker\n",
    "        all_data.append(d)\n",
    "\n",
    "df = pd.DataFrame(all_data)\n",
    "df[\"t\"] = pd.to_datetime(df[\"t\"], unit=\"ms\")  # timestamp to datetime\n",
    "df.rename(columns={\"t\": \"date\", \"c\": \"close\", \"o\": \"open\", \"h\": \"high\", \"l\": \"low\", \"v\": \"volume\"}, inplace=True)\n",
    "df.to_csv(\"historical_prices.csv\", index=False)\n"
   ],
   "id": "36050da3c934682",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 89/1000 [01:08<11:44,  1.29it/s]\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Extra data: line 1 column 5 (char 4)",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mJSONDecodeError\u001B[39m                           Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject2/.venv/lib/python3.11/site-packages/requests/models.py:974\u001B[39m, in \u001B[36mResponse.json\u001B[39m\u001B[34m(self, **kwargs)\u001B[39m\n\u001B[32m    973\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m974\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcomplexjson\u001B[49m\u001B[43m.\u001B[49m\u001B[43mloads\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    975\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m JSONDecodeError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    976\u001B[39m     \u001B[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001B[39;00m\n\u001B[32m    977\u001B[39m     \u001B[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/__init__.py:346\u001B[39m, in \u001B[36mloads\u001B[39m\u001B[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001B[39m\n\u001B[32m    343\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m object_hook \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[32m    344\u001B[39m         parse_int \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m parse_float \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[32m    345\u001B[39m         parse_constant \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m object_pairs_hook \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m kw):\n\u001B[32m--> \u001B[39m\u001B[32m346\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_default_decoder\u001B[49m\u001B[43m.\u001B[49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    347\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/json/decoder.py:340\u001B[39m, in \u001B[36mJSONDecoder.decode\u001B[39m\u001B[34m(self, s, _w)\u001B[39m\n\u001B[32m    339\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m end != \u001B[38;5;28mlen\u001B[39m(s):\n\u001B[32m--> \u001B[39m\u001B[32m340\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m JSONDecodeError(\u001B[33m\"\u001B[39m\u001B[33mExtra data\u001B[39m\u001B[33m\"\u001B[39m, s, end)\n\u001B[32m    341\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m obj\n",
      "\u001B[31mJSONDecodeError\u001B[39m: Extra data: line 1 column 5 (char 4)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[31mJSONDecodeError\u001B[39m                           Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[66]\u001B[39m\u001B[32m, line 14\u001B[39m\n\u001B[32m     11\u001B[39m         ticker\n\u001B[32m     13\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m ticker \u001B[38;5;129;01min\u001B[39;00m tqdm(tickers):\n\u001B[32m---> \u001B[39m\u001B[32m14\u001B[39m     data = \u001B[43mfetch_ohlcv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mticker\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mapi_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     15\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m d \u001B[38;5;129;01min\u001B[39;00m data:\n\u001B[32m     16\u001B[39m         d[\u001B[33m\"\u001B[39m\u001B[33mticker\u001B[39m\u001B[33m\"\u001B[39m] = ticker\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[64]\u001B[39m\u001B[32m, line 5\u001B[39m, in \u001B[36mfetch_ohlcv\u001B[39m\u001B[34m(ticker, api_key, timespan, from_date, to_date)\u001B[39m\n\u001B[32m      3\u001B[39m url = \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mhttps://api.polygon.io/v2/aggs/ticker/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mticker\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m/range/1/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtimespan\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfrom_date\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mto_date\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m      4\u001B[39m params = {\u001B[33m\"\u001B[39m\u001B[33madjusted\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mtrue\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33msort\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33masc\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mlimit\u001B[39m\u001B[33m\"\u001B[39m: \u001B[32m50000\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mapiKey\u001B[39m\u001B[33m\"\u001B[39m: api_key}\n\u001B[32m----> \u001B[39m\u001B[32m5\u001B[39m res = \u001B[43mrequests\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m=\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mjson\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      6\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m res[\u001B[33m\"\u001B[39m\u001B[33mresults\u001B[39m\u001B[33m\"\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mresults\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m res \u001B[38;5;28;01melse\u001B[39;00m []\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/FastAPIProject2/.venv/lib/python3.11/site-packages/requests/models.py:978\u001B[39m, in \u001B[36mResponse.json\u001B[39m\u001B[34m(self, **kwargs)\u001B[39m\n\u001B[32m    974\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m complexjson.loads(\u001B[38;5;28mself\u001B[39m.text, **kwargs)\n\u001B[32m    975\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m JSONDecodeError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    976\u001B[39m     \u001B[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001B[39;00m\n\u001B[32m    977\u001B[39m     \u001B[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m978\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m RequestsJSONDecodeError(e.msg, e.doc, e.pos)\n",
      "\u001B[31mJSONDecodeError\u001B[39m: Extra data: line 1 column 5 (char 4)"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T05:26:03.639051Z",
     "start_time": "2025-05-30T05:26:03.620638Z"
    }
   },
   "cell_type": "code",
   "source": "df",
   "id": "22f55e0e7d5be2a4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        volume        vw      open     close      high       low  \\\n",
       "0     0.326792  248.3450  0.326594  0.314881  0.310404  0.331362   \n",
       "1     0.406343  239.8167  0.311385  0.285368  0.294217  0.305978   \n",
       "2     0.316933  240.2989  0.294298  0.283828  0.285557  0.310402   \n",
       "3     0.267884  237.8911  0.287171  0.282526  0.278058  0.301523   \n",
       "4     0.232411  238.5962  0.285024  0.291288  0.281343  0.302778   \n",
       "...        ...       ...       ...       ...       ...       ...   \n",
       "1051  0.211519  454.3728  0.892956  0.867345  0.898238  0.894326   \n",
       "1052  0.190612  456.1561  0.896228  0.887611  0.922018  0.911757   \n",
       "1053  0.170568  451.0242  0.853427  0.846195  0.858862  0.870059   \n",
       "1054  0.242362  458.9856  0.909404  0.939204  0.928757  0.930267   \n",
       "1055  0.174126  458.5505  0.950224  0.909735  0.943872  0.937077   \n",
       "\n",
       "                    date        n ticker  \n",
       "0    2024-01-02 05:00:00  1177663   TSLA  \n",
       "1    2024-01-03 05:00:00  1273469   TSLA  \n",
       "2    2024-01-04 05:00:00  1001611   TSLA  \n",
       "3    2024-01-05 05:00:00   934668   TSLA  \n",
       "4    2024-01-08 05:00:00   970810   TSLA  \n",
       "...                  ...      ...    ...  \n",
       "1051 2025-05-21 04:00:00   320670   MSFT  \n",
       "1052 2025-05-22 04:00:00   334105   MSFT  \n",
       "1053 2025-05-23 04:00:00   323259   MSFT  \n",
       "1054 2025-05-27 04:00:00   362158   MSFT  \n",
       "1055 2025-05-28 04:00:00   280126   MSFT  \n",
       "\n",
       "[1056 rows x 9 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>volume</th>\n",
       "      <th>vw</th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>date</th>\n",
       "      <th>n</th>\n",
       "      <th>ticker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.326792</td>\n",
       "      <td>248.3450</td>\n",
       "      <td>0.326594</td>\n",
       "      <td>0.314881</td>\n",
       "      <td>0.310404</td>\n",
       "      <td>0.331362</td>\n",
       "      <td>2024-01-02 05:00:00</td>\n",
       "      <td>1177663</td>\n",
       "      <td>TSLA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.406343</td>\n",
       "      <td>239.8167</td>\n",
       "      <td>0.311385</td>\n",
       "      <td>0.285368</td>\n",
       "      <td>0.294217</td>\n",
       "      <td>0.305978</td>\n",
       "      <td>2024-01-03 05:00:00</td>\n",
       "      <td>1273469</td>\n",
       "      <td>TSLA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.316933</td>\n",
       "      <td>240.2989</td>\n",
       "      <td>0.294298</td>\n",
       "      <td>0.283828</td>\n",
       "      <td>0.285557</td>\n",
       "      <td>0.310402</td>\n",
       "      <td>2024-01-04 05:00:00</td>\n",
       "      <td>1001611</td>\n",
       "      <td>TSLA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.267884</td>\n",
       "      <td>237.8911</td>\n",
       "      <td>0.287171</td>\n",
       "      <td>0.282526</td>\n",
       "      <td>0.278058</td>\n",
       "      <td>0.301523</td>\n",
       "      <td>2024-01-05 05:00:00</td>\n",
       "      <td>934668</td>\n",
       "      <td>TSLA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.232411</td>\n",
       "      <td>238.5962</td>\n",
       "      <td>0.285024</td>\n",
       "      <td>0.291288</td>\n",
       "      <td>0.281343</td>\n",
       "      <td>0.302778</td>\n",
       "      <td>2024-01-08 05:00:00</td>\n",
       "      <td>970810</td>\n",
       "      <td>TSLA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051</th>\n",
       "      <td>0.211519</td>\n",
       "      <td>454.3728</td>\n",
       "      <td>0.892956</td>\n",
       "      <td>0.867345</td>\n",
       "      <td>0.898238</td>\n",
       "      <td>0.894326</td>\n",
       "      <td>2025-05-21 04:00:00</td>\n",
       "      <td>320670</td>\n",
       "      <td>MSFT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1052</th>\n",
       "      <td>0.190612</td>\n",
       "      <td>456.1561</td>\n",
       "      <td>0.896228</td>\n",
       "      <td>0.887611</td>\n",
       "      <td>0.922018</td>\n",
       "      <td>0.911757</td>\n",
       "      <td>2025-05-22 04:00:00</td>\n",
       "      <td>334105</td>\n",
       "      <td>MSFT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053</th>\n",
       "      <td>0.170568</td>\n",
       "      <td>451.0242</td>\n",
       "      <td>0.853427</td>\n",
       "      <td>0.846195</td>\n",
       "      <td>0.858862</td>\n",
       "      <td>0.870059</td>\n",
       "      <td>2025-05-23 04:00:00</td>\n",
       "      <td>323259</td>\n",
       "      <td>MSFT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1054</th>\n",
       "      <td>0.242362</td>\n",
       "      <td>458.9856</td>\n",
       "      <td>0.909404</td>\n",
       "      <td>0.939204</td>\n",
       "      <td>0.928757</td>\n",
       "      <td>0.930267</td>\n",
       "      <td>2025-05-27 04:00:00</td>\n",
       "      <td>362158</td>\n",
       "      <td>MSFT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1055</th>\n",
       "      <td>0.174126</td>\n",
       "      <td>458.5505</td>\n",
       "      <td>0.950224</td>\n",
       "      <td>0.909735</td>\n",
       "      <td>0.943872</td>\n",
       "      <td>0.937077</td>\n",
       "      <td>2025-05-28 04:00:00</td>\n",
       "      <td>280126</td>\n",
       "      <td>MSFT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1056 rows × 9 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T05:24:37.797045Z",
     "start_time": "2025-05-30T05:24:22.895067Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Basic preprocessing\n",
    "df = pd.read_csv(\"historical_prices.csv\")\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Normalize per ticker\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def scale_ticker(df, columns):\n",
    "    scalers = {}\n",
    "    for ticker in df['ticker'].unique():\n",
    "        scaler = MinMaxScaler()\n",
    "        mask = df['ticker'] == ticker\n",
    "        df.loc[mask, columns] = scaler.fit_transform(df.loc[mask, columns])\n",
    "        scalers[ticker] = scaler\n",
    "    return df, scalers\n",
    "\n",
    "df, scalers = scale_ticker(df, [\"close\", \"open\", \"high\", \"low\", \"volume\"])\n"
   ],
   "id": "947e24b078c06d94",
   "outputs": [],
   "execution_count": 37
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
